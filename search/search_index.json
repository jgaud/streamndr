{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"StreamNDR","text":"<p>This site contains the documentation for <code>StreamNDR</code>, a Python library based on river which aims to implement novelty detection algorithm for data streams. The library is open-source and availabble on Github.</p>"},{"location":"#what-is-novelty-detection","title":"What is novelty detection?","text":"<p>Novelty Detection consists of the task of detecting novelty concepts (or classes) in a data stream, that is, classes that the model has not seen before. In order to do so, the algorithms often implement an offline phase, where ther learn the known classes in supervised manner, followed by an online phase, where the algorithm will try to label and detect novel classes within the stream of data. </p>"},{"location":"getting_started/","title":"Getting Started","text":""},{"location":"getting_started/#installation","title":"\ud83d\udee0 Installation","text":"<p>Note: StreamNDR is intended to be used with Python 3.6 or above and requires the package ClusOpt-Core which requires a C/C++ compiler (such as gcc) and the Boost.Thread library to build. To install the Boost.Thread library on Debian systems, the following command can be used:</p> <pre><code>sudo apt install libboost-thread-dev\n</code></pre> <p>The package can be installed simply with <code>pip</code> : <pre><code>pip install streamndr\n</code></pre></p>"},{"location":"getting_started/#quickstart","title":"\u26a1\ufe0f Quickstart","text":"<p>As a quick example, we'll train two models (MINAS and ECSMiner-WF) to classify a synthetic dataset created using RandomRBF. The models are trained on only two of the four generated classes ([0,1]) and will try to detect the other classes ([2,3]) as novelty patterns in the dataset in an online fashion.</p> <p>Let's first generate the dataset. <pre><code>import numpy as np\nfrom river.datasets import synth\nds = synth.RandomRBF(seed_model=42, seed_sample=42, n_classes=4, n_features=5, n_centroids=10)\noffline_size = 1000\nonline_size = 5000\nX_train = []\ny_train = []\nX_test = []\ny_test = []\nfor x,y in ds.take(10*(offline_size+online_size)):\n#Create our training data (known classes)\nif len(y_train) &lt; offline_size:\nif y == 0 or y == 1: #Only showing two first classes in the training set\nX_train.append(np.array(list(x.values())))\ny_train.append(y)\n#Create our online stream of data\nelif len(y_test) &lt; online_size:\nX_test.append(x)\ny_test.append(y)\nelse:\nbreak\nX_train = np.array(X_train)\ny_train = np.array(y_train)\n</code></pre></p>"},{"location":"getting_started/#minas","title":"MINAS","text":"<p>Let's train our MINAS model on the offline (known) data. <pre><code>from streamndr.model import Minas\nclf = Minas(kini=10, cluster_algorithm='kmeans', \nwindow_size=100, threshold_strategy=1, threshold_factor=1.1, \nmin_short_mem_trigger=100, min_examples_cluster=50, verbose=1, random_state=42)\nclf.learn_many(np.array(X_train), np.array(y_train)) #learn_many expects numpy arrays or pandas dataframes\n</code></pre></p> <p>Let's now test our algorithm in an online fashion, note that our unsupervised clusters are automatically updated with the call to <code>predict_one</code>.</p> <p><pre><code>from streamndr.metrics import ConfusionMatrixNovelty, MNew, FNew, ErrRate\nknown_classes = [0,1]\nconf_matrix = ConfusionMatrixNovelty(known_classes)\nm_new = MNew(known_classes)\nf_new = FNew(known_classes)\nerr_rate = ErrRate(known_classes)\nfor x, y_true in zip(X_test, y_test):\ny_pred = clf.predict_one(x) #predict_one takes python dictionaries as per River API\nif y_pred is not None: #Update our metrics\nconf_matrix = conf_matrix.update(y_true, y_pred[0])\nm_new = m_new.update(y_true, y_pred[0])\nf_new = f_new.update(y_true, y_pred[0])\nerr_rate = err_rate.update(y_true, y_pred[0])\n</code></pre> Looking at the confusion matrix below, with -1 being the unknown class, we can see that our model succesfully detected some of our novel classes ([3,4]) as novel concepts. The percentage of novel classes instances misclassified as known is also fairly low (2.05%), but we did classified a lot of our known classes samples as novel ones (54.13%). Of course, the hyperparameters of the model can be tuned a lot more to get better results. <pre><code>print(conf_matrix)\nprint(m_new) #Percentage of novel class instances misclassified as known.\nprint(f_new) #Percentage of known classes misclassified as novel.\nprint(err_rate) #Total misclassification error percentage\n</code></pre> |        | -1 | 0 | 1 | 2 | 3 | |--------|--------|-------|-------|-------|-------| | -1 | 0      | 0     | 0     | 0     | 0     | | 0  | 722    | 341   | 33    | 10     | 44    | | 1  | 1155   | 19     | 1296  | 58    | 4     | | 2  | 386    | 7     | 19    | 312   | 0     | | 3  | 172    | 1     | 0     | 0     | 421   |</p> <p>MNew: 2.05%  FNew: 54.13%  ErrRate: 41.44% </p>"},{"location":"getting_started/#ecsminer-wf","title":"ECSMiner-WF","text":"<p>Let's train our model on the offline (known) data.</p> <p><pre><code>from streamndr.model import ECSMinerWF\nclf = ECSMinerWF(K=5, min_examples_cluster=5, verbose=1, random_state=42, ensemble_size=20)\nclf.learn_many(np.array(X_train), np.array(y_train))\n</code></pre> Once again, let's use our model in an online fashion. <pre><code>conf_matrix = ConfusionMatrixNovelty(known_classes)\nm_new = MNew(known_classes)\nf_new = FNew(known_classes)\nerr_rate = ErrRate(known_classes)\nfor x, y_true in zip(X_test, y_test):\ny_pred = clf.predict_one(x) #predict_one takes python dictionaries as per River API\nif y_pred is not None: #Update our metrics\nconf_matrix = conf_matrix.update(y_true, y_pred[0])\nm_new = m_new.update(y_true, y_pred[0])\nf_new = f_new.update(y_true, y_pred[0])\nerr_rate = err_rate.update(y_true, y_pred[0])\n</code></pre></p> <p>The confusion matrix shows us that ECSMiner successfully detected some of the samples of our third class as novel concepts, but not our second class. Again, a lot more tuning can be done to the hyperparameters to improve the results. It is to be noted too that ECSMiner is originally an algorithm that receives feedback (true values) back from the user. With feedback, the algorithm would perform a lot better. <pre><code>print(conf_matrix)\nprint(m_new) #Percentage of novel class instances misclassified as known.\nprint(f_new) #Percentage of known classes misclassified as novel.\nprint(err_rate) #Total misclassification error percentage\n</code></pre> |        | -1 | 0 | 1 | 2 | 3 | 4 | 5 | 6 | |--------|--------|-------|-------|-------|-------|-------|-------|-------| | -1 | 0      | 0     | 0     | 0     | 0     | 0     | 0     | 0     | | 0  | 92     | 835   | 219   | 3     | 0     | 0     | 1     | 0     | | 1  | 216    | 180   | 2131  | 0     | 0     | 1     | 2     | 2     | | 2  | 44     | 6     | 673   | 0     | 0     | 1     | 0     | 0     | | 3  | 106    | 280   | 88    | 0     | 67    | 23    | 19    | 11    | | 4  | 0      | 0     | 0     | 0     | 0     | 0     | 0     | 0     | | 5  | 0      | 0     | 0     | 0     | 0     | 0     | 0     | 0     | | 6  | 0      | 0     | 0     | 0     | 0     | 0     | 0     | 0     |</p> <p>MNew: 79.44%  FNew: 8.61%  ErrRate: 35.26% </p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>metrics<ul> <li>confusion</li> <li>err_rate</li> <li>f_new</li> <li>m_new</li> <li>unk_rate</li> </ul> </li> <li>model<ul> <li>ecsminerwf</li> <li>minas</li> </ul> </li> <li>utils<ul> <li>data_structure</li> </ul> </li> </ul>"},{"location":"reference/metrics/confusion/","title":"confusion","text":""},{"location":"reference/metrics/confusion/#streamndr.metrics.confusion.ConfusionMatrixNovelty","title":"<code>ConfusionMatrixNovelty</code>","text":"<p>         Bases: <code>metrics.confusion.ConfusionMatrix</code></p> <p>Confusion Matrix for novelty detection in data streams.</p> <p>Parameters:</p> Name Type Description Default <code>known_classes</code> <code>list of int</code> <p>List of known labels, the labels the algorithm knows prior to the online phase</p> required <p>Attributes:</p> Name Type Description <code>novel_cm</code> <code>river.metrics.Confusion.ConfusionMatrix</code> <p>Binary confusion matrix representing the problem in a binary manner, including class 0 (known) and class 1 (novelty)</p> <code>nc_samples</code> <code>int</code> <p>Number of samples representing a novelty</p> <code>fe</code> <code>int</code> <p>Known samples that have been classified as a known class other than its ground truth</p> Source code in <code>streamndr/metrics/confusion.py</code> <pre><code>class ConfusionMatrixNovelty(metrics.confusion.ConfusionMatrix):\n\"\"\"Confusion Matrix for novelty detection in data streams.\n    Parameters\n    ----------\n    known_classes : list of int\n        List of known labels, the labels the algorithm knows prior to the online phase\n    Attributes\n    ----------\n    novel_cm : river.metrics.Confusion.ConfusionMatrix\n        Binary confusion matrix representing the problem in a binary manner, including class 0 (known) and class 1 (novelty)\n    nc_samples : int\n        Number of samples representing a novelty\n    fe : int\n        Known samples that have been classified as a known class other than its ground truth\n    \"\"\"\ndef __init__(self, known_classes):\nsuper().__init__(known_classes)\nself.novel_cm = metrics.confusion.ConfusionMatrix()\nself.nc_samples = 0\nself.fe = 0\ndef update(self, y_true, y_pred, sample_weight=1.0):\nsuper().update(y_true, y_pred, sample_weight)\nknown_class = int(y_true in self._init_classes)\npred_known_class = int(y_pred in self._init_classes)\nif known_class == 0:\nself.nc_samples += 1\nelif known_class == pred_known_class == 1 and y_true != y_pred: #If the prediction is not a novelty, but we predicted the wrong class\nself.fe += 1\nself.novel_cm.update(1-known_class, 1-pred_known_class)\nreturn self\ndef revert(self, y_true, y_pred, sample_weight=1.0):\nsuper.revert(self, y_true, y_pred, sample_weight)\nknown_class = int(y_true in self._init_classes)\npred_known_class = int(y_pred in self._init_classes)\nif known_class == 1:\nself.nc_samples -= 1\nif pred_known_class == 1 and y_true != y_pred:\nself.fe -= 1\nself.novel_cm.revert(1-known_class, 1-pred_known_class)\nreturn self\ndef true_positives_novelty(self):\nreturn self.novel_cm.true_positives(1)\ndef true_negatives_novelty(self):\nreturn self.novel_cm.true_negatives(1)\ndef false_positives_novelty(self):\nreturn self.novel_cm.false_positives(1)\ndef false_negatives_novelty(self):\nreturn self.novel_cm.false_negatives(1)\n</code></pre>"},{"location":"reference/metrics/err_rate/","title":"err_rate","text":""},{"location":"reference/metrics/err_rate/#streamndr.metrics.err_rate.ErrRate","title":"<code>ErrRate</code>","text":"<p>         Bases: <code>metrics.base.MultiClassMetric</code></p> <p>Error rate, represents the total misclassification error percentage.</p> <p>Parameters:</p> Name Type Description Default <code>known_classes</code> <code>list of int</code> <p>List of known labels, the labels the algorithm knows prior to the online phase</p> required <p>Attributes:</p> Name Type Description <code>cm</code> <code>ConfusionMatrixNovelty</code> <p>Confusion matrix</p> Source code in <code>streamndr/metrics/err_rate.py</code> <pre><code>class ErrRate(metrics.base.MultiClassMetric):\n\"\"\"Error rate, represents the total misclassification error percentage.\n    Parameters\n    ----------\n    known_classes : list of int\n        List of known labels, the labels the algorithm knows prior to the online phase\n    Attributes\n    ----------\n    cm : ConfusionMatrixNovelty\n        Confusion matrix\n    \"\"\"\ndef __init__(self, known_classes):\ncm = ConfusionMatrixNovelty(known_classes)\nsuper(metrics.base.MultiClassMetric, self).__init__(cm)\ndef get(self):\nfp = self.cm.false_positives_novelty() #Number of known class samples wrongly classified as novelties\nfn = self.cm.false_negatives_novelty() #Number of novelties wrongly classified as known\ntry:\nreturn (fp + fn + self.cm.fe) / self.cm.n_samples\nexcept ZeroDivisionError:\nreturn 0.0\n</code></pre>"},{"location":"reference/metrics/f_new/","title":"f_new","text":""},{"location":"reference/metrics/f_new/#streamndr.metrics.f_new.FNew","title":"<code>FNew</code>","text":"<p>         Bases: <code>metrics.base.MultiClassMetric</code></p> <p>Metric F_new, which represents the percentage of known classes misclassified as novel.</p> <p>Parameters:</p> Name Type Description Default <code>known_classes</code> <code>list of int</code> <p>List of known labels, the labels the algorithm knows prior to the online phase</p> required <p>Attributes:</p> Name Type Description <code>cm</code> <code>ConfusionMatrixNovelty</code> <p>Confusion matrix</p> Source code in <code>streamndr/metrics/f_new.py</code> <pre><code>class FNew(metrics.base.MultiClassMetric):\n\"\"\"Metric F_new, which represents the percentage of known classes misclassified as novel.\n    Parameters\n    ----------\n    known_classes : list of int\n        List of known labels, the labels the algorithm knows prior to the online phase\n    Attributes\n    ----------\n    cm : ConfusionMatrixNovelty\n        Confusion matrix\n    \"\"\"\ndef __init__(self, known_classes):\ncm = ConfusionMatrixNovelty(known_classes)\nsuper(metrics.base.MultiClassMetric, self).__init__(cm)\ndef get(self):\nfp = self.cm.false_positives_novelty() #Number of known class samples wrongly classified as novelties\ntry:\nreturn fp / (self.cm.n_samples - self.cm.nc_samples)\nexcept ZeroDivisionError:\nreturn 0.0\n</code></pre>"},{"location":"reference/metrics/m_new/","title":"m_new","text":""},{"location":"reference/metrics/m_new/#streamndr.metrics.m_new.MNew","title":"<code>MNew</code>","text":"<p>         Bases: <code>metrics.base.MultiClassMetric</code></p> <p>Metric M_new, which represents the percentage of novel class instances misclassified as known.</p> <p>Parameters:</p> Name Type Description Default <code>known_classes</code> <code>list of int</code> <p>List of known labels, the labels the algorithm knows prior to the online phase</p> required <p>Attributes:</p> Name Type Description <code>cm</code> <code>ConfusionMatrixNovelty</code> <p>Confusion matrix</p> Source code in <code>streamndr/metrics/m_new.py</code> <pre><code>class MNew(metrics.base.MultiClassMetric):\n\"\"\"Metric M_new, which represents the percentage of novel class instances misclassified as known.\n    Parameters\n    ----------\n    known_classes : list of int\n        List of known labels, the labels the algorithm knows prior to the online phase\n    Attributes\n    ----------\n    cm : ConfusionMatrixNovelty\n        Confusion matrix\n    \"\"\"\ndef __init__(self, known_classes):\ncm = ConfusionMatrixNovelty(known_classes)\nsuper(metrics.base.MultiClassMetric, self).__init__(cm)\ndef get(self):\nfn = self.cm.false_negatives_novelty() #Number of novelties wrongly classified as known\ntry:\nreturn fn / self.cm.nc_samples\nexcept ZeroDivisionError:\nreturn 0.0\n</code></pre>"},{"location":"reference/metrics/unk_rate/","title":"unk_rate","text":""},{"location":"reference/metrics/unk_rate/#streamndr.metrics.unk_rate.UnkRate","title":"<code>UnkRate</code>","text":"<p>         Bases: <code>metrics.base.MultiClassMetric</code></p> <p>Unknown rate, represents the percentage of unknown samples.</p> <p>Parameters:</p> Name Type Description Default <code>known_classes</code> <code>list of int</code> <p>List of known labels, the labels the algorithm knows prior to the online phase</p> required <p>Attributes:</p> Name Type Description <code>cm</code> <code>ConfusionMatrixNovelty</code> <p>Confusion matrix</p> Source code in <code>streamndr/metrics/unk_rate.py</code> <pre><code>class UnkRate(metrics.base.MultiClassMetric):\n\"\"\"Unknown rate, represents the percentage of unknown samples.\n    Parameters\n    ----------\n    known_classes : list of int\n        List of known labels, the labels the algorithm knows prior to the online phase\n    Attributes\n    ----------\n    cm : ConfusionMatrixNovelty\n        Confusion matrix\n    \"\"\"\ndef __init__(self, known_classes):\ncm = ConfusionMatrixNovelty(known_classes)\nsuper(metrics.base.MultiClassMetric, self).__init__(cm)\ndef get(self):\ntry:\nreturn self.cm.sum_col[-1] / self.cm.n_samples\nexcept ZeroDivisionError:\nreturn 0.0\n</code></pre>"},{"location":"reference/model/ecsminerwf/","title":"ecsminerwf","text":""},{"location":"reference/model/ecsminerwf/#streamndr.model.ecsminerwf.ECSMinerWF","title":"<code>ECSMinerWF</code>","text":"<p>         Bases: <code>base.MiniBatchClassifier</code></p> <p>Implementation of the ECSMinerWF (ECSMiner without feedback) algorithm for novelty detection.</p> <p>Parameters:</p> Name Type Description Default <code>K</code> <code>int</code> <p>Number of pseudopoints per classifier. In other words, it is the number of K cluster for the clustering algorithm.</p> <code>50</code> <code>min_examples_cluster</code> <code>int</code> <p>Minimum number of examples to declare a novel class</p> <code>50</code> <code>ensemble_size</code> <code>int</code> <p>Number of classifiers to use to create the ensemble</p> <code>6</code> <code>verbose</code> <code>int</code> <p>Controls the level of verbosity, the higher, the more messages are displayed. Can be '1', '2', or '3'.</p> <code>0</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generation. Makes the algorithm deterministic if a number is provided.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>MAX_MEMORY_SIZE</code> <code>int</code> <p>Constant used to determine the maximum number of rows used by numpy for the computation of the closest clusters. A higher number is faster but takes more memory.</p> <code>models</code> <code>list of list of MicroCluster</code> <p>List containing the models created in the offline phase. In other words, it contains multiple lists of MicroClusters.</p> <code>novel_models</code> <code>list of MicroCluster</code> <p>Contains the clusters representing novel classes, added during the online phase</p> <code>sample_counter</code> <code>int</code> <p>Number of samples treated, used by the forgetting mechanism</p> <code>short_mem</code> <code>list of ShortMemInstance</code> <p>Buffer memory containing the samples labeled as unknown temporarily for the novelty detection process</p> <code>last_nd</code> <code>int</code> <p>Timestamp when the last novelty detection was performed. Used to determine if a new novelty detection should be performed.</p> <code>before_offline_phase</code> <code>bool</code> <p>Whether or not the algorithm was initialized (offline phase). The algorithm needs to first be initialized to be used in an online fashion.</p> Source code in <code>streamndr/model/ecsminerwf.py</code> <pre><code>class ECSMinerWF(base.MiniBatchClassifier):\n\"\"\"Implementation of the ECSMinerWF (ECSMiner without feedback) algorithm for novelty detection.\n    Parameters\n    ----------\n    K : int\n        Number of pseudopoints per classifier. In other words, it is the number of K cluster for the clustering algorithm.\n    min_examples_cluster : int\n        Minimum number of examples to declare a novel class \n    ensemble_size : int\n        Number of classifiers to use to create the ensemble\n    verbose : int\n        Controls the level of verbosity, the higher, the more messages are displayed. Can be '1', '2', or '3'.\n    random_state : int\n        Seed for the random number generation. Makes the algorithm deterministic if a number is provided.\n    Attributes\n    ----------\n    MAX_MEMORY_SIZE : int\n        Constant used to determine the maximum number of rows used by numpy for the computation of the closest clusters. A higher number is faster but takes more memory.\n    models : list of list of MicroCluster\n        List containing the models created in the offline phase. In other words, it contains multiple lists of MicroClusters.\n    novel_models : list of MicroCluster\n        Contains the clusters representing novel classes, added during the online phase\n    sample_counter : int\n        Number of samples treated, used by the forgetting mechanism\n    short_mem : list of ShortMemInstance\n        Buffer memory containing the samples labeled as unknown temporarily for the novelty detection process\n    last_nd : int\n        Timestamp when the last novelty detection was performed. Used to determine if a new novelty detection should be performed.\n    before_offline_phase : bool\n        Whether or not the algorithm was initialized (offline phase). The algorithm needs to first be initialized to be used in an online fashion.\n    \"\"\"\nMAX_MEMORY_SIZE = 50000\ndef __init__(self,\nK=50, \nmin_examples_cluster=50, #Number of instances requried to declare a novel class \nensemble_size=6, \nverbose=0,\nrandom_state=None):\nsuper().__init__()\nself.K = K\nself.min_examples_cluster = min_examples_cluster\nself.ensemble_size = ensemble_size\nself.verbose = verbose\nself.random_state = random_state\nself.models = []\nself.novel_models = []\nself.sample_counter = 0\nself.short_mem = [] #Potential novel class instances\nself.last_nd = -self.min_examples_cluster #No novelty detection performed yet\nself.before_offline_phase = True\ndef learn_one(self, x, y, w=1.0):\n#Function used by river algorithms to learn one sample. It is not applicable to this algorithm since the offline phase requires all samples\n#to arrive at once. It is only added as to follow River's API.\npass\ndef learn_many(self, X, y, w=1.0):\n\"\"\"Represents the offline phase of the algorithm. Receives a number of samples and their given labels and learns all of the known classes.\n        Parameters\n        ----------\n        X : pandas.DataFrame or numpy.ndarray\n            Samples to be learned by the model\n        y : list of int\n            Labels corresponding to the given samples, must be the same length as the number of samples\n        w : float, optional\n            Weights, not used, by default 1.0\n        Returns\n        -------\n        ECSMinerWF\n            Itself\n        \"\"\"\nif isinstance(X, pd.DataFrame):\nX = X.to_numpy()\nself.chunk_size = math.ceil(len(X)/self.ensemble_size)\n# in offline phase, consider all instances arriving at the same time in the microclusters:\ntimestamp = len(X)\n#Separate data into (ensemble_size) chunks\nfor i in range(0, self.ensemble_size):\nX_chunk = X[i:i+self.chunk_size]\ny_chunk = y[i:i+self.chunk_size]\nself.models.append(self._generate_microclusters(X_chunk, y_chunk, timestamp, self.K, min_samples=3)) #As per ECSMiner paper, any microcluster with less than 3 instances is discarded\nself.before_offline_phase = False\nreturn self\ndef predict_one(self, X):\n\"\"\"Represents the online phase. Equivalent to predict_many() with only one sample. Receives only one sample, predict its label and adds \n        it to the cluster if it is a known class. Otherwise, if it's unknown, it is added to the short term memory and novelty detection is \n        performed once the trigger has been reached (min_examples_cluster).\n        Parameters\n        ----------\n        X : dict\n            Sample\n        Returns\n        -------\n        numpy.ndarray\n            Label predicted for the given sample, predicts -1 if labeled as unknown\n        \"\"\"\nreturn self.predict_many(np.array(list(X.values()))[None,:])\ndef predict_many(self, X):\n\"\"\"Represents the online phase. Receives multiple samples, for each sample predict its label and adds it to the cluster if it is a known class. \n        Otherwise, if it's unknown, it is added to the short term memory and novelty detection is performed once the trigger has been reached (min_examples_cluster).\n        Parameters\n        ----------\n        X : pandas.DataFrame or numpy.ndarray\n            Samples\n        Returns\n        -------\n        numpy.ndarray\n            Array of length len(X) containing the predicted labels, predicts -1 if the corresponding sample labeled as unknown\n        Raises\n        ------\n        Exception\n            If the model has not been trained first with learn_many() (offline phase)\n        \"\"\"\nif self.before_offline_phase:\nraise Exception(\"Model must be fitted first\")\nif isinstance(X, pd.DataFrame):\nX = X.to_numpy() #Converting DataFrame to numpy array\nclosest_model_cluster, y_preds = self._majority_voting(X)\nif len(self.novel_models) &gt; 0: #We have novel clusters in our list\nnovel_closest_clusters, _ = self._get_closest_clusters(X, [microcluster.centroid for microcluster in self.novel_models])\npred_label = []\nfor i in range(len(X)):\nself.sample_counter += 1\nclosest_cluster = self.models[closest_model_cluster[i][0]][closest_model_cluster[i][1]]\nself._filter_buffer()\nif closest_cluster.distance_to_centroid(X[i]) &lt;= closest_cluster.max_distance: # classify with the label from majority voting\npred_label.append(y_preds[i])\nclosest_cluster.update_cluster(X[i], self.sample_counter, False)\nelif (len(self.novel_models) &gt; 0) and (self.novel_models[novel_closest_clusters[i]].distance_to_centroid(X[i]) &lt;= closest_cluster.max_distance): #One of our novel cluster can explain our sample\npred_label.append(self.novel_models[novel_closest_clusters[i]].label)\nself.novel_models[novel_closest_clusters[i]].update_cluster(X[i], self.sample_counter, False)\nelse: #Classify as unknown\npred_label.append(-1)\nself.short_mem.append(ShortMemInstance(X[i], self.sample_counter))\nif len(self.short_mem) &gt; self.min_examples_cluster and (self.last_nd + self.min_examples_cluster) &lt;= self.sample_counter:\nself.last_nd = self.sample_counter\nnovel_clusters = self._novelty_detect()\nif novel_clusters is not None: #We have novelty clusters\nfor novel_cluster in novel_clusters:\nmax_label_ensemble = max([cluster.label for model in self.models for cluster in model])\nmax_label_novel = max([cluster.label for cluster in self.novel_models]) if len(self.novel_models) &gt; 0 else -1\nnovel_cluster.label = max(max_label_ensemble, max_label_novel) + 1\nif self.verbose &gt; 0: print(\"Novel cluster detected: \", novel_cluster.small_str())\n#Add novel cluster to our novel models list\nself.novel_models.append(novel_cluster)\n#Remove instances from the buffer\nfor instance in novel_cluster.instances:\nself.short_mem.remove(instance)\nreturn np.array(pred_label)\ndef predict_proba_one(self,X):\n#Function used by river algorithms to get the probability of the prediction. It is not applicable to this algorithm since it only predicts labels. \n#It is only added as to follow River's API.\npass\ndef predict_proba_many(self, X):\n#Function used by river algorithms to get the probability of the predictions. It is not applicable to this algorithm since it only predicts labels. \n#It is only added as to follow River's API.\npass\ndef _generate_microclusters(self, X, y, timestamp, K, keep_instances=False, min_samples=0):\nclf = KMeans(n_clusters=K, n_init='auto', random_state=self.random_state).fit(X)\nlabels = clf.labels_\nmicroclusters = []\nfor microcluster in np.unique(labels):\ncluster_instances = X[labels == microcluster]\ny_cluster_instances = y[labels == microcluster]\nvalues, counts = np.unique(y_cluster_instances, return_counts=True)\nmost_common_y = values[np.argmax(counts)]\nif len(cluster_instances) &gt;= min_samples:\nmc = MicroCluster(most_common_y, instances=cluster_instances, timestamp=timestamp, keep_instances=keep_instances)\nmicroclusters.append(mc)\nreturn microclusters\ndef _majority_voting(self, X):\nclosest_clusters = []\nlabels = []\ndists = []\nfor model in self.models:\nclosest_clusters_model, dist = self._get_closest_clusters(X, [microcluster.centroid for microcluster in model])\nclosest_clusters.append(closest_clusters_model)\nlabels.append([model[closest_cluster].label for closest_cluster in closest_clusters_model])\ndists.append(dist) \nbest_models = np.argmin(dists, axis=0)\nclosest_model_cluster = []\nfor i in range(len(X)):\nclosest_model_cluster.append((best_models[i], closest_clusters[best_models[i]][i]))\nreturn closest_model_cluster, [Counter(col).most_common(1)[0][0] for col in zip(*labels)]\ndef _get_closest_clusters(self, X, centroids):   \nif len(centroids) == 0:\nprint(\"No clusters\")\nreturn\ncentroids = np.array(centroids)\nnorm_dists = np.zeros((X.shape[0],centroids.shape[0]))\n# Cut into batches if there are too many samples to save on memory\nfor idx in range(math.ceil(X.shape[0]/ECSMinerWF.MAX_MEMORY_SIZE)):\nsl = slice(idx*ECSMinerWF.MAX_MEMORY_SIZE, (idx+1)*ECSMinerWF.MAX_MEMORY_SIZE)\nnorm_dists[sl] = np.linalg.norm(np.subtract(X[sl, :, None], np.transpose(centroids)), axis=1)\nreturn np.argmin(norm_dists, axis=1), np.amin(norm_dists, axis=1)\ndef _novelty_detect(self):\nif self.verbose &gt; 0: print(\"Novelty detection started\")\nX = np.array([instance.point for instance in self.short_mem])\nnew_class_vote = 0\n#Creating F-pseudopoints\nK0 = math.ceil(self.K * (len(X) / self.chunk_size))\nK0 = max(K0, self.K)\nf_microclusters = self._generate_microclusters(X, np.array([-1] * len(X)), self.sample_counter, K0, keep_instances=True)\nf_microclusters_centroids = np.array([cl.centroid for cl in f_microclusters])\npotential_novel_clusters_idx = []\n#Computing qNSC for each model in our ensemble\nfor model in self.models:\nqnscs = self._qnsc(f_microclusters_centroids, model)\npotential_clusters = []\ntotal_instances = 0\nfor i, f_microcluster in enumerate(f_microclusters):\nif qnscs[i] &gt; 0:\npotential_clusters.append(f_microcluster)\ntotal_instances += f_microcluster.n\npotential_novel_clusters_idx.append(i)\nif total_instances &gt; self.min_examples_cluster: new_class_vote += 1\nif new_class_vote == len(self.models):\n#Get the indices of all clusters which had a positive qnsc for all models\nnovel_clusters_idx = [item for item, count in Counter(potential_novel_clusters_idx).items() if count == len(self.models)]\nnovel_clusters = [f_microclusters[i] for i in novel_clusters_idx]\nreturn novel_clusters\nelse:\nreturn None\ndef _qnsc(self, pseudopoints, model):\n#Calculate mean distance of all points between themselves\ndists = np.linalg.norm(pseudopoints - pseudopoints[:,None], axis=-1)\ndists[np.arange(dists.shape[0]), np.arange(dists.shape[0])] = np.nan\nmean_distances_between_points = np.nanmean(dists, axis=0)\n#Calculate minimum distance between points known cluster\nall_centroids = [microcluster.centroid for microcluster in model]\n_, minimum_distances_to_class = self._get_closest_clusters(pseudopoints, all_centroids)\nqnscs = (minimum_distances_to_class - mean_distances_between_points) / np.maximum(minimum_distances_to_class, mean_distances_between_points)\nreturn qnscs\ndef _filter_buffer(self):\nfor instance in self.short_mem:\nif (self.sample_counter - instance.timestamp &gt; self.chunk_size): #We remove samples that have an age greater than the chunk size\nself.short_mem.remove(instance)\nelse: #No need to iterate over the whole buffer since older elements are at the beginning\nbreak;\n</code></pre>"},{"location":"reference/model/ecsminerwf/#streamndr.model.ecsminerwf.ECSMinerWF.learn_many","title":"<code>learn_many(X, y, w=1.0)</code>","text":"<p>Represents the offline phase of the algorithm. Receives a number of samples and their given labels and learns all of the known classes.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>pandas.DataFrame or numpy.ndarray</code> <p>Samples to be learned by the model</p> required <code>y</code> <code>list of int</code> <p>Labels corresponding to the given samples, must be the same length as the number of samples</p> required <code>w</code> <code>float, optional</code> <p>Weights, not used, by default 1.0</p> <code>1.0</code> <p>Returns:</p> Type Description <code>ECSMinerWF</code> <p>Itself</p> Source code in <code>streamndr/model/ecsminerwf.py</code> <pre><code>def learn_many(self, X, y, w=1.0):\n\"\"\"Represents the offline phase of the algorithm. Receives a number of samples and their given labels and learns all of the known classes.\n    Parameters\n    ----------\n    X : pandas.DataFrame or numpy.ndarray\n        Samples to be learned by the model\n    y : list of int\n        Labels corresponding to the given samples, must be the same length as the number of samples\n    w : float, optional\n        Weights, not used, by default 1.0\n    Returns\n    -------\n    ECSMinerWF\n        Itself\n    \"\"\"\nif isinstance(X, pd.DataFrame):\nX = X.to_numpy()\nself.chunk_size = math.ceil(len(X)/self.ensemble_size)\n# in offline phase, consider all instances arriving at the same time in the microclusters:\ntimestamp = len(X)\n#Separate data into (ensemble_size) chunks\nfor i in range(0, self.ensemble_size):\nX_chunk = X[i:i+self.chunk_size]\ny_chunk = y[i:i+self.chunk_size]\nself.models.append(self._generate_microclusters(X_chunk, y_chunk, timestamp, self.K, min_samples=3)) #As per ECSMiner paper, any microcluster with less than 3 instances is discarded\nself.before_offline_phase = False\nreturn self\n</code></pre>"},{"location":"reference/model/ecsminerwf/#streamndr.model.ecsminerwf.ECSMinerWF.predict_many","title":"<code>predict_many(X)</code>","text":"<p>Represents the online phase. Receives multiple samples, for each sample predict its label and adds it to the cluster if it is a known class.  Otherwise, if it's unknown, it is added to the short term memory and novelty detection is performed once the trigger has been reached (min_examples_cluster).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>pandas.DataFrame or numpy.ndarray</code> <p>Samples</p> required <p>Returns:</p> Type Description <code>numpy.ndarray</code> <p>Array of length len(X) containing the predicted labels, predicts -1 if the corresponding sample labeled as unknown</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the model has not been trained first with learn_many() (offline phase)</p> Source code in <code>streamndr/model/ecsminerwf.py</code> <pre><code>def predict_many(self, X):\n\"\"\"Represents the online phase. Receives multiple samples, for each sample predict its label and adds it to the cluster if it is a known class. \n    Otherwise, if it's unknown, it is added to the short term memory and novelty detection is performed once the trigger has been reached (min_examples_cluster).\n    Parameters\n    ----------\n    X : pandas.DataFrame or numpy.ndarray\n        Samples\n    Returns\n    -------\n    numpy.ndarray\n        Array of length len(X) containing the predicted labels, predicts -1 if the corresponding sample labeled as unknown\n    Raises\n    ------\n    Exception\n        If the model has not been trained first with learn_many() (offline phase)\n    \"\"\"\nif self.before_offline_phase:\nraise Exception(\"Model must be fitted first\")\nif isinstance(X, pd.DataFrame):\nX = X.to_numpy() #Converting DataFrame to numpy array\nclosest_model_cluster, y_preds = self._majority_voting(X)\nif len(self.novel_models) &gt; 0: #We have novel clusters in our list\nnovel_closest_clusters, _ = self._get_closest_clusters(X, [microcluster.centroid for microcluster in self.novel_models])\npred_label = []\nfor i in range(len(X)):\nself.sample_counter += 1\nclosest_cluster = self.models[closest_model_cluster[i][0]][closest_model_cluster[i][1]]\nself._filter_buffer()\nif closest_cluster.distance_to_centroid(X[i]) &lt;= closest_cluster.max_distance: # classify with the label from majority voting\npred_label.append(y_preds[i])\nclosest_cluster.update_cluster(X[i], self.sample_counter, False)\nelif (len(self.novel_models) &gt; 0) and (self.novel_models[novel_closest_clusters[i]].distance_to_centroid(X[i]) &lt;= closest_cluster.max_distance): #One of our novel cluster can explain our sample\npred_label.append(self.novel_models[novel_closest_clusters[i]].label)\nself.novel_models[novel_closest_clusters[i]].update_cluster(X[i], self.sample_counter, False)\nelse: #Classify as unknown\npred_label.append(-1)\nself.short_mem.append(ShortMemInstance(X[i], self.sample_counter))\nif len(self.short_mem) &gt; self.min_examples_cluster and (self.last_nd + self.min_examples_cluster) &lt;= self.sample_counter:\nself.last_nd = self.sample_counter\nnovel_clusters = self._novelty_detect()\nif novel_clusters is not None: #We have novelty clusters\nfor novel_cluster in novel_clusters:\nmax_label_ensemble = max([cluster.label for model in self.models for cluster in model])\nmax_label_novel = max([cluster.label for cluster in self.novel_models]) if len(self.novel_models) &gt; 0 else -1\nnovel_cluster.label = max(max_label_ensemble, max_label_novel) + 1\nif self.verbose &gt; 0: print(\"Novel cluster detected: \", novel_cluster.small_str())\n#Add novel cluster to our novel models list\nself.novel_models.append(novel_cluster)\n#Remove instances from the buffer\nfor instance in novel_cluster.instances:\nself.short_mem.remove(instance)\nreturn np.array(pred_label)\n</code></pre>"},{"location":"reference/model/ecsminerwf/#streamndr.model.ecsminerwf.ECSMinerWF.predict_one","title":"<code>predict_one(X)</code>","text":"<p>Represents the online phase. Equivalent to predict_many() with only one sample. Receives only one sample, predict its label and adds  it to the cluster if it is a known class. Otherwise, if it's unknown, it is added to the short term memory and novelty detection is  performed once the trigger has been reached (min_examples_cluster).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>dict</code> <p>Sample</p> required <p>Returns:</p> Type Description <code>numpy.ndarray</code> <p>Label predicted for the given sample, predicts -1 if labeled as unknown</p> Source code in <code>streamndr/model/ecsminerwf.py</code> <pre><code>def predict_one(self, X):\n\"\"\"Represents the online phase. Equivalent to predict_many() with only one sample. Receives only one sample, predict its label and adds \n    it to the cluster if it is a known class. Otherwise, if it's unknown, it is added to the short term memory and novelty detection is \n    performed once the trigger has been reached (min_examples_cluster).\n    Parameters\n    ----------\n    X : dict\n        Sample\n    Returns\n    -------\n    numpy.ndarray\n        Label predicted for the given sample, predicts -1 if labeled as unknown\n    \"\"\"\nreturn self.predict_many(np.array(list(X.values()))[None,:])\n</code></pre>"},{"location":"reference/model/minas/","title":"minas","text":""},{"location":"reference/model/minas/#streamndr.model.minas.Minas","title":"<code>Minas</code>","text":"<p>         Bases: <code>base.MiniBatchClassifier</code></p> <p>Implementation of the MINAS algorithm for novelty detection.</p> <p>Parameters:</p> Name Type Description Default <code>kini</code> <code>int</code> <p>Number of K clusters for the clustering (KMeans or Clustream) algorithm</p> <code>3</code> <code>cluster_algorithm</code> <code>str</code> <p>String containing the clustering algorithm to use, supports 'kmeans' and 'clustream'</p> <code>'kmeans'</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generation. Makes the algorithm deterministic if a number is provided.</p> <code>None</code> <code>min_short_mem_trigger</code> <code>int</code> <p>Minimum number of samples in the short term memory to trigger the novelty detection process</p> <code>10</code> <code>min_examples_cluster</code> <code>int</code> <p>Minimum number of samples to from a cluster</p> <code>10</code> <code>threshold_strategy</code> <code>int</code> <p>Strategy to use to compute the threshold. Can be '1', '2', or '3' as described in the MINAS paper.</p> <code>1</code> <code>threshold_factor</code> <code>float</code> <p>Factor for the threshold computation</p> <code>1.1</code> <code>window_size</code> <code>int</code> <p>Number of samples used by the forgetting mechanism</p> <code>100</code> <code>update_summary</code> <code>bool</code> <p>Whether or not the microcluster's properties are updated when a new point is added to it</p> <code>False</code> <code>verbose</code> <code>int</code> <p>Controls the level of verbosity, the higher, the more messages are displayed. Can be '1', '2', or '3'.</p> <code>0</code> <p>Attributes:</p> Name Type Description <code>MAX_MEMORY_SIZE</code> <code>int</code> <p>Constant used to determine the maximum number of rows used by numpy for the computation of the closest clusters. A higher number is faster but takes more memory.</p> <code>before_offline_phase</code> <code>bool</code> <p>Whether or not the algorithm was initialized (offline phase). The algorithm needs to first be initialized to be used in an online fashion.</p> <code>short_mem</code> <code>list of ShortMemInstance</code> <p>Buffer memory containing the samples labeled as unknown temporarily for the novelty detection process</p> <code>sleep_mem</code> <code>list of MicroCluster</code> <p>Microclusters that have not have any new points added from the strem for a period of time are temporarily moved to a sleep memory</p> <code>sample_counter</code> <code>int</code> <p>Number of samples treated, used by the forgetting mechanism</p> Source code in <code>streamndr/model/minas.py</code> <pre><code>class Minas(base.MiniBatchClassifier):\n\"\"\"Implementation of the MINAS algorithm for novelty detection.\n    Parameters\n    ----------\n    kini : int\n        Number of K clusters for the clustering (KMeans or Clustream) algorithm\n    cluster_algorithm : str\n        String containing the clustering algorithm to use, supports 'kmeans' and 'clustream'\n    random_state : int\n        Seed for the random number generation. Makes the algorithm deterministic if a number is provided.\n    min_short_mem_trigger : int\n        Minimum number of samples in the short term memory to trigger the novelty detection process\n    min_examples_cluster : int\n        Minimum number of samples to from a cluster\n    threshold_strategy : int\n        Strategy to use to compute the threshold. Can be '1', '2', or '3' as described in the MINAS paper.\n    threshold_factor : float\n        Factor for the threshold computation\n    window_size : int\n        Number of samples used by the forgetting mechanism\n    update_summary : bool\n        Whether or not the microcluster's properties are updated when a new point is added to it\n    verbose : int\n        Controls the level of verbosity, the higher, the more messages are displayed. Can be '1', '2', or '3'.\n    Attributes\n    ----------\n    MAX_MEMORY_SIZE : int\n        Constant used to determine the maximum number of rows used by numpy for the computation of the closest clusters. A higher number is faster but takes more memory.\n    before_offline_phase : bool\n        Whether or not the algorithm was initialized (offline phase). The algorithm needs to first be initialized to be used in an online fashion.\n    short_mem : list of ShortMemInstance\n        Buffer memory containing the samples labeled as unknown temporarily for the novelty detection process\n    sleep_mem : list of MicroCluster\n        Microclusters that have not have any new points added from the strem for a period of time are temporarily moved to a sleep memory\n    sample_counter : int\n        Number of samples treated, used by the forgetting mechanism\n    \"\"\"\nMAX_MEMORY_SIZE = 50000\ndef __init__(self,\nkini=3,\ncluster_algorithm='kmeans',\nrandom_state=None,\nmin_short_mem_trigger=10,\nmin_examples_cluster=10,\nthreshold_strategy=1,\nthreshold_factor=1.1,\nwindow_size=100,\nupdate_summary=False,\nverbose=0):\nsuper().__init__()\nself.kini = kini\nself.random_state = random_state\naccepted_algos = ['kmeans','clustream']\nif cluster_algorithm not in accepted_algos:\nprint('Available algorithms: {}'.format(', '.join(accepted_algos)))\nelse:\nself.cluster_algorithm = cluster_algorithm\nself.microclusters = []  # list of microclusters\nself.before_offline_phase = True\nself.short_mem = []\nself.sleep_mem = []\nself.min_short_mem_trigger = min_short_mem_trigger\nself.min_examples_cluster = min_examples_cluster\nself.threshold_strategy = threshold_strategy\nself.threshold_factor = threshold_factor\nself.window_size = window_size\nself.update_summary = update_summary\nself.verbose = verbose\nself.sample_counter = 0  # to be used with window_size\ndef learn_one(self, x, y, w=1.0):\n\"\"\"Function used by river algorithms to learn one sample. It is not applicable to this algorithm since the offline phase requires all samples\n        to arrive at once. It is only added as to follow River's API.\n        Parameters\n        ----------\n        x : dict\n            Sample\n        y : int\n            Label of the given sample\n        w : float, optional\n            Weight, not used, by default 1.0\n        \"\"\"\n# Not applicable\npass\ndef learn_many(self, X, y, w=1.0):\n\"\"\"Represents the offline phase of the algorithm. Receives a number of samples and their given labels and learns all of the known classes.\n        Parameters\n        ----------\n        X : pandas.DataFrame or numpy.ndarray\n            Samples to be learned by the model\n        y : list of int\n            Labels corresponding to the given samples, must be the same length as the number of samples\n        w : float, optional\n            Weights, not used, by default 1.0\n        Returns\n        -------\n        Minas\n            Itself\n        \"\"\"\nif isinstance(X, pd.DataFrame):\nX = X.to_numpy()\nself.microclusters = self._offline(X, y)\nself.before_offline_phase = False\nreturn self\ndef predict_one(self, X):\n\"\"\"Represents the online phase. Equivalent to predict_many() with only one sample. Receives only one sample, predict its label and adds \n        it to the cluster if it is a known class. Otherwise, if it's unknown, it is added to the short term memory and novelty detection is \n        performed once the trigger has been reached (min_short_mem_trigger).\n        Parameters\n        ----------\n        X : dict\n            Sample\n        Returns\n        -------\n        numpy.ndarray\n            Label predicted for the given sample, predicts -1 if labeled as unknown\n        \"\"\"\nreturn self.predict_many(np.array(list(X.values()))[None,:])\ndef predict_many(self, X):\n\"\"\"Represents the online phase. Receives multiple samples, for each sample predict its label and adds it to the cluster if it is a known class. \n        Otherwise, if it's unknown, it is added to the short term memory and novelty detection is performed once the trigger has been reached (min_short_mem_trigger).\n        Parameters\n        ----------\n        X : pandas.DataFrame or numpy.ndarray\n            Samples\n        Returns\n        -------\n        numpy.ndarray\n            Array of length len(X) containing the predicted labels, predicts -1 if the corresponding sample labeled as unknown\n        Raises\n        ------\n        Exception\n            If the model has not been trained first with learn_many() (offline phase)\n        \"\"\"\nif self.before_offline_phase:\nraise Exception(\"Model must be fitted first\")\nif isinstance(X, pd.DataFrame):\nX = X.to_numpy() #Converting DataFrame to numpy array\n# Finding closest clusters for received samples\nclosest_clusters = self._get_closest_clusters(X, [microcluster.centroid for microcluster in self.microclusters])\npred_label = []\nfor i in range(len(closest_clusters)):\nself.sample_counter += 1\nclosest_cluster = self.microclusters[closest_clusters[i]]\nif closest_cluster.encompasses(X[i]):  # classify in this cluster\npred_label.append(closest_cluster.label)\nclosest_cluster.update_cluster(X[i], self.sample_counter, self.update_summary)\nelse:  # classify as unknown\npred_label.append(-1)\nself.short_mem.append(ShortMemInstance(X[i], self.sample_counter))\nif self.verbose &gt; 1:\nprint('Memory length: ', len(self.short_mem))\nelif self.verbose &gt; 0:\nif len(self.short_mem) % 100 == 0: print('Memory length: ', len(self.short_mem))\nif len(self.short_mem) &gt;= self.min_short_mem_trigger:\nself._novelty_detect()\n# forgetting mechanism\nif self.sample_counter % self.window_size == 0:\nself._trigger_forget()\nreturn np.array(pred_label)\ndef predict_proba_one(self,X):\n#Function used by river algorithms to get the probability of the prediction. It is not applicable to this algorithm since it only predicts labels. \n#It is only added as to follow River's API.\npass\ndef predict_proba_many(self, X):\n#Function used by river algorithms to get the probability of the predictions. It is not applicable to this algorithm since it only predicts labels. \n#It is only added as to follow River's API.\npass\ndef _offline(self, X_train, y_train):\nmicroclusters = []\n# in offline phase, consider all instances arriving at the same time in the microclusters:\ntimestamp = len(X_train)\nfor y_class in np.unique(y_train):\n# subset with instances from each class\nX_class = X_train[y_train == y_class]\nif self.cluster_algorithm == 'kmeans':\nclass_cluster_clf = KMeans(n_clusters=self.kini, n_init='auto',\nrandom_state=self.random_state)\nclass_cluster_clf.fit(X_class)\nlabels = class_cluster_clf.labels_\nelse:\nclass_cluster_clf = CluStream(m=self.kini)\nclass_cluster_clf.init_offline(X_class, seed=self.random_state)\ncluster_centers = class_cluster_clf.get_partial_cluster_centers()\nlabels = self._get_closest_clusters(X_class, cluster_centers)\nfor class_cluster in np.unique(labels):\n# get instances in cluster\ncluster_instances = X_class[labels == class_cluster]\nmicroclusters.append(\nMicroCluster(y_class, cluster_instances, timestamp)\n)\nreturn microclusters\ndef _novelty_detect(self):\nif self.verbose &gt; 0: print(\"Novelty detection started\")\npossible_clusters = []\nX = np.array([instance.point for instance in self.short_mem])\nif self.cluster_algorithm == 'kmeans':\ncluster_clf = KMeans(n_clusters=self.kini, n_init='auto',\nrandom_state=self.random_state)\ncluster_clf.fit(X)\nlabels = cluster_clf.labels_\nelse:\ncluster_clf = CluStream(m=self.kini)\ncluster_clf.init_offline(X, seed=self.random_state)\ncluster_centers = cluster_clf.get_partial_cluster_centers()\nlabels = self._get_closest_clusters(X, cluster_centers)\nfor cluster_label in np.unique(labels):\ncluster_instances = X[labels == cluster_label]\npossible_clusters.append(\nMicroCluster(-1, cluster_instances, self.sample_counter))\nfor cluster in possible_clusters:\nif cluster.is_cohesive(self.microclusters) and cluster.is_representative(self.min_examples_cluster):\nclosest_cluster = cluster.find_closest_cluster(self.microclusters)\nclosest_distance = cluster.distance_to_centroid(closest_cluster.centroid)\nthreshold = self._best_threshold(cluster, closest_cluster,\nself.threshold_strategy)\n# TODO make these ifs elifs cleaner\nif closest_distance &lt;= threshold:  # the new microcluster is an extension\nif self.verbose &gt; 1:\nprint(\"Extension of cluster: \", closest_cluster)\nelif self.verbose &gt; 0:\nprint(\"Extension of cluster: \", closest_cluster.small_str())\ncluster.label = closest_cluster.label\nelif self.sleep_mem:  # look in the sleep memory, if not empty\nclosest_cluster = cluster.find_closest_cluster(self.sleep_mem)\nclosest_distance = cluster.distance_to_centroid(closest_cluster.centroid)\nif closest_distance &lt;= threshold:  # check again: the new microcluster is an extension\nif self.verbose &gt; 1:\nprint(\"Waking cluster: \", closest_cluster)\nelif self.verbose &gt; 0:\nprint(\"Waking cluster: \", closest_cluster.small_str())\ncluster.label = closest_cluster.label\n# awake old cluster\nself.sleep_mem.remove(closest_cluster)\nclosest_cluster.timestamp = self.sample_counter\nself.microclusters.append(closest_cluster)\nelse:  # the new microcluster is a novelty pattern\ncluster.label = max([cluster.label for cluster in self.microclusters]) + 1\nif self.verbose &gt; 1:\nprint(\"Novel cluster: \", cluster)\nelif self.verbose &gt; 0:\nprint(\"Novel cluster: \", cluster.small_str())\nelse:  # the new microcluster is a novelty pattern\ncluster.label = max([cluster.label for cluster in self.microclusters]) + 1\nif self.verbose &gt; 1:\nprint(\"Novel cluster: \", cluster)\nelif self.verbose &gt; 0:\nprint(\"Novel cluster: \", cluster.small_str())\n# add the new cluster to the model\nself.microclusters.append(cluster)\n# remove these examples from short term memory\nfor instance in cluster.instances:\nself.short_mem.remove(instance)\ndef _best_threshold(self, new_cluster, closest_cluster, strategy):\ndef run_strategy_1():\nfactor_1 = self.threshold_factor\n# factor_1 = 5  # good for artificial, separated data sets\nreturn factor_1 * np.std(closest_cluster.distance_to_centroid(closest_cluster.instances))\nif strategy == 1:\nreturn run_strategy_1()\nelse:\nfactor_2 = factor_3 = self.threshold_factor\n# factor_2 = factor_3 = 1.2 # good for artificial, separated data sets\nclusters_same_class = self._get_clusters_in_class(closest_cluster.label)\nif len(clusters_same_class) == 1:\nreturn run_strategy_1()\nelse:\nclass_centroids = np.array([cluster.centroid for cluster in clusters_same_class])\ndistances = closest_cluster.distance_to_centroid(class_centroids)\nif strategy == 2:\nreturn factor_2 * np.max(distances)\nelif strategy == 3:\nreturn factor_3 * np.mean(distances)\ndef _get_closest_clusters(self, X, centroids):   \nif len(centroids) == 0:\nprint(\"No clusters\")\nreturn\ncentroids = np.array(centroids)\nnorm_dists = np.zeros((X.shape[0],centroids.shape[0]))\n# Cut into batches if there are too many samples to save on memory\nfor idx in range(math.ceil(X.shape[0]/Minas.MAX_MEMORY_SIZE)):\nsl = slice(idx*Minas.MAX_MEMORY_SIZE, (idx+1)*Minas.MAX_MEMORY_SIZE)\nnorm_dists[sl] = np.linalg.norm(np.subtract(X[sl, :, None], np.transpose(centroids)), axis=1)\nreturn np.argmin(norm_dists, axis=1)\ndef _get_clusters_in_class(self, label):\nreturn [cluster for cluster in self.microclusters if cluster.label == label]\ndef _trigger_forget(self):\nfor cluster in self.microclusters:\nif cluster.timestamp &lt; self.sample_counter - self.window_size:\nif self.verbose &gt; 1:\nprint(\"Forgetting cluster: \", cluster)\nelif self.verbose &gt; 0:\nprint(\"Forgetting cluster: \", cluster.small_str())\nself.sleep_mem.append(cluster)\nself.microclusters.remove(cluster)\nfor instance in self.short_mem:\nif instance.timestamp &lt; self.sample_counter - self.window_size:\nself.short_mem.remove(instance)\n</code></pre>"},{"location":"reference/model/minas/#streamndr.model.minas.Minas.learn_many","title":"<code>learn_many(X, y, w=1.0)</code>","text":"<p>Represents the offline phase of the algorithm. Receives a number of samples and their given labels and learns all of the known classes.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>pandas.DataFrame or numpy.ndarray</code> <p>Samples to be learned by the model</p> required <code>y</code> <code>list of int</code> <p>Labels corresponding to the given samples, must be the same length as the number of samples</p> required <code>w</code> <code>float, optional</code> <p>Weights, not used, by default 1.0</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Minas</code> <p>Itself</p> Source code in <code>streamndr/model/minas.py</code> <pre><code>def learn_many(self, X, y, w=1.0):\n\"\"\"Represents the offline phase of the algorithm. Receives a number of samples and their given labels and learns all of the known classes.\n    Parameters\n    ----------\n    X : pandas.DataFrame or numpy.ndarray\n        Samples to be learned by the model\n    y : list of int\n        Labels corresponding to the given samples, must be the same length as the number of samples\n    w : float, optional\n        Weights, not used, by default 1.0\n    Returns\n    -------\n    Minas\n        Itself\n    \"\"\"\nif isinstance(X, pd.DataFrame):\nX = X.to_numpy()\nself.microclusters = self._offline(X, y)\nself.before_offline_phase = False\nreturn self\n</code></pre>"},{"location":"reference/model/minas/#streamndr.model.minas.Minas.learn_one","title":"<code>learn_one(x, y, w=1.0)</code>","text":"<p>Function used by river algorithms to learn one sample. It is not applicable to this algorithm since the offline phase requires all samples to arrive at once. It is only added as to follow River's API.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>dict</code> <p>Sample</p> required <code>y</code> <code>int</code> <p>Label of the given sample</p> required <code>w</code> <code>float, optional</code> <p>Weight, not used, by default 1.0</p> <code>1.0</code> Source code in <code>streamndr/model/minas.py</code> <pre><code>def learn_one(self, x, y, w=1.0):\n\"\"\"Function used by river algorithms to learn one sample. It is not applicable to this algorithm since the offline phase requires all samples\n    to arrive at once. It is only added as to follow River's API.\n    Parameters\n    ----------\n    x : dict\n        Sample\n    y : int\n        Label of the given sample\n    w : float, optional\n        Weight, not used, by default 1.0\n    \"\"\"\n# Not applicable\npass\n</code></pre>"},{"location":"reference/model/minas/#streamndr.model.minas.Minas.predict_many","title":"<code>predict_many(X)</code>","text":"<p>Represents the online phase. Receives multiple samples, for each sample predict its label and adds it to the cluster if it is a known class.  Otherwise, if it's unknown, it is added to the short term memory and novelty detection is performed once the trigger has been reached (min_short_mem_trigger).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>pandas.DataFrame or numpy.ndarray</code> <p>Samples</p> required <p>Returns:</p> Type Description <code>numpy.ndarray</code> <p>Array of length len(X) containing the predicted labels, predicts -1 if the corresponding sample labeled as unknown</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the model has not been trained first with learn_many() (offline phase)</p> Source code in <code>streamndr/model/minas.py</code> <pre><code>def predict_many(self, X):\n\"\"\"Represents the online phase. Receives multiple samples, for each sample predict its label and adds it to the cluster if it is a known class. \n    Otherwise, if it's unknown, it is added to the short term memory and novelty detection is performed once the trigger has been reached (min_short_mem_trigger).\n    Parameters\n    ----------\n    X : pandas.DataFrame or numpy.ndarray\n        Samples\n    Returns\n    -------\n    numpy.ndarray\n        Array of length len(X) containing the predicted labels, predicts -1 if the corresponding sample labeled as unknown\n    Raises\n    ------\n    Exception\n        If the model has not been trained first with learn_many() (offline phase)\n    \"\"\"\nif self.before_offline_phase:\nraise Exception(\"Model must be fitted first\")\nif isinstance(X, pd.DataFrame):\nX = X.to_numpy() #Converting DataFrame to numpy array\n# Finding closest clusters for received samples\nclosest_clusters = self._get_closest_clusters(X, [microcluster.centroid for microcluster in self.microclusters])\npred_label = []\nfor i in range(len(closest_clusters)):\nself.sample_counter += 1\nclosest_cluster = self.microclusters[closest_clusters[i]]\nif closest_cluster.encompasses(X[i]):  # classify in this cluster\npred_label.append(closest_cluster.label)\nclosest_cluster.update_cluster(X[i], self.sample_counter, self.update_summary)\nelse:  # classify as unknown\npred_label.append(-1)\nself.short_mem.append(ShortMemInstance(X[i], self.sample_counter))\nif self.verbose &gt; 1:\nprint('Memory length: ', len(self.short_mem))\nelif self.verbose &gt; 0:\nif len(self.short_mem) % 100 == 0: print('Memory length: ', len(self.short_mem))\nif len(self.short_mem) &gt;= self.min_short_mem_trigger:\nself._novelty_detect()\n# forgetting mechanism\nif self.sample_counter % self.window_size == 0:\nself._trigger_forget()\nreturn np.array(pred_label)\n</code></pre>"},{"location":"reference/model/minas/#streamndr.model.minas.Minas.predict_one","title":"<code>predict_one(X)</code>","text":"<p>Represents the online phase. Equivalent to predict_many() with only one sample. Receives only one sample, predict its label and adds  it to the cluster if it is a known class. Otherwise, if it's unknown, it is added to the short term memory and novelty detection is  performed once the trigger has been reached (min_short_mem_trigger).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>dict</code> <p>Sample</p> required <p>Returns:</p> Type Description <code>numpy.ndarray</code> <p>Label predicted for the given sample, predicts -1 if labeled as unknown</p> Source code in <code>streamndr/model/minas.py</code> <pre><code>def predict_one(self, X):\n\"\"\"Represents the online phase. Equivalent to predict_many() with only one sample. Receives only one sample, predict its label and adds \n    it to the cluster if it is a known class. Otherwise, if it's unknown, it is added to the short term memory and novelty detection is \n    performed once the trigger has been reached (min_short_mem_trigger).\n    Parameters\n    ----------\n    X : dict\n        Sample\n    Returns\n    -------\n    numpy.ndarray\n        Label predicted for the given sample, predicts -1 if labeled as unknown\n    \"\"\"\nreturn self.predict_many(np.array(list(X.values()))[None,:])\n</code></pre>"},{"location":"reference/utils/data_structure/","title":"data_structure","text":""},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.MicroCluster","title":"<code>MicroCluster</code>","text":"<p>         Bases: <code>object</code></p> <p>A representation of a cluster with compressed information.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>int</code> <p>Label associated with this microcluster</p> required <code>instances</code> <code>numpy.ndarray</code> <p>Instances in this microcluster, preferably these would not be stored if not needed using keep_instances=False</p> <code>None</code> <code>timestamp</code> <code>int</code> <p>Timestamp this microcluster was last updated, used for forgetting mechanisms</p> <code>0</code> <code>keep_instances</code> <code>bool</code> <p>Whether or not to store the instances within the microcluster. Should preferably set to false, but some implementations require access to the instances</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>n</code> <code>int</code> <p>Number of instances stored in this microcluster</p> <code>linear_sum</code> <code>numpy.ndarray</code> <p>Linear sum of the points belonging to this microcluster</p> <code>squared_sum</code> <code>numpy.ndarray</code> <p>Sum of the squared l2 norms of all samples belonging to this microcluster</p> <code>centroid</code> <code>numpy.ndarray</code> <p>Centroid coordinates of the microcluster</p> <code>max_distance</code> <code>numpy.ndarray</code> <p>Maximum distance between a point belonging to the microcluster and its centroid</p> <code>mean_distance</code> <code>numpy.ndarray</code> <p>Mean distance of the distances between the cluster's points and its centroid</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>class MicroCluster(object):\n\"\"\"A representation of a cluster with compressed information.\n    Parameters\n    ----------\n    label : int\n        Label associated with this microcluster\n    instances : numpy.ndarray\n        Instances in this microcluster, preferably these would not be stored if not needed using keep_instances=False\n    timestamp : int\n        Timestamp this microcluster was last updated, used for forgetting mechanisms  \n    keep_instances : bool\n        Whether or not to store the instances within the microcluster. Should preferably set to false, but some implementations require\n        access to the instances\n    Attributes\n    ----------\n    n : int\n        Number of instances stored in this microcluster\n    linear_sum : numpy.ndarray\n        Linear sum of the points belonging to this microcluster\n    squared_sum : numpy.ndarray\n        Sum of the squared l2 norms of all samples belonging to this microcluster\n    centroid : numpy.ndarray\n        Centroid coordinates of the microcluster\n    max_distance : numpy.ndarray\n        Maximum distance between a point belonging to the microcluster and its centroid\n    mean_distance : numpy.ndarray\n        Mean distance of the distances between the cluster's points and its centroid\n    \"\"\"\ndef __init__(self,\nlabel,  # the class the microcluster belongs to\ninstances=None,\ntimestamp=0, \nkeep_instances=True #Required True for MINAS\n):\n# TODO: remove instances entirely so it doesn't need to be stored in memory; Might not be possible because of _best_threshold used by MINAS which needs instances\nsuper(MicroCluster, self).__init__()\nself.label = label\nself.instances = instances\nself.n = len(instances)\nself.linear_sum = instances.sum(axis=0)\n# Sum of the squared l2 norms of all samples belonging to a microcluster:\nself.squared_sum = np.square(np.linalg.norm(self.instances, axis=1)).sum()\n# self.squared_sum = np.square(instances).sum(axis=0)  # From CluSTREAM paper\nself.centroid = self.linear_sum / self.n\nself.max_distance = np.max(self.distance_to_centroid(instances))\nself.mean_distance = np.mean(self.distance_to_centroid(instances))\nself.timestamp = timestamp\nself.update_properties()\nif not keep_instances:\nself.instances = None\ndef __str__(self):\n\"\"\"Returns string representation of a microcluster.\n        Returns\n        -------\n        str\n            String representation of microcluster\n        \"\"\"\nreturn f\"\"\"Target class {self.label}\n                # of instances: {self.n}\n                Linear sum: {self.linear_sum}\n                Squared sum: {self.squared_sum}\n                Centroid: {self.centroid}\n                Radius: {self.radius}\n                Timestamp of last change: {self.timestamp}\"\"\"\ndef small_str(self):\n\"\"\"Returns string representation of a microcluster.\n        Returns\n        -------\n        str\n            Small string representation of microcluster\n        \"\"\"\nreturn f\"\"\"Target class {self.label}\n                # of instances: {self.n}\n                Timestamp of last change: {self.timestamp}\"\"\"\ndef get_radius(self):\n\"\"\"Returns radius of the microcluster.\n        Returns\n        -------\n        float\n            Radius of the microcluster\n        \"\"\"\nfactor = 1.5\n# from BIRCH Wikipedia\n#diff = (self.squared_sum / self.n) - np.dot(self.centroid, self.centroid)\n#if diff &gt; 1e-15:\n#return factor * np.sqrt(diff)\n#else:  # in this case diff should be zero, but sometimes it's an infinitesimal difference\n#return 0\n# from MINAS paper:\nreturn factor*np.std(self.distance_to_centroid(self.instances))\ndef distance_to_centroid(self, X):\n\"\"\"Returns distance from X to centroid of this cluster.\n        Parameters\n        ----------\n        X : numpy.ndarray\n            Point or multiple points\n        Returns\n        -------\n        numpy.ndarray\n            Distance from X to the microcluster's centroid\n        \"\"\"\nif len(X.shape) == 1:  # X is only one point\nreturn np.linalg.norm(X - self.centroid)\nelse:  # X contains several points\nreturn np.linalg.norm(X - self.centroid, axis=1)\ndef encompasses(self, X):\n\"\"\"Checks if point X is inside this microcluster. The point X is considered within the microcluster if the distance \n        between the point and the microcluster's centroid is less than the radius of the microcluster.\n        Parameters\n        ----------\n        X : numpy.ndarray\n            One point\n        Returns\n        -------\n        bool\n            If the point distance to centroid is contained within the microcluster or not\n        \"\"\"\nreturn np.less(self.distance_to_centroid(X), self.radius)\ndef find_closest_cluster(self, clusters):\n\"\"\"Finds closest microcluster to this one among passed microclusters.\n        Parameters\n        ----------\n        clusters : list of MicroCluster\n        Returns\n        -------\n        MicroCluster\n            Closest microcluster\n        \"\"\"\nreturn min(clusters, key=lambda cl: cl.distance_to_centroid(self.centroid))\ndef update_cluster(self, X, timestamp, update_summary):\n\"\"\"Adds point received in parameter to the cluster and update cluster's centroid if wanted.\n        Parameters\n        ----------\n        X : numpy.ndarray\n            One point\n        timestamp : int\n            Timestamp when this point was added to this microcluster\n        update_summary : bool\n            Whether or not to update the microcluster properties with this new point\n        \"\"\"\nassert len(X.shape) == 1  # it's just one point\nself.timestamp = timestamp\nif self.instances is not None:\nself.instances = np.append(self.instances, [X],\naxis=0)\nif update_summary:\nself.mean_distance = (self.n * self.mean_distance + self.distance_to_centroid(X)) / (self.n + 1)\nself.n += 1\nself.linear_sum = np.sum([self.linear_sum, X], axis=0)\nself.squared_sum = np.sum([self.squared_sum, np.square(X)], axis=0)\nself.update_properties()\ndef update_properties(self):\n\"\"\"Updates centroid and radius based on current cluster properties.\"\"\"\nself.centroid = self.linear_sum / self.n\nif self.instances is not None:\nself.radius = self.get_radius()\nif np.max(self.distance_to_centroid(self.instances)) &gt; self.max_distance:\nself.max_distance = np.max(self.distance_to_centroid(self.instances))\ndef is_cohesive(self, clusters):\n\"\"\"Verifies if this cluster is cohesive for novelty detection purposes.\n        A new micro-cluster is cohesive if its silhouette coefficient is larger than 0.\n        'b' represents the Euclidean distance between the centroid of the new micro-cluster and the centroid of its\n        closest micro-cluster, and 'a' represents the standard deviation of the distances between the examples of the\n        new micro-cluster and the centroid of the new micro-cluster.\n        Parameters\n        ----------\n        clusters : List of MicroCluster\n            Existing known micro-clusters\n        Returns\n        -------\n        bool\n            If the cluster is cohesive (silhouette&gt;0) or not\n        \"\"\"\nb = self.distance_to_centroid(self.find_closest_cluster(clusters).centroid)\na = np.std(self.distance_to_centroid(self.instances))\nsilhouette = (b - a) / max(a, b)  # hm, this is always positive if b &gt; a\nreturn silhouette &gt; 0\ndef is_representative(self, min_examples):\n\"\"\"Verifies if this cluster is representative for novelty detection purposes.\n        A new micro-cluster is representative if it contains a minimal number of examples,\n        where this number is a user-defined parameter.\n        Parameters\n        ----------\n        min_examples : int\n            The number of samples the microcluster needs to have to be considered representative.\n        Returns\n        -------\n        bool\n            If the cluster is representative or not\n        \"\"\"\nreturn self.n &gt;= min_examples\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.MicroCluster.__str__","title":"<code>__str__()</code>","text":"<p>Returns string representation of a microcluster.</p> <p>Returns:</p> Type Description <code>str</code> <p>String representation of microcluster</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def __str__(self):\n\"\"\"Returns string representation of a microcluster.\n    Returns\n    -------\n    str\n        String representation of microcluster\n    \"\"\"\nreturn f\"\"\"Target class {self.label}\n            # of instances: {self.n}\n            Linear sum: {self.linear_sum}\n            Squared sum: {self.squared_sum}\n            Centroid: {self.centroid}\n            Radius: {self.radius}\n            Timestamp of last change: {self.timestamp}\"\"\"\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.MicroCluster.distance_to_centroid","title":"<code>distance_to_centroid(X)</code>","text":"<p>Returns distance from X to centroid of this cluster.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy.ndarray</code> <p>Point or multiple points</p> required <p>Returns:</p> Type Description <code>numpy.ndarray</code> <p>Distance from X to the microcluster's centroid</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def distance_to_centroid(self, X):\n\"\"\"Returns distance from X to centroid of this cluster.\n    Parameters\n    ----------\n    X : numpy.ndarray\n        Point or multiple points\n    Returns\n    -------\n    numpy.ndarray\n        Distance from X to the microcluster's centroid\n    \"\"\"\nif len(X.shape) == 1:  # X is only one point\nreturn np.linalg.norm(X - self.centroid)\nelse:  # X contains several points\nreturn np.linalg.norm(X - self.centroid, axis=1)\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.MicroCluster.encompasses","title":"<code>encompasses(X)</code>","text":"<p>Checks if point X is inside this microcluster. The point X is considered within the microcluster if the distance  between the point and the microcluster's centroid is less than the radius of the microcluster.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy.ndarray</code> <p>One point</p> required <p>Returns:</p> Type Description <code>bool</code> <p>If the point distance to centroid is contained within the microcluster or not</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def encompasses(self, X):\n\"\"\"Checks if point X is inside this microcluster. The point X is considered within the microcluster if the distance \n    between the point and the microcluster's centroid is less than the radius of the microcluster.\n    Parameters\n    ----------\n    X : numpy.ndarray\n        One point\n    Returns\n    -------\n    bool\n        If the point distance to centroid is contained within the microcluster or not\n    \"\"\"\nreturn np.less(self.distance_to_centroid(X), self.radius)\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.MicroCluster.find_closest_cluster","title":"<code>find_closest_cluster(clusters)</code>","text":"<p>Finds closest microcluster to this one among passed microclusters.</p> <p>Parameters:</p> Name Type Description Default <code>clusters</code> <code>list of MicroCluster</code> required <p>Returns:</p> Type Description <code>MicroCluster</code> <p>Closest microcluster</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def find_closest_cluster(self, clusters):\n\"\"\"Finds closest microcluster to this one among passed microclusters.\n    Parameters\n    ----------\n    clusters : list of MicroCluster\n    Returns\n    -------\n    MicroCluster\n        Closest microcluster\n    \"\"\"\nreturn min(clusters, key=lambda cl: cl.distance_to_centroid(self.centroid))\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.MicroCluster.get_radius","title":"<code>get_radius()</code>","text":"<p>Returns radius of the microcluster.</p> <p>Returns:</p> Type Description <code>float</code> <p>Radius of the microcluster</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def get_radius(self):\n\"\"\"Returns radius of the microcluster.\n    Returns\n    -------\n    float\n        Radius of the microcluster\n    \"\"\"\nfactor = 1.5\n# from BIRCH Wikipedia\n#diff = (self.squared_sum / self.n) - np.dot(self.centroid, self.centroid)\n#if diff &gt; 1e-15:\n#return factor * np.sqrt(diff)\n#else:  # in this case diff should be zero, but sometimes it's an infinitesimal difference\n#return 0\n# from MINAS paper:\nreturn factor*np.std(self.distance_to_centroid(self.instances))\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.MicroCluster.is_cohesive","title":"<code>is_cohesive(clusters)</code>","text":"<p>Verifies if this cluster is cohesive for novelty detection purposes. A new micro-cluster is cohesive if its silhouette coefficient is larger than 0. 'b' represents the Euclidean distance between the centroid of the new micro-cluster and the centroid of its closest micro-cluster, and 'a' represents the standard deviation of the distances between the examples of the new micro-cluster and the centroid of the new micro-cluster.</p> <p>Parameters:</p> Name Type Description Default <code>clusters</code> <code>List of MicroCluster</code> <p>Existing known micro-clusters</p> required <p>Returns:</p> Type Description <code>bool</code> <p>If the cluster is cohesive (silhouette&gt;0) or not</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def is_cohesive(self, clusters):\n\"\"\"Verifies if this cluster is cohesive for novelty detection purposes.\n    A new micro-cluster is cohesive if its silhouette coefficient is larger than 0.\n    'b' represents the Euclidean distance between the centroid of the new micro-cluster and the centroid of its\n    closest micro-cluster, and 'a' represents the standard deviation of the distances between the examples of the\n    new micro-cluster and the centroid of the new micro-cluster.\n    Parameters\n    ----------\n    clusters : List of MicroCluster\n        Existing known micro-clusters\n    Returns\n    -------\n    bool\n        If the cluster is cohesive (silhouette&gt;0) or not\n    \"\"\"\nb = self.distance_to_centroid(self.find_closest_cluster(clusters).centroid)\na = np.std(self.distance_to_centroid(self.instances))\nsilhouette = (b - a) / max(a, b)  # hm, this is always positive if b &gt; a\nreturn silhouette &gt; 0\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.MicroCluster.is_representative","title":"<code>is_representative(min_examples)</code>","text":"<p>Verifies if this cluster is representative for novelty detection purposes. A new micro-cluster is representative if it contains a minimal number of examples, where this number is a user-defined parameter.</p> <p>Parameters:</p> Name Type Description Default <code>min_examples</code> <code>int</code> <p>The number of samples the microcluster needs to have to be considered representative.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>If the cluster is representative or not</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def is_representative(self, min_examples):\n\"\"\"Verifies if this cluster is representative for novelty detection purposes.\n    A new micro-cluster is representative if it contains a minimal number of examples,\n    where this number is a user-defined parameter.\n    Parameters\n    ----------\n    min_examples : int\n        The number of samples the microcluster needs to have to be considered representative.\n    Returns\n    -------\n    bool\n        If the cluster is representative or not\n    \"\"\"\nreturn self.n &gt;= min_examples\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.MicroCluster.small_str","title":"<code>small_str()</code>","text":"<p>Returns string representation of a microcluster.</p> <p>Returns:</p> Type Description <code>str</code> <p>Small string representation of microcluster</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def small_str(self):\n\"\"\"Returns string representation of a microcluster.\n    Returns\n    -------\n    str\n        Small string representation of microcluster\n    \"\"\"\nreturn f\"\"\"Target class {self.label}\n            # of instances: {self.n}\n            Timestamp of last change: {self.timestamp}\"\"\"\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.MicroCluster.update_cluster","title":"<code>update_cluster(X, timestamp, update_summary)</code>","text":"<p>Adds point received in parameter to the cluster and update cluster's centroid if wanted.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy.ndarray</code> <p>One point</p> required <code>timestamp</code> <code>int</code> <p>Timestamp when this point was added to this microcluster</p> required <code>update_summary</code> <code>bool</code> <p>Whether or not to update the microcluster properties with this new point</p> required Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def update_cluster(self, X, timestamp, update_summary):\n\"\"\"Adds point received in parameter to the cluster and update cluster's centroid if wanted.\n    Parameters\n    ----------\n    X : numpy.ndarray\n        One point\n    timestamp : int\n        Timestamp when this point was added to this microcluster\n    update_summary : bool\n        Whether or not to update the microcluster properties with this new point\n    \"\"\"\nassert len(X.shape) == 1  # it's just one point\nself.timestamp = timestamp\nif self.instances is not None:\nself.instances = np.append(self.instances, [X],\naxis=0)\nif update_summary:\nself.mean_distance = (self.n * self.mean_distance + self.distance_to_centroid(X)) / (self.n + 1)\nself.n += 1\nself.linear_sum = np.sum([self.linear_sum, X], axis=0)\nself.squared_sum = np.sum([self.squared_sum, np.square(X)], axis=0)\nself.update_properties()\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.MicroCluster.update_properties","title":"<code>update_properties()</code>","text":"<p>Updates centroid and radius based on current cluster properties.</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def update_properties(self):\n\"\"\"Updates centroid and radius based on current cluster properties.\"\"\"\nself.centroid = self.linear_sum / self.n\nif self.instances is not None:\nself.radius = self.get_radius()\nif np.max(self.distance_to_centroid(self.instances)) &gt; self.max_distance:\nself.max_distance = np.max(self.distance_to_centroid(self.instances))\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.ShortMemInstance","title":"<code>ShortMemInstance</code>","text":"<p>Instance of a point associated with a timestamp. Used for the buffer memory which stores the unknown samples.</p> <p>Attributes:</p> Name Type Description <code>point</code> <code>numpy.ndarray</code> <p>The coordinates of the point</p> <code>timestamp</code> <code>int</code> <p>The timestamp the point was added/treated</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>class ShortMemInstance:\n\"\"\"Instance of a point associated with a timestamp. Used for the buffer memory which stores the unknown samples.\n    Attributes\n    ----------\n    point : numpy.ndarray\n        The coordinates of the point\n    timestamp : int\n        The timestamp the point was added/treated\n    \"\"\"\ndef __init__(self, point, timestamp):\nself.point = point\nself.timestamp = timestamp\ndef __eq__(self, other):\n\"\"\"Elements are equal if they have the same values for all variables.\n        This currently does not consider the timestamp.\n        Parameters\n        ----------\n        other : ShortMemInstance\n            Other instance to compared to\n        Returns\n        -------\n        bool\n            If the instances are equals or not\n        \"\"\"\nif type(other) == np.ndarray:\nreturn np.all(self.point == other)\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.ShortMemInstance.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Elements are equal if they have the same values for all variables. This currently does not consider the timestamp.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>ShortMemInstance</code> <p>Other instance to compared to</p> required <p>Returns:</p> Type Description <code>bool</code> <p>If the instances are equals or not</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def __eq__(self, other):\n\"\"\"Elements are equal if they have the same values for all variables.\n    This currently does not consider the timestamp.\n    Parameters\n    ----------\n    other : ShortMemInstance\n        Other instance to compared to\n    Returns\n    -------\n    bool\n        If the instances are equals or not\n    \"\"\"\nif type(other) == np.ndarray:\nreturn np.all(self.point == other)\n</code></pre>"}]}