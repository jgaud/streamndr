{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"StreamNDR","text":"<p>This site contains the documentation for <code>StreamNDR</code>, a Python library based on river which aims to implement novelty detection algorithm for data streams. The library is open-source and availabble on Github.</p>"},{"location":"#what-is-novelty-detection","title":"What is novelty detection?","text":"<p>Novelty Detection consists of the task of detecting novelty concepts (or classes) in a data stream, that is, classes that the model has not seen before. In order to do so, the algorithms often implement an offline phase, where ther learn the known classes in supervised manner, followed by an online phase, where the algorithm will try to label and detect novel classes within the stream of data. </p>"},{"location":"getting_started/","title":"Getting Started","text":""},{"location":"getting_started/#installation","title":"\ud83d\udee0 Installation","text":"<p>Note: StreamNDR is intended to be used with Python 3.6 or above and requires the package ClusOpt-Core which requires a C/C++ compiler (such as gcc) and the Boost.Thread library to build. To install the Boost.Thread library on Debian systems, the following command can be used:</p> <pre><code>sudo apt install libboost-thread-dev\n</code></pre> <p>The package can be installed simply with <code>pip</code> : <pre><code>pip install streamndr\n</code></pre></p>"},{"location":"getting_started/#quickstart","title":"\u26a1\ufe0f Quickstart","text":"<p>As a quick example, we'll train two models (MINAS and ECSMiner-WF) to classify a synthetic dataset created using RandomRBF. The models are trained on only two of the four generated classes ([0,1]) and will try to detect the other classes ([2,3]) as novelty patterns in the dataset in an online fashion.</p> <p>Let's first generate the dataset. <pre><code>import numpy as np\nfrom river.datasets import synth\n\nds = synth.RandomRBF(seed_model=42, seed_sample=42, n_classes=4, n_features=5, n_centroids=10)\n\noffline_size = 1000\nonline_size = 5000\nX_train = []\ny_train = []\nX_test = []\ny_test = []\n\nfor x,y in ds.take(10*(offline_size+online_size)):\n\n    #Create our training data (known classes)\n    if len(y_train) &lt; offline_size:\n        if y == 0 or y == 1: #Only showing two first classes in the training set\n            X_train.append(np.array(list(x.values())))\n            y_train.append(y)\n\n    #Create our online stream of data\n    elif len(y_test) &lt; online_size:\n        X_test.append(x)\n        y_test.append(y)\n\n    else:\n        break\n\nX_train = np.array(X_train)\ny_train = np.array(y_train)\n</code></pre></p>"},{"location":"getting_started/#minas","title":"MINAS","text":"<p>Let's train our MINAS model on the offline (known) data. <pre><code>from streamndr.model import Minas\nclf = Minas(kini=100, cluster_algorithm='clustream', \n            window_size=600, threshold_strategy=1, threshold_factor=1.1, \n            min_short_mem_trigger=100, min_examples_cluster=20, verbose=1, random_state=42)\n\nclf.learn_many(np.array(X_train), np.array(y_train)) #learn_many expects numpy arrays or pandas dataframes\n</code></pre></p> <p>Let's now test our algorithm in an online fashion, note that our unsupervised clusters are automatically updated with the call to <code>predict_one</code>.</p> <pre><code>from streamndr.metrics import ConfusionMatrixNovelty, MNew, FNew, ErrRate\n\nknown_classes = [0,1]\n\nconf_matrix = ConfusionMatrixNovelty(known_classes)\nm_new = MNew(known_classes)\nf_new = FNew(known_classes)\nerr_rate = ErrRate(known_classes)\n\ni = 1\nfor x, y_true in zip(X_test, y_test):\n\n    y_pred = clf.predict_one(x) #predict_one takes python dictionaries as per River API\n\n    if y_pred is not None: #Update our metrics\n        conf_matrix.update(y_true, y_pred[0])\n        m_new.update(y_true, y_pred[0])\n        f_new.update(y_true, y_pred[0])\n        err_rate.update(y_true, y_pred[0])\n\n\n    #Show progress\n    if i % 100 == 0:\n        print(f\"{i}/{len(X_test)}\")\n    i += 1\n</code></pre> <p>Let's look at the results, of course, the hyperparameters of the model can be tuned to get better results.</p> <p><pre><code>#print(conf_matrix) #Shows the confusion matrix of the given problem, can be very wide due to one class being detected as multiple Novelty Patterns\nprint(m_new) #Percentage of novel class instances misclassified as known.\nprint(f_new) #Percentage of known classes misclassified as novel.\nprint(err_rate) #Total misclassification error percentage\n</code></pre> MNew: 17.15%  FNew: 40.11%  ErrRate: 36.80% </p>"},{"location":"getting_started/#ecsminer-wf","title":"ECSMiner-WF","text":"<p>Let's train our model on the offline (known) data.</p> <p><pre><code>from streamndr.model import ECSMinerWF\nclf = ECSMinerWF(K=50, min_examples_cluster=10, verbose=1, random_state=42, ensemble_size=7, init_algorithm=\"kmeans\")\nclf.learn_many(np.array(X_train), np.array(y_train))\n</code></pre> Once again, let's use our model in an online fashion. <pre><code>conf_matrix = ConfusionMatrixNovelty(known_classes)\nm_new = MNew(known_classes)\nf_new = FNew(known_classes)\nerr_rate = ErrRate(known_classes)\n\nfor x, y_true in zip(X_test, y_test):\n\n    y_pred = clf.predict_one(x) #predict_one takes python dictionaries as per River API\n\n    if y_pred is not None: #Update our metrics\n        conf_matrix.update(y_true, y_pred[0])\n        m_new.update(y_true, y_pred[0])\n        f_new.update(y_true, y_pred[0])\n        err_rate.update(y_true, y_pred[0])\n</code></pre></p> <pre><code>#print(conf_matrix) #Shows the confusion matrix of the given problem, can be very wide due to one class being detected as multiple Novelty Patterns\nprint(m_new) #Percentage of novel class instances misclassified as known.\nprint(f_new) #Percentage of known classes misclassified as novel.\nprint(err_rate) #Total misclassification error percentage\n</code></pre> <p>MNew: 28.98%  FNew: 30.26%  ErrRate: 32.40% </p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>metrics<ul> <li>cer</li> <li>confusion</li> <li>err_rate</li> <li>f_new</li> <li>m_new</li> <li>ttd</li> </ul> </li> <li>model<ul> <li>ecsminer</li> <li>ecsminerwf</li> <li>minas</li> </ul> </li> <li>utils<ul> <li>cluster_utils</li> <li>data_structure</li> <li>mcikmeans</li> </ul> </li> </ul>"},{"location":"reference/metrics/cer/","title":"cer","text":""},{"location":"reference/metrics/cer/#streamndr.metrics.cer.CER","title":"<code>CER</code>","text":"<p>             Bases: <code>MultiClassMetric</code></p> <p>Combined Error Rate (CER), defined as the average of the weighted rate of false positive and false negative per class [1].</p> <p>[1] E. R. Faria, I. J. C. R. Gon\u00e7alves, J. Gama and A. C. P. L. F. Carvalho, \"Evaluation Methodology for Multiclass Novelty Detection Algorithms,\"      2013 Brazilian Conference on Intelligent Systems, Fortaleza, Brazil, 2013, pp. 19-25, doi: 10.1109/BRACIS.2013.12.</p> <p>Parameters:</p> Name Type Description Default <code>known_classes</code> <code>list of int</code> <p>List of known labels, the labels the algorithm knows prior to the online phase</p> required <code>cm</code> <code>ConfusionMatrixNovelty</code> <p>Optional, can specify an already existing confusion matrix instead of creating a new one for the metric</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>cm</code> <code>ConfusionMatrixNovelty</code> <p>Confusion matrix</p> Source code in <code>streamndr/metrics/cer.py</code> <pre><code>class CER(metrics.base.MultiClassMetric):\n    \"\"\"Combined Error Rate (CER), defined as the average of the weighted rate of false positive and false negative per class [1].\n\n    [1] E. R. Faria, I. J. C. R. Gon\u00e7alves, J. Gama and A. C. P. L. F. Carvalho, \"Evaluation Methodology for Multiclass Novelty Detection Algorithms,\" \n        2013 Brazilian Conference on Intelligent Systems, Fortaleza, Brazil, 2013, pp. 19-25, doi: 10.1109/BRACIS.2013.12.\n\n    Parameters\n    ----------\n    known_classes : list of int\n        List of known labels, the labels the algorithm knows prior to the online phase\n    cm : ConfusionMatrixNovelty\n        Optional, can specify an already existing confusion matrix instead of creating a new one for the metric\n\n    Attributes\n    ----------\n    cm : ConfusionMatrixNovelty\n        Confusion matrix\n    \"\"\"\n    def __init__(self, known_classes, cm: ConfusionMatrixNovelty = None):\n        if cm is None:\n            cm = ConfusionMatrixNovelty(known_classes)\n\n        super(metrics.base.MultiClassMetric, self).__init__(cm)\n\n    def get(self):\n        associated_classes_conf_matrix = self.cm.get_associated_classes()\n        total = 0\n\n        for c in self.cm.classes:\n            try:\n                fp = associated_classes_conf_matrix.false_positives(c)\n                tn = associated_classes_conf_matrix.true_negatives(c)\n                fn = associated_classes_conf_matrix.false_negatives(c)\n                tp = associated_classes_conf_matrix.true_positives(c)\n\n                fpr = fp / (fp + tn)\n                fnr = fn / (fn + tp)\n\n                w = associated_classes_conf_matrix.support(c) / associated_classes_conf_matrix.total_weight\n\n                total += (w * fpr) + (w * fnr)\n\n            except ZeroDivisionError:\n                continue\n\n\n        return total / 2\n\n    def get_aic(self):\n        \"\"\"Computes the Akaike Information Criterion (AIC) as defined in [1], which measures the complexity of the model using the CER and the number of classes detected.\n\n        Returns\n        -------\n        float\n            The Akaike Information Criterion (AIC)\n        \"\"\"\n        all_classes = self.cm.classes\n\n        if -1 in all_classes:\n            all_classes.remove(-1)\n\n        num_classes_detected = 0\n        N = 0\n\n        for cl in all_classes:\n            N += self.cm.sum_col[cl]\n            if self.cm.sum_col[cl] &gt; 0:\n                num_classes_detected += 1\n\n        try:\n            return -2 * math.log(1-self.get()) + 2 * num_classes_detected / math.log(N)\n        except:\n            return 0.0\n</code></pre>"},{"location":"reference/metrics/cer/#streamndr.metrics.cer.CER.get_aic","title":"<code>get_aic()</code>","text":"<p>Computes the Akaike Information Criterion (AIC) as defined in [1], which measures the complexity of the model using the CER and the number of classes detected.</p> <p>Returns:</p> Type Description <code>float</code> <p>The Akaike Information Criterion (AIC)</p> Source code in <code>streamndr/metrics/cer.py</code> <pre><code>def get_aic(self):\n    \"\"\"Computes the Akaike Information Criterion (AIC) as defined in [1], which measures the complexity of the model using the CER and the number of classes detected.\n\n    Returns\n    -------\n    float\n        The Akaike Information Criterion (AIC)\n    \"\"\"\n    all_classes = self.cm.classes\n\n    if -1 in all_classes:\n        all_classes.remove(-1)\n\n    num_classes_detected = 0\n    N = 0\n\n    for cl in all_classes:\n        N += self.cm.sum_col[cl]\n        if self.cm.sum_col[cl] &gt; 0:\n            num_classes_detected += 1\n\n    try:\n        return -2 * math.log(1-self.get()) + 2 * num_classes_detected / math.log(N)\n    except:\n        return 0.0\n</code></pre>"},{"location":"reference/metrics/confusion/","title":"confusion","text":""},{"location":"reference/metrics/confusion/#streamndr.metrics.confusion.ConfusionMatrixNovelty","title":"<code>ConfusionMatrixNovelty</code>","text":"<p>             Bases: <code>ConfusionMatrix</code></p> <p>Confusion Matrix for novelty detection in data streams.</p> <p>Parameters:</p> Name Type Description Default <code>known_classes</code> <code>list of int</code> <p>List of known labels, the labels the algorithm knows prior to the online phase</p> required <p>Attributes:</p> Name Type Description <code>novel_cm</code> <code>ConfusionMatrix</code> <p>Binary confusion matrix representing the problem in a binary manner, including class 0 (known) and class 1 (novelty)</p> <code>nc_samples</code> <code>int</code> <p>Number of samples representing a novelty</p> <code>fe</code> <code>int</code> <p>Known samples that have been classified as a known class other than its ground truth</p> Source code in <code>streamndr/metrics/confusion.py</code> <pre><code>class ConfusionMatrixNovelty(metrics.confusion.ConfusionMatrix):\n    \"\"\"Confusion Matrix for novelty detection in data streams.\n\n    Parameters\n    ----------\n    known_classes : list of int\n        List of known labels, the labels the algorithm knows prior to the online phase\n\n    Attributes\n    ----------\n    novel_cm : river.metrics.Confusion.ConfusionMatrix\n        Binary confusion matrix representing the problem in a binary manner, including class 0 (known) and class 1 (novelty)\n    nc_samples : int\n        Number of samples representing a novelty\n    fe : int\n        Known samples that have been classified as a known class other than its ground truth\n    \"\"\"\n    def __init__(self, known_classes):\n        super().__init__(known_classes)\n        self.novel_cm = metrics.confusion.ConfusionMatrix()\n        self.nc_samples = 0\n        self.fe = 0\n\n    def get_associated_classes(self):\n        \"\"\"Computes the associated known class to each novelty pattern discovered, as described in [1], by using the real class most represented in each novelty pattern.\n        Ignores the unknown samples (label -1).\n\n        [1] E. R. Faria, I. J. C. R. Gon\u00e7alves, J. Gama and A. C. P. L. F. Carvalho, \"Evaluation Methodology for Multiclass Novelty Detection Algorithms,\" \n        2013 Brazilian Conference on Intelligent Systems, Fortaleza, Brazil, 2013, pp. 19-25, doi: 10.1109/BRACIS.2013.12.\n\n        Returns\n        -------\n        ConfusionMatrixNovelty\n            The confusion matrix using the most represented known class for each of the novelty pattern reported\n        \"\"\"\n        associated_classes_conf_matrix = ConfusionMatrixNovelty(self._init_classes)\n        unknown_classes = [x for x in self.classes if x not in self._init_classes.union({-1})]\n\n        for cl in self.classes:\n            col = [int(self.data[row][cl]) for row in self.classes]\n\n            #If the class is a novelty pattern, we select the real class most represented within the novelty pattern\n            if cl in unknown_classes:\n                index_max = col.index(max(col))\n                pred = self.classes[index_max]\n\n                for row in self.classes:\n                    associated_classes_conf_matrix.update(row, pred, self.data[row][cl])\n            elif cl != -1:\n                for row in self.classes:\n                    associated_classes_conf_matrix.update(row, cl, self.data[row][cl])\n\n        return associated_classes_conf_matrix\n\n\n    def update(self, y_true, y_pred, w=1.0):\n        super().update(y_true, y_pred, w)\n\n        known_class = int(y_true in self._init_classes)\n        pred_known_class = int(y_pred in self._init_classes)\n\n        if known_class == 0:\n            self.nc_samples += 1\n        elif known_class == pred_known_class == 1 and y_true != y_pred: #If the prediction is not a novelty, but we predicted the wrong class\n            self.fe += 1\n\n        self.novel_cm.update(1-known_class, 1-pred_known_class)\n\n    def revert(self, y_true, y_pred, w=1.0):\n        super.revert(self, y_true, y_pred, w)\n\n        known_class = int(y_true in self._init_classes)\n        pred_known_class = int(y_pred in self._init_classes)\n\n        if known_class == 1:\n            self.nc_samples -= 1\n            if pred_known_class == 1 and y_true != y_pred:\n                self.fe -= 1\n\n        self.novel_cm.revert(1-known_class, 1-pred_known_class)\n\n    def true_positives_novelty(self):\n        return self.novel_cm.true_positives(1)\n\n    def true_negatives_novelty(self):\n        return self.novel_cm.true_negatives(1)\n\n    def false_positives_novelty(self):\n        return self.novel_cm.false_positives(1)\n\n    def false_negatives_novelty(self):\n        return self.novel_cm.false_negatives(1)\n</code></pre>"},{"location":"reference/metrics/confusion/#streamndr.metrics.confusion.ConfusionMatrixNovelty.get_associated_classes","title":"<code>get_associated_classes()</code>","text":"<p>Computes the associated known class to each novelty pattern discovered, as described in [1], by using the real class most represented in each novelty pattern. Ignores the unknown samples (label -1).</p> <p>[1] E. R. Faria, I. J. C. R. Gon\u00e7alves, J. Gama and A. C. P. L. F. Carvalho, \"Evaluation Methodology for Multiclass Novelty Detection Algorithms,\"  2013 Brazilian Conference on Intelligent Systems, Fortaleza, Brazil, 2013, pp. 19-25, doi: 10.1109/BRACIS.2013.12.</p> <p>Returns:</p> Type Description <code>ConfusionMatrixNovelty</code> <p>The confusion matrix using the most represented known class for each of the novelty pattern reported</p> Source code in <code>streamndr/metrics/confusion.py</code> <pre><code>def get_associated_classes(self):\n    \"\"\"Computes the associated known class to each novelty pattern discovered, as described in [1], by using the real class most represented in each novelty pattern.\n    Ignores the unknown samples (label -1).\n\n    [1] E. R. Faria, I. J. C. R. Gon\u00e7alves, J. Gama and A. C. P. L. F. Carvalho, \"Evaluation Methodology for Multiclass Novelty Detection Algorithms,\" \n    2013 Brazilian Conference on Intelligent Systems, Fortaleza, Brazil, 2013, pp. 19-25, doi: 10.1109/BRACIS.2013.12.\n\n    Returns\n    -------\n    ConfusionMatrixNovelty\n        The confusion matrix using the most represented known class for each of the novelty pattern reported\n    \"\"\"\n    associated_classes_conf_matrix = ConfusionMatrixNovelty(self._init_classes)\n    unknown_classes = [x for x in self.classes if x not in self._init_classes.union({-1})]\n\n    for cl in self.classes:\n        col = [int(self.data[row][cl]) for row in self.classes]\n\n        #If the class is a novelty pattern, we select the real class most represented within the novelty pattern\n        if cl in unknown_classes:\n            index_max = col.index(max(col))\n            pred = self.classes[index_max]\n\n            for row in self.classes:\n                associated_classes_conf_matrix.update(row, pred, self.data[row][cl])\n        elif cl != -1:\n            for row in self.classes:\n                associated_classes_conf_matrix.update(row, cl, self.data[row][cl])\n\n    return associated_classes_conf_matrix\n</code></pre>"},{"location":"reference/metrics/err_rate/","title":"err_rate","text":""},{"location":"reference/metrics/err_rate/#streamndr.metrics.err_rate.ErrRate","title":"<code>ErrRate</code>","text":"<p>             Bases: <code>MultiClassMetric</code></p> <p>Error rate, represents the total misclassification error percentage.</p> <p>Parameters:</p> Name Type Description Default <code>known_classes</code> <code>list of int</code> <p>List of known labels, the labels the algorithm knows prior to the online phase</p> required <code>cm</code> <code>ConfusionMatrixNovelty</code> <p>Optional, can specify an already existing confusion matrix instead of creating a new one for the metric</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>cm</code> <code>ConfusionMatrixNovelty</code> <p>Confusion matrix</p> Source code in <code>streamndr/metrics/err_rate.py</code> <pre><code>class ErrRate(metrics.base.MultiClassMetric):\n    \"\"\"Error rate, represents the total misclassification error percentage.\n\n    Parameters\n    ----------\n    known_classes : list of int\n        List of known labels, the labels the algorithm knows prior to the online phase\n    cm : ConfusionMatrixNovelty\n        Optional, can specify an already existing confusion matrix instead of creating a new one for the metric\n\n    Attributes\n    ----------\n    cm : ConfusionMatrixNovelty\n        Confusion matrix\n    \"\"\"\n    def __init__(self, known_classes, cm: ConfusionMatrixNovelty = None):\n        if cm is None:\n            cm = ConfusionMatrixNovelty(known_classes)\n\n        super(metrics.base.MultiClassMetric, self).__init__(cm)\n\n    def get(self):\n        fp = self.cm.false_positives_novelty() #Number of known class samples wrongly classified as novelties\n        fn = self.cm.false_negatives_novelty() #Number of novelties wrongly classified as known\n\n        try:\n            return (fp + fn + self.cm.fe) / self.cm.n_samples\n        except ZeroDivisionError:\n            return 0.0\n</code></pre>"},{"location":"reference/metrics/f_new/","title":"f_new","text":""},{"location":"reference/metrics/f_new/#streamndr.metrics.f_new.FNew","title":"<code>FNew</code>","text":"<p>             Bases: <code>MultiClassMetric</code></p> <p>Metric F_new, which represents the percentage of known classes misclassified as novel.</p> <p>Parameters:</p> Name Type Description Default <code>known_classes</code> <code>list of int</code> <p>List of known labels, the labels the algorithm knows prior to the online phase</p> required <code>cm</code> <code>ConfusionMatrixNovelty</code> <p>Optional, can specify an already existing confusion matrix instead of creating a new one for the metric</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>cm</code> <code>ConfusionMatrixNovelty</code> <p>Confusion matrix</p> Source code in <code>streamndr/metrics/f_new.py</code> <pre><code>class FNew(metrics.base.MultiClassMetric):\n    \"\"\"Metric F_new, which represents the percentage of known classes misclassified as novel.\n\n    Parameters\n    ----------\n    known_classes : list of int\n        List of known labels, the labels the algorithm knows prior to the online phase\n    cm : ConfusionMatrixNovelty\n        Optional, can specify an already existing confusion matrix instead of creating a new one for the metric\n\n    Attributes\n    ----------\n    cm : ConfusionMatrixNovelty\n        Confusion matrix\n    \"\"\"\n    def __init__(self, known_classes, cm: ConfusionMatrixNovelty = None):\n        if cm is None:\n            cm = ConfusionMatrixNovelty(known_classes)\n\n        super(metrics.base.MultiClassMetric, self).__init__(cm)\n\n    def get(self):\n        fp = self.cm.false_positives_novelty() #Number of known class samples wrongly classified as novelties\n\n        try:\n            return fp / (self.cm.n_samples - self.cm.nc_samples)\n        except ZeroDivisionError:\n            return 0.0\n</code></pre>"},{"location":"reference/metrics/m_new/","title":"m_new","text":""},{"location":"reference/metrics/m_new/#streamndr.metrics.m_new.MNew","title":"<code>MNew</code>","text":"<p>             Bases: <code>MultiClassMetric</code></p> <p>Metric M_new, which represents the percentage of novel class instances misclassified as known.</p> <p>Parameters:</p> Name Type Description Default <code>known_classes</code> <code>list of int</code> <p>List of known labels, the labels the algorithm knows prior to the online phase</p> required <code>cm</code> <code>ConfusionMatrixNovelty</code> <p>Optional, can specify an already existing confusion matrix instead of creating a new one for the metric</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>cm</code> <code>ConfusionMatrixNovelty</code> <p>Confusion matrix</p> Source code in <code>streamndr/metrics/m_new.py</code> <pre><code>class MNew(metrics.base.MultiClassMetric):\n    \"\"\"Metric M_new, which represents the percentage of novel class instances misclassified as known.\n\n    Parameters\n    ----------\n    known_classes : list of int\n        List of known labels, the labels the algorithm knows prior to the online phase\n    cm : ConfusionMatrixNovelty\n        Optional, can specify an already existing confusion matrix instead of creating a new one for the metric\n\n    Attributes\n    ----------\n    cm : ConfusionMatrixNovelty\n        Confusion matrix\n    \"\"\"\n    def __init__(self, known_classes, cm: ConfusionMatrixNovelty = None):\n        if cm is None:\n            cm = ConfusionMatrixNovelty(known_classes)\n\n        super(metrics.base.MultiClassMetric, self).__init__(cm)\n\n    def get(self):\n        fn = self.cm.false_negatives_novelty() #Number of novelties wrongly classified as known\n\n        try:\n            return fn / self.cm.nc_samples\n        except ZeroDivisionError:\n            return 0.0\n</code></pre>"},{"location":"reference/metrics/ttd/","title":"ttd","text":""},{"location":"reference/metrics/ttd/#streamndr.metrics.ttd.TTD","title":"<code>TTD</code>","text":"<p>             Bases: <code>MultiClassMetric</code></p> <p>Time To Detection (TTD), represents the amount of samples needed for a novel class to be classified as a novel concept, as defined in [1].</p> <p>[1] Gaudreault, JG., Branco, P. (2023). Toward Streamlining the Evaluation of Novelty Detection in Data Streams.      In: Bifet, A., Lorena, A.C., Ribeiro, R.P., Gama, J., Abreu, P.H. (eds) Discovery Science. DS 2023. Lecture Notes in Computer Science(), vol 14276. Springer, Cham. </p> <p>Parameters:</p> Name Type Description Default <code>known_classes</code> <code>list of int</code> <p>List of known labels, the labels the algorithm knows prior to the online phase</p> required <code>cm</code> <code>ConfusionMatrixNovelty</code> <p>Optional, can specify an already existing confusion matrix instead of creating a new one for the metric</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>cm</code> <code>ConfusionMatrixNovelty</code> <p>Confusion matrix</p> Source code in <code>streamndr/metrics/ttd.py</code> <pre><code>class TTD(metrics.base.MultiClassMetric):\n    \"\"\"Time To Detection (TTD), represents the amount of samples needed for a novel class to be classified as a novel concept, as defined in [1].\n\n    [1] Gaudreault, JG., Branco, P. (2023). Toward Streamlining the Evaluation of Novelty Detection in Data Streams. \n        In: Bifet, A., Lorena, A.C., Ribeiro, R.P., Gama, J., Abreu, P.H. (eds) Discovery Science. DS 2023. Lecture Notes in Computer Science(), vol 14276. Springer, Cham. \n\n    Parameters\n    ----------\n    known_classes : list of int\n        List of known labels, the labels the algorithm knows prior to the online phase\n    cm : ConfusionMatrixNovelty\n        Optional, can specify an already existing confusion matrix instead of creating a new one for the metric\n\n    Attributes\n    ----------\n    cm : ConfusionMatrixNovelty\n        Confusion matrix\n    \"\"\"\n    def __init__(self, known_classes, cm: ConfusionMatrixNovelty = None):\n        if cm is None:\n            cm = ConfusionMatrixNovelty(known_classes)\n\n        super(metrics.base.MultiClassMetric, self).__init__(cm)\n\n        self.time_since_first_seen = dict()\n        self.time_to_detection = dict()\n\n    def update(self, y_true, y_pred, w=1.0):\n        super().update(y_true, y_pred, w)\n\n        if (not y_true in self.time_since_first_seen) and (not y_true in self.cm._init_classes):\n            self.time_since_first_seen[y_true] = 0\n\n        elif (y_true in self.time_since_first_seen) and (not y_true in self.time_to_detection) and (not y_pred in self.cm._init_classes.union({-1})):\n            self.time_to_detection[y_true] = self.time_since_first_seen[y_true]\n\n        elif (y_true in self.time_since_first_seen) and (not y_true in self.time_to_detection) and (y_pred in self.cm._init_classes.union({-1})):\n            self.time_since_first_seen[y_true] += 1\n\n    def get(self):\n        tmp = dict()\n\n        for key in self.time_since_first_seen:\n            if key in self.time_to_detection:\n                tmp[key] = self.time_to_detection[key]\n            else:\n                tmp[key] = -1\n\n        return tmp\n\n    def __repr__(self):\n        return \"TTD: \" + pprint.pformat(self.get())\n</code></pre>"},{"location":"reference/model/ecsminer/","title":"ecsminer","text":""},{"location":"reference/model/ecsminer/#streamndr.model.ecsminer.ECSMiner","title":"<code>ECSMiner</code>","text":"<p>             Bases: <code>MiniBatchClassifier</code></p> <p>Implementation of the ECSMiner algorithm for novelty detection [1].</p> <p>[1] Masud, Mohammad, et al. \"Classification and novel class detection in concept-drifting data streams under time constraints.\"  IEEE Transactions on knowledge and data engineering 23.6 (2010): 859-874.</p> <p>Parameters:</p> Name Type Description Default <code>K</code> <code>int</code> <p>Number of pseudopoints per classifier. In other words, it is the number of K cluster for the clustering algorithm.</p> <code>50</code> <code>min_examples_cluster</code> <code>int</code> <p>Minimum number of examples to declare a novel class</p> <code>50</code> <code>ensemble_size</code> <code>int</code> <p>Number of classifiers to use to create the ensemble</p> <code>6</code> <code>T_l</code> <code>int</code> <p>Labeling time constraint</p> <code>1000</code> <code>verbose</code> <code>int</code> <p>Controls the level of verbosity, the higher, the more messages are displayed. Can be '1', '2', or '3'.</p> <code>0</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generation. Makes the algorithm deterministic if a number is provided.</p> <code>None</code> <code>init_algorithm</code> <code>string</code> <p>String containing the clustering algorithm to use to initialize the clusters, supports 'kmeans' and 'mcikmeans'</p> <code>'mcikmeans'</code> <p>Attributes:</p> Name Type Description <code>models</code> <code>list of ClusterModel</code> <p>List containing the models of the ensemble.</p> <code>nb_class_unknown</code> <code>dict</code> <p>Tracks the number of samples of each true class value currently in the unknown buffer (short_mem). Used to compute the unknown rate.</p> <code>class_sample_counter</code> <code>dict</code> <p>Tracks the total number of samples of each true class value seen in the stream. Used to compute the unknown rate.</p> <code>sample_counter</code> <code>int</code> <p>Number of samples treated, used by the forgetting mechanism</p> <code>short_mem</code> <code>list of ShortMemInstance</code> <p>Buffer memory containing the samples labeled as unknown temporarily for the novelty detection process</p> <code>unlabeled_buffer</code> <code>list of ShortMemInstance</code> <p>Buffer memory containing the unlabeled data points until they are labeled</p> <code>labeled_buffer</code> <code>list of ShortMemInstance</code> <p>Buffer memory containing the labeled data points until they are used for training</p> <code>last_nd</code> <code>int</code> <p>Timestamp when the last novelty detection was performed. Used to determine if a new novelty detection should be performed.</p> <code>before_offline_phase</code> <code>bool</code> <p>Whether or not the algorithm was initialized (offline phase). The algorithm needs to first be initialized to be used in an online fashion.</p> Source code in <code>streamndr/model/ecsminer.py</code> <pre><code>class ECSMiner(base.MiniBatchClassifier):\n    \"\"\"Implementation of the ECSMiner algorithm for novelty detection [1].\n\n    [1] Masud, Mohammad, et al. \"Classification and novel class detection in concept-drifting data streams under time constraints.\" \n    IEEE Transactions on knowledge and data engineering 23.6 (2010): 859-874.\n\n    Parameters\n    ----------\n    K : int\n        Number of pseudopoints per classifier. In other words, it is the number of K cluster for the clustering algorithm.\n    min_examples_cluster : int\n        Minimum number of examples to declare a novel class \n    ensemble_size : int\n        Number of classifiers to use to create the ensemble\n    T_l : int\n        Labeling time constraint\n    verbose : int\n        Controls the level of verbosity, the higher, the more messages are displayed. Can be '1', '2', or '3'.\n    random_state : int\n        Seed for the random number generation. Makes the algorithm deterministic if a number is provided.\n    init_algorithm : string\n        String containing the clustering algorithm to use to initialize the clusters, supports 'kmeans' and 'mcikmeans'\n\n    Attributes\n    ----------\n    models : list of ClusterModel\n        List containing the models of the ensemble.\n    nb_class_unknown : dict\n        Tracks the number of samples of each true class value currently in the unknown buffer (short_mem). Used to compute the unknown rate.\n    class_sample_counter : dict\n        Tracks the total number of samples of each true class value seen in the stream. Used to compute the unknown rate.\n    sample_counter : int\n        Number of samples treated, used by the forgetting mechanism\n    short_mem : list of ShortMemInstance\n        Buffer memory containing the samples labeled as unknown temporarily for the novelty detection process\n    unlabeled_buffer : list of ShortMemInstance\n        Buffer memory containing the unlabeled data points until they are labeled\n    labeled_buffer : list of ShortMemInstance\n        Buffer memory containing the labeled data points until they are used for training\n    last_nd : int\n        Timestamp when the last novelty detection was performed. Used to determine if a new novelty detection should be performed.\n    before_offline_phase : bool\n        Whether or not the algorithm was initialized (offline phase). The algorithm needs to first be initialized to be used in an online fashion.\n    \"\"\"\n\n    def __init__(self,\n                 K=50, \n                 min_examples_cluster=50, #Number of instances requried to declare a novel class \n                 ensemble_size=6,\n                 T_l=1000,\n                 verbose=0,\n                 random_state=None,\n                 init_algorithm=\"mcikmeans\"):\n\n        super().__init__()\n        self.K = K\n        self.min_examples_cluster = min_examples_cluster\n        self.ensemble_size = ensemble_size\n        self.T_l = T_l\n        self.verbose = verbose\n        self.random_state = random_state\n\n        accepted_algos = ['kmeans','mcikmeans']\n        if init_algorithm not in accepted_algos:\n            print('Available algorithms: {}'.format(', '.join(accepted_algos)))\n        else:\n            self.init_algorithm = init_algorithm\n\n        self.models = []\n        self.novel_models = []\n        self.nb_class_unknown = dict()\n        self.class_sample_counter = dict()\n        self.sample_counter = 0\n        self.short_mem = ShortMem() #Potential novel class instances\n        self.unlabeled_buffer = [] #Unlabeled data points\n        self.labeled_buffer = [] #Labeled data points for training\n        self.last_nd = -self.min_examples_cluster #No novelty detection performed yet\n        self.before_offline_phase = True\n\n    def learn_one(self, x, y, w=1.0):\n        #Function used by river algorithms to learn one sample. It is not applicable to this algorithm since the offline phase requires all samples\n        #to arrive at once. It is only added as to follow River's API.\n        pass\n\n    def learn_many(self, X, y, w=1.0):\n        \"\"\"Represents the offline phase of the algorithm. Receives a number of samples and their given labels and learns all of the known classes.\n\n        Parameters\n        ----------\n        X : pandas.DataFrame or numpy.ndarray\n            Samples to be learned by the model\n        y : list of int\n            Labels corresponding to the given samples, must be the same length as the number of samples\n        w : float, optional\n            Weights, not used, by default 1.0\n\n        Returns\n        -------\n        ECSMiner\n            Fitted estimator\n        \"\"\"\n        if isinstance(X, pd.DataFrame):\n            X = X.to_numpy()\n\n        #The chunk size is equal to the number of samples given to learn divided by the size of the ensemble as one model is trained per chunk\n        self.chunk_size = math.ceil(len(X)/self.ensemble_size)\n\n        # in offline phase, consider all instances arriving at the same time in the microclusters:\n        timestamp = len(X)\n\n        #Separate data into {ensemble_size} chunks\n        for i in range(0, self.ensemble_size):\n            X_chunk = X[i:i+self.chunk_size]\n            y_chunk = y[i:i+self.chunk_size]\n\n            microclusters = self._generate_microclusters(X_chunk, y_chunk, timestamp, self.K, min_samples=3, algorithm=self.init_algorithm) #As per ECSMiner paper, any microcluster with less than 3 instances is discarded\n\n            model = ClusterModel(microclusters, np.unique(y_chunk))\n\n            if len(microclusters) &gt; 0:\n                self.models.append(model)\n\n        self.before_offline_phase = False\n\n        return self\n\n    def predict_one(self, X, y=None):\n        \"\"\"Represents the online phase. Equivalent to predict_many() with only one sample. Receives only one sample, predict its label and adds \n        it to the cluster if it is a known class. Otherwise, if it's unknown, it is added to the short term memory and novelty detection is \n        performed once the trigger has been reached (min_examples_cluster).\n\n        Parameters\n        ----------\n        X : dict\n            Sample\n        y : int\n            True y value of the sample.\n\n        Returns\n        -------\n        numpy.ndarray\n            Label predicted for the given sample, predicts -1 if labeled as unknown\n        \"\"\"\n        return self.predict_many(np.array(list(X.values()))[None,:], [y])\n\n\n    def predict_many(self, X, y=None):\n        \"\"\"Represents the online phase. Receives multiple samples, for each sample predict its label and adds it to the cluster if it is a known class. \n        Otherwise, if it's unknown, it is added to the short term memory and novelty detection is performed once the trigger has been reached (min_examples_cluster).\n\n        Parameters\n        ----------\n        X : pandas.DataFrame or numpy.ndarray\n            Samples\n        y : list of int\n            True y values of the samples.\n\n        Returns\n        -------\n        numpy.ndarray\n            Array of length len(X) containing the predicted labels, predicts -1 if the corresponding sample is labeled as unknown\n\n        Raises\n        ------\n        Exception\n            If the model has not been trained first with learn_many() (offline phase)\n        \"\"\"\n        if self.before_offline_phase:\n            raise Exception(\"Model must be fitted first\")\n\n        if isinstance(X, pd.DataFrame):\n            X = X.to_numpy() #Converting DataFrame to numpy array\n\n        f_outliers = self._check_f_outlier(X, self.models)\n        closest_model_cluster, y_preds = self._majority_voting(X)\n\n        pred_label = []\n        for i in range(len(X)):\n            self.sample_counter += 1\n            if y is not None:\n                if y[i] not in self.class_sample_counter:\n                    self.class_sample_counter[y[i]] = 1\n                else:\n                    self.class_sample_counter[y[i]] += 1\n\n            #Get the closest microcluster with our list of tuples self.models[closest_model_index][closest_cluster_index]\n            closest_cluster = self.models[closest_model_cluster[i][0]].microclusters[closest_model_cluster[i][1]]\n\n            self._filter_buffer()\n\n            #If X is not an F-outlier (inside the closest cluster radius), then we classify it with the label from the majority voting\n            if not f_outliers[i]:\n                pred_label.append(y_preds[i])\n                closest_cluster.update_cluster(X[i], self.sample_counter, False)\n\n            else: #X is an F-outlier (outside the boundary of all classifiers)\n                pred_label.append(-1)\n\n                if y is not None:\n                    self.short_mem.append(ShortMemInstance(X[i], self.sample_counter, y[i]))\n                    if y[i] not in self.nb_class_unknown:\n                        self.nb_class_unknown[y[i]] = 1\n                    else:\n                        self.nb_class_unknown[y[i]] += 1\n                else:\n                    self.short_mem.append(ShortMemInstance(X[i], self.sample_counter))\n\n                #Check if the length of the buffer is at least {min_examples_cluster} and that the last check was at least {min_examples_cluster} samples ago\n                if (len(self.short_mem) &gt; self.min_examples_cluster) and ((self.last_nd + self.min_examples_cluster) &lt;= self.sample_counter):\n                    self.last_nd = self.sample_counter\n\n                    #Find the list of novel clusters, if any\n                    novel_clusters = self._novelty_detect()\n\n                    if novel_clusters is not None: #We have novelty clusters\n                        for novel_cluster in novel_clusters:\n                            if self.verbose &gt; 0: print(\"Novel cluster detected: \", novel_cluster.small_str())\n\n\n                            #Remove instances from the buffer\n                            for instance in novel_cluster.instances:\n                                self._remove_sample_from_short_mem(self.short_mem.index(np.array(instance)))\n\n            #Enqueue X in the unlabeled buffer\n            self.unlabeled_buffer.append(ShortMemInstance(X[i], self.sample_counter, y[i]))\n\n            #If we have reached the labeling time constraint, we need to label the oldest instance\n            if len(self.unlabeled_buffer) &gt; self.T_l:\n                instance_to_label = self.unlabeled_buffer.pop(0)\n                self.labeled_buffer.append(instance_to_label)\n\n                #Remove from short_term_memory if it was in there and is a known class\n                if any(instance_to_label.y_true in model.labels for model in self.models) and (instance_to_label in self.short_mem.get_all_instances()):\n                    self.short_mem.remove(instance_to_label)\n\n                if len(self.labeled_buffer) == self.chunk_size:\n                    if self.verbose &gt; 0: \n                            print(\"Labeled buffer reached chunk size, creating new model...\")\n                    #Create a new model on the labeled buffer\n                    points = np.vstack([inst.point for inst in self.labeled_buffer])\n                    true_labels = np.array([inst.y_true for inst in self.labeled_buffer])\n                    new_microclusters = self._generate_microclusters(points, true_labels, self.sample_counter, self.K, min_samples=3, algorithm=self.init_algorithm)\n                    new_model = ClusterModel(new_microclusters, np.unique(true_labels))\n\n                    #Update the existing ensemble\n                    self.models.append(new_model)\n\n                    #Check if the oldest classifier has a class not included in any of the new models\n                    labels_of_oldest_model = set(self.models[0].labels)\n                    labels_of_new_models = set(element for sublist in self.models[1:] for element in sublist.labels)\n\n                    if labels_of_oldest_model - labels_of_new_models:\n                        if self.verbose &gt; 0: \n                            print(\"Oldest model includes label not in new models, forgetting...\")\n                        #Remove oldest model\n                        self.models.pop(0)\n\n\n                    if len(self.models) &gt; self.ensemble_size:\n                        accuracies = []\n                        #Iterate over all of the models in the ensemble and compute the accuracy of each model on the labeled buffer\n                        for model in self.models:\n                            #Get the model's closest microcluster\n                            closest_clusters_model, _ = get_closest_clusters(points, [microcluster.centroid for microcluster in model.microclusters])\n                            accuracies.append(accuracy_score(true_labels, [model.microclusters[closest_cluster].label for closest_cluster in closest_clusters_model]))\n\n                        #Remove the model with the lowest accuracy\n                        self.models.pop(np.argmin(accuracies))\n\n                    #Clear the labeled buffer\n                    self.labeled_buffer.clear()\n\n        return np.array(pred_label)\n\n    def get_unknown_rate(self):\n        \"\"\"Returns the unknown rate, represents the percentage of unknown samples on the total number of samples classified in the online phase.\n\n        Returns\n        -------\n        float\n            Unknown rate\n        \"\"\"\n        return len(self.short_mem) / self.sample_counter\n\n    def get_class_unknown_rate(self):\n        \"\"\"Returns the unknown rate per class. Represents the percentage of unknown samples on the total number of samples of that class seen during the stream.\n\n        Returns\n        -------\n        dict\n            Dictionary containing the unknown rate of each class\n        \"\"\"\n        return {key: val / self.class_sample_counter[key] for key, val in self.nb_class_unknown.items()}\n\n    def predict_proba_one(self,X):\n        #Function used by river algorithms to get the probability of the prediction. It is not applicable to this algorithm since it only predicts labels. \n        #It is only added as to follow River's API.\n        pass\n\n    def predict_proba_many(self, X):\n        #Function used by river algorithms to get the probability of the predictions. It is not applicable to this algorithm since it only predicts labels. \n        #It is only added as to follow River's API.\n        pass\n\n    def _generate_microclusters(self, X, y, timestamp, K, keep_instances=False, min_samples=0, algorithm=\"kmeans\"):\n        if K == 0:\n            return []\n\n        #Create K clusters\n        if algorithm == \"kmeans\":\n            clf = KMeans(n_clusters=K, n_init=\"auto\", random_state=self.random_state).fit(X)\n        else:\n            clf = MCIKMeans(n_clusters=K, random_state=self.random_state).fit(X, y)\n\n        cluster_labels = clf.labels_\n\n        #For each cluster, create a microcluster (cluster summary) and discard the data points\n        microclusters = []\n        for microcluster in np.unique(cluster_labels):\n            cluster_instances = X[cluster_labels == microcluster]\n            y_cluster_instances = y[cluster_labels == microcluster]\n\n            #Assign the label of the cluster (the class label with the highest frequency in the cluster)\n            values, counts = np.unique(y_cluster_instances, return_counts=True)\n            most_common_y = values[np.argmax(counts)]\n\n            if len(cluster_instances) &gt;= min_samples:\n                mc = MicroCluster(most_common_y, instances=cluster_instances, timestamp=timestamp, keep_instances=keep_instances)\n                microclusters.append(mc)\n\n        return microclusters\n\n    def _check_f_outlier(self, X, models):\n\n        #TODO: Parallelize these for loops through Numpy arrays\n        f_outliers = []\n        for point in X:\n            f_outlier = True\n            for model in models:\n                #X is an F-outlier if it is outside the decision boundary of all models\n                for microcluster in model.microclusters:\n                    if microcluster.distance_to_centroid(point) &lt;= microcluster.max_distance:\n                        f_outlier = False\n                        break\n                else:\n                    #If the inner condition was not triggered, we continue checking for the next model\n                    continue\n                break #Otherwise, we know X it not an F-outlier so we pass to the next point\n\n\n            f_outliers.append(f_outlier)\n\n        return f_outliers\n\n    def _majority_voting(self, X, return_labels=True):\n        closest_clusters = []\n        labels = []\n        dists = []\n\n        #Iterate over all of the models in the ensemble\n        for model in self.models:\n            #Get the model's closest microcluster and its corresponding distance for each X\n            closest_clusters_model, dist = get_closest_clusters(X, [microcluster.centroid for microcluster in model.microclusters])\n            closest_clusters.append(closest_clusters_model)\n            labels.append([model.microclusters[closest_cluster].label for closest_cluster in closest_clusters_model])\n            dists.append(dist)\n\n        #From all the closest microclusters of each model, get the index of the closest model for each X\n        best_models = np.argmin(dists, axis=0)\n\n        #Check if younger classifier classifies as new class C and older classifier wasn't trained on C\n        for k in range(len(X)):\n            for i in range(len(self.models)-1, 0, -1):\n                for j in range(0, i):\n                    if not labels[i][k] in self.models[j].labels: #If the label predicted by new classifier i was not in training sample of older classifier j\n                        #Check if the point is an outlier of model j\n                        outlier = True\n                        for microcluster in self.models[j].microclusters:\n                            if microcluster.distance_to_centroid(X[k]) &lt;= microcluster.max_distance:\n                                outlier = False\n                                break\n                        #If the point is an outlier of classifier j, don't consider its label\n                        if outlier:\n                            labels[j][k] = -1\n\n        #Finally, create a list of tuples, which contain the index of the closest model and the index of the closest microcluster within that model for each X\n        closest_model_cluster = []\n        for i in range(len(X)):\n            closest_model_cluster.append((best_models[i], closest_clusters[best_models[i]][i]))\n\n        #Return the list of tuples (index of closest model, index of closest microcluster within that model), \n        # and a list containing the label Y with the most occurence between all of the models (majority voting) for each X. \n        if return_labels:\n            return closest_model_cluster, self._get_most_occuring_by_column(labels)\n        else:\n            return closest_model_cluster\n\n    def _novelty_detect(self):\n        if self.verbose &gt; 1: print(\"Novelty detection started\")\n        X = self.short_mem.get_all_points()\n        new_class_vote = 0\n\n        #Creating F-pseudopoints representing all F-outliers to speedup computation of qnsc\n        K0 = round(self.K * (len(X) / self.chunk_size))\n        K0 = max(K0, self.K)\n        K0 = min(K0, len(X)) #Can't create K clusters if K is higher than the number of samples\n\n        f_microclusters = self._generate_microclusters(X, np.array([-1] * len(X)), self.sample_counter, K0, keep_instances=True, min_samples=0, algorithm=\"kmeans\")\n        f_microclusters_centroids = np.array([cl.centroid for cl in f_microclusters])\n\n        potential_novel_clusters_idx = []\n        #Computing qNSC for each model in our ensemble\n        for model in self.models:\n            qnscs = qnsc(f_microclusters_centroids, model.microclusters, self.min_examples_cluster)\n            potential_clusters = []\n            total_instances = 0\n            for i, f_microcluster in enumerate(f_microclusters):\n                if qnscs[i] &gt; 0:\n                    potential_clusters.append(f_microcluster)\n                    total_instances += f_microcluster.n\n                    potential_novel_clusters_idx.append(i)\n            if total_instances &gt; 0 and self.verbose &gt; 1:\n                print(f\"Total instances in F-outliers: {total_instances}\")\n\n            if total_instances &gt; self.min_examples_cluster: \n                new_class_vote += 1\n\n        if new_class_vote == len(self.models):\n            #Get the indices of all clusters which had a positive qnsc for all models\n            novel_clusters_idx = [item for item, count in Counter(potential_novel_clusters_idx).items() if count == len(self.models)]\n            novel_clusters = [f_microclusters[i] for i in novel_clusters_idx]\n\n            return novel_clusters\n\n        else:\n            return None\n\n    def _filter_buffer(self):\n        closest_model_cluster = self._majority_voting(self.short_mem.get_all_points(), False)\n\n\n        for i, instance in enumerate(self.short_mem.get_all_instances()):\n            closest_cluster = self.models[closest_model_cluster[i][0]].microclusters[closest_model_cluster[i][1]]\n\n            if ((self.sample_counter - instance.timestamp &gt; self.chunk_size) #The instance has an age greater than the chunk size\n                or (closest_cluster.distance_to_centroid(instance.point) &lt;= closest_cluster.max_distance)): #The instance is no longer an F-outlier \n\n                self._remove_sample_from_short_mem(self.short_mem.index(instance))\n\n    def _get_most_occuring_by_column(self, l):\n        most_common_values = {}\n        for col in zip(*l):\n            #Use a Counter to count the occurrences of each value in the column while ignoring -1 since it is a label we want to ignore\n            counts = Counter(val for val in col if val != -1)\n\n            #Find the most common value in the Counter\n            most_common_value = counts.most_common(1)\n\n            #If there are no valid values in the column, set the most_common_value to -1\n            if not most_common_value:\n                most_common_value = -1\n            else:\n                most_common_value = most_common_value[0][0]\n\n            most_common_values[len(most_common_values)] = most_common_value\n\n        return [most_common_values[i] for i in range(len(most_common_values))]\n\n    def _remove_sample_from_short_mem(self, index):\n        y_true = self.short_mem.get_instance(index).y_true\n        if y_true is not None:\n            self.nb_class_unknown[y_true] -= 1\n        self.short_mem.remove(index)\n</code></pre>"},{"location":"reference/model/ecsminer/#streamndr.model.ecsminer.ECSMiner.get_class_unknown_rate","title":"<code>get_class_unknown_rate()</code>","text":"<p>Returns the unknown rate per class. Represents the percentage of unknown samples on the total number of samples of that class seen during the stream.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the unknown rate of each class</p> Source code in <code>streamndr/model/ecsminer.py</code> <pre><code>def get_class_unknown_rate(self):\n    \"\"\"Returns the unknown rate per class. Represents the percentage of unknown samples on the total number of samples of that class seen during the stream.\n\n    Returns\n    -------\n    dict\n        Dictionary containing the unknown rate of each class\n    \"\"\"\n    return {key: val / self.class_sample_counter[key] for key, val in self.nb_class_unknown.items()}\n</code></pre>"},{"location":"reference/model/ecsminer/#streamndr.model.ecsminer.ECSMiner.get_unknown_rate","title":"<code>get_unknown_rate()</code>","text":"<p>Returns the unknown rate, represents the percentage of unknown samples on the total number of samples classified in the online phase.</p> <p>Returns:</p> Type Description <code>float</code> <p>Unknown rate</p> Source code in <code>streamndr/model/ecsminer.py</code> <pre><code>def get_unknown_rate(self):\n    \"\"\"Returns the unknown rate, represents the percentage of unknown samples on the total number of samples classified in the online phase.\n\n    Returns\n    -------\n    float\n        Unknown rate\n    \"\"\"\n    return len(self.short_mem) / self.sample_counter\n</code></pre>"},{"location":"reference/model/ecsminer/#streamndr.model.ecsminer.ECSMiner.learn_many","title":"<code>learn_many(X, y, w=1.0)</code>","text":"<p>Represents the offline phase of the algorithm. Receives a number of samples and their given labels and learns all of the known classes.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame or ndarray</code> <p>Samples to be learned by the model</p> required <code>y</code> <code>list of int</code> <p>Labels corresponding to the given samples, must be the same length as the number of samples</p> required <code>w</code> <code>float</code> <p>Weights, not used, by default 1.0</p> <code>1.0</code> <p>Returns:</p> Type Description <code>ECSMiner</code> <p>Fitted estimator</p> Source code in <code>streamndr/model/ecsminer.py</code> <pre><code>def learn_many(self, X, y, w=1.0):\n    \"\"\"Represents the offline phase of the algorithm. Receives a number of samples and their given labels and learns all of the known classes.\n\n    Parameters\n    ----------\n    X : pandas.DataFrame or numpy.ndarray\n        Samples to be learned by the model\n    y : list of int\n        Labels corresponding to the given samples, must be the same length as the number of samples\n    w : float, optional\n        Weights, not used, by default 1.0\n\n    Returns\n    -------\n    ECSMiner\n        Fitted estimator\n    \"\"\"\n    if isinstance(X, pd.DataFrame):\n        X = X.to_numpy()\n\n    #The chunk size is equal to the number of samples given to learn divided by the size of the ensemble as one model is trained per chunk\n    self.chunk_size = math.ceil(len(X)/self.ensemble_size)\n\n    # in offline phase, consider all instances arriving at the same time in the microclusters:\n    timestamp = len(X)\n\n    #Separate data into {ensemble_size} chunks\n    for i in range(0, self.ensemble_size):\n        X_chunk = X[i:i+self.chunk_size]\n        y_chunk = y[i:i+self.chunk_size]\n\n        microclusters = self._generate_microclusters(X_chunk, y_chunk, timestamp, self.K, min_samples=3, algorithm=self.init_algorithm) #As per ECSMiner paper, any microcluster with less than 3 instances is discarded\n\n        model = ClusterModel(microclusters, np.unique(y_chunk))\n\n        if len(microclusters) &gt; 0:\n            self.models.append(model)\n\n    self.before_offline_phase = False\n\n    return self\n</code></pre>"},{"location":"reference/model/ecsminer/#streamndr.model.ecsminer.ECSMiner.predict_many","title":"<code>predict_many(X, y=None)</code>","text":"<p>Represents the online phase. Receives multiple samples, for each sample predict its label and adds it to the cluster if it is a known class.  Otherwise, if it's unknown, it is added to the short term memory and novelty detection is performed once the trigger has been reached (min_examples_cluster).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame or ndarray</code> <p>Samples</p> required <code>y</code> <code>list of int</code> <p>True y values of the samples.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of length len(X) containing the predicted labels, predicts -1 if the corresponding sample is labeled as unknown</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the model has not been trained first with learn_many() (offline phase)</p> Source code in <code>streamndr/model/ecsminer.py</code> <pre><code>def predict_many(self, X, y=None):\n    \"\"\"Represents the online phase. Receives multiple samples, for each sample predict its label and adds it to the cluster if it is a known class. \n    Otherwise, if it's unknown, it is added to the short term memory and novelty detection is performed once the trigger has been reached (min_examples_cluster).\n\n    Parameters\n    ----------\n    X : pandas.DataFrame or numpy.ndarray\n        Samples\n    y : list of int\n        True y values of the samples.\n\n    Returns\n    -------\n    numpy.ndarray\n        Array of length len(X) containing the predicted labels, predicts -1 if the corresponding sample is labeled as unknown\n\n    Raises\n    ------\n    Exception\n        If the model has not been trained first with learn_many() (offline phase)\n    \"\"\"\n    if self.before_offline_phase:\n        raise Exception(\"Model must be fitted first\")\n\n    if isinstance(X, pd.DataFrame):\n        X = X.to_numpy() #Converting DataFrame to numpy array\n\n    f_outliers = self._check_f_outlier(X, self.models)\n    closest_model_cluster, y_preds = self._majority_voting(X)\n\n    pred_label = []\n    for i in range(len(X)):\n        self.sample_counter += 1\n        if y is not None:\n            if y[i] not in self.class_sample_counter:\n                self.class_sample_counter[y[i]] = 1\n            else:\n                self.class_sample_counter[y[i]] += 1\n\n        #Get the closest microcluster with our list of tuples self.models[closest_model_index][closest_cluster_index]\n        closest_cluster = self.models[closest_model_cluster[i][0]].microclusters[closest_model_cluster[i][1]]\n\n        self._filter_buffer()\n\n        #If X is not an F-outlier (inside the closest cluster radius), then we classify it with the label from the majority voting\n        if not f_outliers[i]:\n            pred_label.append(y_preds[i])\n            closest_cluster.update_cluster(X[i], self.sample_counter, False)\n\n        else: #X is an F-outlier (outside the boundary of all classifiers)\n            pred_label.append(-1)\n\n            if y is not None:\n                self.short_mem.append(ShortMemInstance(X[i], self.sample_counter, y[i]))\n                if y[i] not in self.nb_class_unknown:\n                    self.nb_class_unknown[y[i]] = 1\n                else:\n                    self.nb_class_unknown[y[i]] += 1\n            else:\n                self.short_mem.append(ShortMemInstance(X[i], self.sample_counter))\n\n            #Check if the length of the buffer is at least {min_examples_cluster} and that the last check was at least {min_examples_cluster} samples ago\n            if (len(self.short_mem) &gt; self.min_examples_cluster) and ((self.last_nd + self.min_examples_cluster) &lt;= self.sample_counter):\n                self.last_nd = self.sample_counter\n\n                #Find the list of novel clusters, if any\n                novel_clusters = self._novelty_detect()\n\n                if novel_clusters is not None: #We have novelty clusters\n                    for novel_cluster in novel_clusters:\n                        if self.verbose &gt; 0: print(\"Novel cluster detected: \", novel_cluster.small_str())\n\n\n                        #Remove instances from the buffer\n                        for instance in novel_cluster.instances:\n                            self._remove_sample_from_short_mem(self.short_mem.index(np.array(instance)))\n\n        #Enqueue X in the unlabeled buffer\n        self.unlabeled_buffer.append(ShortMemInstance(X[i], self.sample_counter, y[i]))\n\n        #If we have reached the labeling time constraint, we need to label the oldest instance\n        if len(self.unlabeled_buffer) &gt; self.T_l:\n            instance_to_label = self.unlabeled_buffer.pop(0)\n            self.labeled_buffer.append(instance_to_label)\n\n            #Remove from short_term_memory if it was in there and is a known class\n            if any(instance_to_label.y_true in model.labels for model in self.models) and (instance_to_label in self.short_mem.get_all_instances()):\n                self.short_mem.remove(instance_to_label)\n\n            if len(self.labeled_buffer) == self.chunk_size:\n                if self.verbose &gt; 0: \n                        print(\"Labeled buffer reached chunk size, creating new model...\")\n                #Create a new model on the labeled buffer\n                points = np.vstack([inst.point for inst in self.labeled_buffer])\n                true_labels = np.array([inst.y_true for inst in self.labeled_buffer])\n                new_microclusters = self._generate_microclusters(points, true_labels, self.sample_counter, self.K, min_samples=3, algorithm=self.init_algorithm)\n                new_model = ClusterModel(new_microclusters, np.unique(true_labels))\n\n                #Update the existing ensemble\n                self.models.append(new_model)\n\n                #Check if the oldest classifier has a class not included in any of the new models\n                labels_of_oldest_model = set(self.models[0].labels)\n                labels_of_new_models = set(element for sublist in self.models[1:] for element in sublist.labels)\n\n                if labels_of_oldest_model - labels_of_new_models:\n                    if self.verbose &gt; 0: \n                        print(\"Oldest model includes label not in new models, forgetting...\")\n                    #Remove oldest model\n                    self.models.pop(0)\n\n\n                if len(self.models) &gt; self.ensemble_size:\n                    accuracies = []\n                    #Iterate over all of the models in the ensemble and compute the accuracy of each model on the labeled buffer\n                    for model in self.models:\n                        #Get the model's closest microcluster\n                        closest_clusters_model, _ = get_closest_clusters(points, [microcluster.centroid for microcluster in model.microclusters])\n                        accuracies.append(accuracy_score(true_labels, [model.microclusters[closest_cluster].label for closest_cluster in closest_clusters_model]))\n\n                    #Remove the model with the lowest accuracy\n                    self.models.pop(np.argmin(accuracies))\n\n                #Clear the labeled buffer\n                self.labeled_buffer.clear()\n\n    return np.array(pred_label)\n</code></pre>"},{"location":"reference/model/ecsminer/#streamndr.model.ecsminer.ECSMiner.predict_one","title":"<code>predict_one(X, y=None)</code>","text":"<p>Represents the online phase. Equivalent to predict_many() with only one sample. Receives only one sample, predict its label and adds  it to the cluster if it is a known class. Otherwise, if it's unknown, it is added to the short term memory and novelty detection is  performed once the trigger has been reached (min_examples_cluster).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>dict</code> <p>Sample</p> required <code>y</code> <code>int</code> <p>True y value of the sample.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Label predicted for the given sample, predicts -1 if labeled as unknown</p> Source code in <code>streamndr/model/ecsminer.py</code> <pre><code>def predict_one(self, X, y=None):\n    \"\"\"Represents the online phase. Equivalent to predict_many() with only one sample. Receives only one sample, predict its label and adds \n    it to the cluster if it is a known class. Otherwise, if it's unknown, it is added to the short term memory and novelty detection is \n    performed once the trigger has been reached (min_examples_cluster).\n\n    Parameters\n    ----------\n    X : dict\n        Sample\n    y : int\n        True y value of the sample.\n\n    Returns\n    -------\n    numpy.ndarray\n        Label predicted for the given sample, predicts -1 if labeled as unknown\n    \"\"\"\n    return self.predict_many(np.array(list(X.values()))[None,:], [y])\n</code></pre>"},{"location":"reference/model/ecsminerwf/","title":"ecsminerwf","text":""},{"location":"reference/model/ecsminerwf/#streamndr.model.ecsminerwf.ECSMinerWF","title":"<code>ECSMinerWF</code>","text":"<p>             Bases: <code>ECSMiner</code></p> <p>Implementation of the ECSMinerWF (ECSMiner without feedback) algorithm for novelty detection. [1]</p> <p>[1] de Faria, Elaine Ribeiro, Andr\u00e9 Carlos Ponce de Leon Ferreira Carvalho, and Joao Gama. \"MINAS: multiclass learning algorithm for novelty detection in data streams.\"  Data mining and knowledge discovery 30 (2016): 640-680.</p> <p>Parameters:</p> Name Type Description Default <code>K</code> <code>int</code> <p>Number of pseudopoints per classifier. In other words, it is the number of K cluster for the clustering algorithm.</p> <code>50</code> <code>min_examples_cluster</code> <code>int</code> <p>Minimum number of examples to declare a novel class</p> <code>50</code> <code>ensemble_size</code> <code>int</code> <p>Number of classifiers to use to create the ensemble</p> <code>6</code> <code>verbose</code> <code>int</code> <p>Controls the level of verbosity, the higher, the more messages are displayed. Can be '1', '2', or '3'.</p> <code>0</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generation. Makes the algorithm deterministic if a number is provided.</p> <code>None</code> <code>init_algorithm</code> <code>string</code> <p>String containing the clustering algorithm to use to initialize the clusters, supports 'kmeans' and 'mcikmeans'</p> <code>'mcikmeans'</code> <p>Attributes:</p> Name Type Description <code>novel_models</code> <code>list of ClusterModel</code> <p>List containing the models representing novel classes, added during the online phase.</p> Source code in <code>streamndr/model/ecsminerwf.py</code> <pre><code>class ECSMinerWF(ECSMiner):\n    \"\"\"Implementation of the ECSMinerWF (ECSMiner without feedback) algorithm for novelty detection. [1]\n\n    [1] de Faria, Elaine Ribeiro, Andr\u00e9 Carlos Ponce de Leon Ferreira Carvalho, and Joao Gama. \"MINAS: multiclass learning algorithm for novelty detection in data streams.\" \n    Data mining and knowledge discovery 30 (2016): 640-680.\n\n    Parameters\n    ----------\n    K : int\n        Number of pseudopoints per classifier. In other words, it is the number of K cluster for the clustering algorithm.\n    min_examples_cluster : int\n        Minimum number of examples to declare a novel class \n    ensemble_size : int\n        Number of classifiers to use to create the ensemble\n    verbose : int\n        Controls the level of verbosity, the higher, the more messages are displayed. Can be '1', '2', or '3'.\n    random_state : int\n        Seed for the random number generation. Makes the algorithm deterministic if a number is provided.\n    init_algorithm : string\n        String containing the clustering algorithm to use to initialize the clusters, supports 'kmeans' and 'mcikmeans'\n\n    Attributes\n    ----------\n    novel_models : list of ClusterModel\n        List containing the models representing novel classes, added during the online phase.\n    \"\"\"\n\n    def __init__(self,\n                 K=50, \n                 min_examples_cluster=50, #Number of instances requried to declare a novel class \n                 ensemble_size=6, \n                 verbose=0,\n                 random_state=None,\n                 init_algorithm=\"mcikmeans\"):\n\n        super().__init__(K=K, \n                         min_examples_cluster=min_examples_cluster, \n                         ensemble_size=ensemble_size, \n                         T_l=0,\n                         verbose=verbose, \n                         random_state=random_state, \n                         init_algorithm=init_algorithm)\n\n        self.novel_models = []\n\n    def predict_many(self, X, y=None):\n        \"\"\"Represents the online phase. Receives multiple samples, for each sample predict its label and adds it to the cluster if it is a known class. \n        Otherwise, if it's unknown, it is added to the short term memory and novelty detection is performed once the trigger has been reached (min_examples_cluster).\n\n        Parameters\n        ----------\n        X : pandas.DataFrame or numpy.ndarray\n            Samples\n        y : list of int\n            True y values of the samples.\n\n        Returns\n        -------\n        numpy.ndarray\n            Array of length len(X) containing the predicted labels, predicts -1 if the corresponding sample is labeled as unknown\n\n        Raises\n        ------\n        Exception\n            If the model has not been trained first with learn_many() (offline phase)\n        \"\"\"\n        if self.before_offline_phase:\n            raise Exception(\"Model must be fitted first\")\n\n        if isinstance(X, pd.DataFrame):\n            X = X.to_numpy() #Converting DataFrame to numpy array\n\n        f_outliers = self._check_f_outlier(X, self.models)\n        closest_model_cluster, y_preds = self._majority_voting(X)\n\n        #If we have novel models, get the closest ones for all Xs\n        if len(self.novel_models) &gt; 0:\n            closest_novel_clusters, novel_dists = get_closest_clusters(X, [microcluster.centroid for model in self.novel_models for microcluster in model.microclusters])\n            novel_microclusters = [microcluster for model in self.novel_models for microcluster in model.microclusters]\n\n\n        pred_label = []\n        for i in range(len(X)):\n            self.sample_counter += 1\n            if y is not None:\n                if y[i] not in self.class_sample_counter:\n                    self.class_sample_counter[y[i]] = 1\n                else:\n                    self.class_sample_counter[y[i]] += 1\n\n            #Get the closest microcluster with our list of tuples self.models[closest_model_index][closest_cluster_index]\n            closest_cluster = self.models[closest_model_cluster[i][0]].microclusters[closest_model_cluster[i][1]]\n\n            self._filter_buffer()\n\n            #If X is not an F-outlier (inside the closest cluster radius), then we classify it with the label from the majority voting\n            if not f_outliers[i]:\n                pred_label.append(y_preds[i])\n                closest_cluster.update_cluster(X[i], self.sample_counter, False)\n\n            else: #X is an F-outlier (outside the boundary of all classifiers)\n                #Check if X can be explained by our list of novel models\n                if len(self.novel_models) &gt; 0 and novel_dists[i] &lt;= novel_microclusters[closest_novel_clusters[i]].max_distance:\n                    pred_label.append(novel_microclusters[closest_novel_clusters[i]].label)\n                    novel_microclusters[closest_novel_clusters[i]].update_cluster(X[i], self.sample_counter, False)\n\n                else:\n                    pred_label.append(-1)\n\n                    if y is not None:\n                        self.short_mem.append(ShortMemInstance(X[i], self.sample_counter, y[i]))\n                        if y[i] not in self.nb_class_unknown:\n                            self.nb_class_unknown[y[i]] = 1\n                        else:\n                            self.nb_class_unknown[y[i]] += 1\n                    else:\n                        self.short_mem.append(ShortMemInstance(X[i], self.sample_counter))\n\n                    #Check if the length of the buffer is at least {min_examples_cluster} and that the last check was at least {min_examples_cluster} samples ago\n                    if (len(self.short_mem) &gt; self.min_examples_cluster) and ((self.last_nd + self.min_examples_cluster) &lt;= self.sample_counter):\n                        self.last_nd = self.sample_counter\n\n                        #Find the list of novel clusters, if any\n                        novel_clusters = self._novelty_detect()\n\n                        if novel_clusters is not None: #We have novelty clusters\n                            #Find the next available sequential label\n                            max_label_ensemble = max([label for model in self.models for label in model.labels])\n                            max_label_novel = max([label for model in self.novel_models for label in model.labels]) if len(self.novel_models) &gt; 0 else -1\n                            new_label = max(max_label_ensemble, max_label_novel) + 1\n\n                            for novel_cluster in novel_clusters:\n                                #Set the label for each microcluster\n                                novel_cluster.label = new_label\n\n                                if self.verbose &gt; 0: print(\"Novel cluster detected: \", novel_cluster.small_str())\n\n                                #Remove instances from the buffer\n                                for instance in novel_cluster.instances:\n                                    self._remove_sample_from_short_mem(self.short_mem.index(np.array(instance)))\n\n                            pred_label[-1] = new_label\n\n                            #Add the clusters to our novel models list\n                            self.novel_models.append(ClusterModel(novel_clusters, [new_label]))\n\n        return np.array(pred_label)\n</code></pre>"},{"location":"reference/model/ecsminerwf/#streamndr.model.ecsminerwf.ECSMinerWF.predict_many","title":"<code>predict_many(X, y=None)</code>","text":"<p>Represents the online phase. Receives multiple samples, for each sample predict its label and adds it to the cluster if it is a known class.  Otherwise, if it's unknown, it is added to the short term memory and novelty detection is performed once the trigger has been reached (min_examples_cluster).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame or ndarray</code> <p>Samples</p> required <code>y</code> <code>list of int</code> <p>True y values of the samples.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of length len(X) containing the predicted labels, predicts -1 if the corresponding sample is labeled as unknown</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the model has not been trained first with learn_many() (offline phase)</p> Source code in <code>streamndr/model/ecsminerwf.py</code> <pre><code>def predict_many(self, X, y=None):\n    \"\"\"Represents the online phase. Receives multiple samples, for each sample predict its label and adds it to the cluster if it is a known class. \n    Otherwise, if it's unknown, it is added to the short term memory and novelty detection is performed once the trigger has been reached (min_examples_cluster).\n\n    Parameters\n    ----------\n    X : pandas.DataFrame or numpy.ndarray\n        Samples\n    y : list of int\n        True y values of the samples.\n\n    Returns\n    -------\n    numpy.ndarray\n        Array of length len(X) containing the predicted labels, predicts -1 if the corresponding sample is labeled as unknown\n\n    Raises\n    ------\n    Exception\n        If the model has not been trained first with learn_many() (offline phase)\n    \"\"\"\n    if self.before_offline_phase:\n        raise Exception(\"Model must be fitted first\")\n\n    if isinstance(X, pd.DataFrame):\n        X = X.to_numpy() #Converting DataFrame to numpy array\n\n    f_outliers = self._check_f_outlier(X, self.models)\n    closest_model_cluster, y_preds = self._majority_voting(X)\n\n    #If we have novel models, get the closest ones for all Xs\n    if len(self.novel_models) &gt; 0:\n        closest_novel_clusters, novel_dists = get_closest_clusters(X, [microcluster.centroid for model in self.novel_models for microcluster in model.microclusters])\n        novel_microclusters = [microcluster for model in self.novel_models for microcluster in model.microclusters]\n\n\n    pred_label = []\n    for i in range(len(X)):\n        self.sample_counter += 1\n        if y is not None:\n            if y[i] not in self.class_sample_counter:\n                self.class_sample_counter[y[i]] = 1\n            else:\n                self.class_sample_counter[y[i]] += 1\n\n        #Get the closest microcluster with our list of tuples self.models[closest_model_index][closest_cluster_index]\n        closest_cluster = self.models[closest_model_cluster[i][0]].microclusters[closest_model_cluster[i][1]]\n\n        self._filter_buffer()\n\n        #If X is not an F-outlier (inside the closest cluster radius), then we classify it with the label from the majority voting\n        if not f_outliers[i]:\n            pred_label.append(y_preds[i])\n            closest_cluster.update_cluster(X[i], self.sample_counter, False)\n\n        else: #X is an F-outlier (outside the boundary of all classifiers)\n            #Check if X can be explained by our list of novel models\n            if len(self.novel_models) &gt; 0 and novel_dists[i] &lt;= novel_microclusters[closest_novel_clusters[i]].max_distance:\n                pred_label.append(novel_microclusters[closest_novel_clusters[i]].label)\n                novel_microclusters[closest_novel_clusters[i]].update_cluster(X[i], self.sample_counter, False)\n\n            else:\n                pred_label.append(-1)\n\n                if y is not None:\n                    self.short_mem.append(ShortMemInstance(X[i], self.sample_counter, y[i]))\n                    if y[i] not in self.nb_class_unknown:\n                        self.nb_class_unknown[y[i]] = 1\n                    else:\n                        self.nb_class_unknown[y[i]] += 1\n                else:\n                    self.short_mem.append(ShortMemInstance(X[i], self.sample_counter))\n\n                #Check if the length of the buffer is at least {min_examples_cluster} and that the last check was at least {min_examples_cluster} samples ago\n                if (len(self.short_mem) &gt; self.min_examples_cluster) and ((self.last_nd + self.min_examples_cluster) &lt;= self.sample_counter):\n                    self.last_nd = self.sample_counter\n\n                    #Find the list of novel clusters, if any\n                    novel_clusters = self._novelty_detect()\n\n                    if novel_clusters is not None: #We have novelty clusters\n                        #Find the next available sequential label\n                        max_label_ensemble = max([label for model in self.models for label in model.labels])\n                        max_label_novel = max([label for model in self.novel_models for label in model.labels]) if len(self.novel_models) &gt; 0 else -1\n                        new_label = max(max_label_ensemble, max_label_novel) + 1\n\n                        for novel_cluster in novel_clusters:\n                            #Set the label for each microcluster\n                            novel_cluster.label = new_label\n\n                            if self.verbose &gt; 0: print(\"Novel cluster detected: \", novel_cluster.small_str())\n\n                            #Remove instances from the buffer\n                            for instance in novel_cluster.instances:\n                                self._remove_sample_from_short_mem(self.short_mem.index(np.array(instance)))\n\n                        pred_label[-1] = new_label\n\n                        #Add the clusters to our novel models list\n                        self.novel_models.append(ClusterModel(novel_clusters, [new_label]))\n\n    return np.array(pred_label)\n</code></pre>"},{"location":"reference/model/minas/","title":"minas","text":""},{"location":"reference/model/minas/#streamndr.model.minas.Minas","title":"<code>Minas</code>","text":"<p>             Bases: <code>MiniBatchClassifier</code></p> <p>Implementation of the MINAS algorithm for novelty detection. [1]</p> <p>[1] de Faria, Elaine Ribeiro, Andr\u00e9 Carlos Ponce de Leon Ferreira Carvalho, and Joao Gama. \"MINAS: multiclass learning algorithm for novelty detection in data streams.\"  Data mining and knowledge discovery 30 (2016): 640-680.</p> <p>Parameters:</p> Name Type Description Default <code>kini</code> <code>int</code> <p>Number of K clusters for the clustering (KMeans or Clustream) algorithm</p> <code>3</code> <code>cluster_algorithm</code> <code>str</code> <p>String containing the clustering algorithm to use, supports 'kmeans' and 'clustream'</p> <code>'kmeans'</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generation. Makes the algorithm deterministic if a number is provided.</p> <code>None</code> <code>min_short_mem_trigger</code> <code>int</code> <p>Minimum number of samples in the short term memory to trigger the novelty detection process</p> <code>10</code> <code>min_examples_cluster</code> <code>int</code> <p>Minimum number of samples to from a cluster</p> <code>10</code> <code>threshold_strategy</code> <code>int</code> <p>Strategy to use to compute the threshold. Can be '1', '2', or '3' as described in the MINAS paper.</p> <code>1</code> <code>threshold_factor</code> <code>float</code> <p>Factor for the threshold computation</p> <code>1.1</code> <code>window_size</code> <code>int</code> <p>Number of samples used by the forgetting mechanism</p> <code>100</code> <code>update_summary</code> <code>bool</code> <p>Whether or not the microcluster's properties are updated when a new point is added to it</p> <code>False</code> <code>verbose</code> <code>int</code> <p>Controls the level of verbosity, the higher, the more messages are displayed. Can be '1', '2', or '3'.</p> <code>0</code> <p>Attributes:</p> Name Type Description <code>MAX_MEMORY_SIZE</code> <code>int</code> <p>Constant used to determine the maximum number of rows used by numpy for the computation of the closest clusters. A higher number is faster but takes more memory.</p> <code>before_offline_phase</code> <code>bool</code> <p>Whether or not the algorithm was initialized (offline phase). The algorithm needs to first be initialized to be used in an online fashion.</p> <code>short_mem</code> <code>list of ShortMemInstance</code> <p>Buffer memory containing the samples labeled as unknown temporarily for the novelty detection process</p> <code>sleep_mem</code> <code>list of MicroCluster</code> <p>Microclusters that have not have any new points added from the strem for a period of time are temporarily moved to a sleep memory</p> <code>nb_class_unknown</code> <code>dict</code> <p>Tracks the number of samples of each true class value currently in the unknown buffer (short_mem). Used to compute the unknown rate.</p> <code>class_sample_counter</code> <code>dict</code> <p>Tracks the total number of samples of each true class value seen in the stream. Used to compute the unknown rate.</p> <code>sample_counter</code> <code>int</code> <p>Number of samples treated, used by the forgetting mechanism</p> Source code in <code>streamndr/model/minas.py</code> <pre><code>class Minas(base.MiniBatchClassifier):\n    \"\"\"Implementation of the MINAS algorithm for novelty detection. [1]\n\n    [1] de Faria, Elaine Ribeiro, Andr\u00e9 Carlos Ponce de Leon Ferreira Carvalho, and Joao Gama. \"MINAS: multiclass learning algorithm for novelty detection in data streams.\" \n    Data mining and knowledge discovery 30 (2016): 640-680.\n\n    Parameters\n    ----------\n    kini : int\n        Number of K clusters for the clustering (KMeans or Clustream) algorithm\n    cluster_algorithm : str\n        String containing the clustering algorithm to use, supports 'kmeans' and 'clustream'\n    random_state : int\n        Seed for the random number generation. Makes the algorithm deterministic if a number is provided.\n    min_short_mem_trigger : int\n        Minimum number of samples in the short term memory to trigger the novelty detection process\n    min_examples_cluster : int\n        Minimum number of samples to from a cluster\n    threshold_strategy : int\n        Strategy to use to compute the threshold. Can be '1', '2', or '3' as described in the MINAS paper.\n    threshold_factor : float\n        Factor for the threshold computation\n    window_size : int\n        Number of samples used by the forgetting mechanism\n    update_summary : bool\n        Whether or not the microcluster's properties are updated when a new point is added to it\n    verbose : int\n        Controls the level of verbosity, the higher, the more messages are displayed. Can be '1', '2', or '3'.\n\n    Attributes\n    ----------\n    MAX_MEMORY_SIZE : int\n        Constant used to determine the maximum number of rows used by numpy for the computation of the closest clusters. A higher number is faster but takes more memory.\n    before_offline_phase : bool\n        Whether or not the algorithm was initialized (offline phase). The algorithm needs to first be initialized to be used in an online fashion.\n    short_mem : list of ShortMemInstance\n        Buffer memory containing the samples labeled as unknown temporarily for the novelty detection process\n    sleep_mem : list of MicroCluster\n        Microclusters that have not have any new points added from the strem for a period of time are temporarily moved to a sleep memory\n    nb_class_unknown : dict\n        Tracks the number of samples of each true class value currently in the unknown buffer (short_mem). Used to compute the unknown rate.\n    class_sample_counter : dict\n        Tracks the total number of samples of each true class value seen in the stream. Used to compute the unknown rate.\n    sample_counter : int\n        Number of samples treated, used by the forgetting mechanism\n    \"\"\"\n\n\n    MAX_MEMORY_SIZE = 50000\n\n    def __init__(self,\n                 kini=3,\n                 cluster_algorithm='kmeans',\n                 random_state=None,\n                 min_short_mem_trigger=10,\n                 min_examples_cluster=10,\n                 threshold_strategy=1,\n                 threshold_factor=1.1,\n                 window_size=100,\n                 update_summary=False,\n                 verbose=0):\n        super().__init__()\n        self.kini = kini\n        self.random_state = random_state\n\n        accepted_algos = ['kmeans','clustream']\n        if cluster_algorithm not in accepted_algos:\n            print('Available algorithms: {}'.format(', '.join(accepted_algos)))\n        else:\n            self.cluster_algorithm = cluster_algorithm\n\n        self.microclusters = []  # list of microclusters\n        self.before_offline_phase = True\n\n        self.short_mem = ShortMem()\n        self.sleep_mem = []\n        self.nb_class_unknown = dict()\n        self.class_sample_counter = dict()\n        self.min_short_mem_trigger = min_short_mem_trigger\n        self.min_examples_cluster = min_examples_cluster\n        self.threshold_strategy = threshold_strategy\n        self.threshold_factor = threshold_factor\n        self.window_size = window_size\n        self.update_summary = update_summary\n        self.verbose = verbose\n        self.sample_counter = 0  # to be used with window_size\n\n    def learn_one(self, x, y, w=1.0):\n        \"\"\"Function used by river algorithms to learn one sample. It is not applicable to this algorithm since the offline phase requires all samples\n        to arrive at once. It is only added as to follow River's API.\n\n        Parameters\n        ----------\n        x : dict\n            Sample\n        y : int\n            Label of the given sample\n        w : float, optional\n            Weight, not used, by default 1.0\n        \"\"\"\n        # Not applicable\n        pass\n\n\n    def learn_many(self, X, y, w=1.0):\n        \"\"\"Represents the offline phase of the algorithm. Receives a number of samples and their given labels and learns all of the known classes.\n\n        Parameters\n        ----------\n        X : pandas.DataFrame or numpy.ndarray\n            Samples to be learned by the model\n        y : list of int\n            Labels corresponding to the given samples, must be the same length as the number of samples\n        w : float, optional\n            Weights, not used, by default 1.0\n\n        Returns\n        -------\n        Minas\n            Fitted estimator\n        \"\"\"\n        if isinstance(X, pd.DataFrame):\n            X = X.to_numpy()\n\n        self.microclusters = self._offline(X, y)\n        self.before_offline_phase = False\n\n        return self\n\n    def predict_one(self, X, y=None):\n        \"\"\"Represents the online phase. Equivalent to predict_many() with only one sample. Receives only one sample, predict its label and adds \n        it to the cluster if it is a known class. Otherwise, if it's unknown, it is added to the short term memory and novelty detection is \n        performed once the trigger has been reached (min_short_mem_trigger).\n\n        Parameters\n        ----------\n        X : dict\n            Sample\n        y : int\n            True y value of the sample, if available. Only used for metric evaluation (UnkRate).\n\n        Returns\n        -------\n        numpy.ndarray\n            Label predicted for the given sample, predicts -1 if labeled as unknown\n        \"\"\"\n        return self.predict_many(np.array(list(X.values()))[None,:], [y])\n\n    def predict_many(self, X, y=None):\n        \"\"\"Represents the online phase. Receives multiple samples, for each sample predict its label and adds it to the cluster if it is a known class. \n        Otherwise, if it's unknown, it is added to the short term memory and novelty detection is performed once the trigger has been reached (min_short_mem_trigger).\n\n        Parameters\n        ----------\n        X : pandas.DataFrame or numpy.ndarray\n            Samples\n        y : list of int\n            True y values of the samples, if available. Only used for metric evaluation (UnkRate).\n\n        Returns\n        -------\n        numpy.ndarray\n            Array of length len(X) containing the predicted labels, predicts -1 if the corresponding sample labeled as unknown\n\n        Raises\n        ------\n        Exception\n            If the model has not been trained first with learn_many() (offline phase)\n        \"\"\"\n        if self.before_offline_phase:\n            raise Exception(\"Model must be fitted first\")\n\n        if isinstance(X, pd.DataFrame):\n            X = X.to_numpy() #Converting DataFrame to numpy array\n\n        # Finding closest clusters for received samples\n        closest_clusters, _ = get_closest_clusters(X, [microcluster.centroid for microcluster in self.microclusters])\n\n        pred_label = []\n\n        for i in range(len(closest_clusters)):\n            self.sample_counter += 1\n            if y is not None:\n                if y[i] not in self.class_sample_counter:\n                    self.class_sample_counter[y[i]] = 1\n                else:\n                    self.class_sample_counter[y[i]] += 1\n\n            if closest_clusters[i] != -1:\n                closest_cluster = self.microclusters[closest_clusters[i]]\n\n                if closest_cluster.encompasses(X[i]):  # classify in this cluster\n                    pred_label.append(closest_cluster.label)\n\n                    closest_cluster.update_cluster(X[i], self.sample_counter, self.update_summary)\n\n                else:  # classify as unknown\n                    pred_label.append(-1)\n                    self._label_as_unknown(X[i], y[i])\n\n            else: # classify as unknown\n                pred_label.append(-1)\n                self._label_as_unknown(X[i], y[i])\n\n        # forgetting mechanism\n        if self.sample_counter % self.window_size == 0:\n            self._trigger_forget()\n\n\n        return np.array(pred_label)\n\n    def get_unknown_rate(self):\n        \"\"\"Returns the unknown rate, represents the percentage of unknown samples on the total number of samples classified in the online phase.\n\n        Returns\n        -------\n        float\n            Unknown rate\n        \"\"\"\n        return len(self.short_mem) / self.sample_counter\n\n    def get_class_unknown_rate(self):\n        \"\"\"Returns the unknown rate per class. Represents the percentage of unknown samples on the total number of samples of that class seen during the stream.\n\n        Returns\n        -------\n        dict\n            Dictionary containing the unknown rate of each class\n        \"\"\"\n        return {key: val / self.class_sample_counter[key] for key, val in self.nb_class_unknown.items()}\n\n    def predict_proba_one(self,X):\n        #Function used by river algorithms to get the probability of the prediction. It is not applicable to this algorithm since it only predicts labels. \n        #It is only added as to follow River's API.\n        pass\n\n    def predict_proba_many(self, X):\n        #Function used by river algorithms to get the probability of the predictions. It is not applicable to this algorithm since it only predicts labels. \n        #It is only added as to follow River's API.\n        pass\n\n    def _label_as_unknown(self, X, y=None):\n        if y is not None:\n            self.short_mem.append(ShortMemInstance(X, self.sample_counter, y))\n            if y not in self.nb_class_unknown:\n                self.nb_class_unknown[y] = 1\n            else:\n                self.nb_class_unknown[y] += 1\n        else:\n            self.short_mem.append(ShortMemInstance(X, self.sample_counter))\n\n        if self.verbose &gt; 1:\n            print('Memory length: ', len(self.short_mem))\n        elif self.verbose &gt; 0:\n            if len(self.short_mem) % 100 == 0: print('Memory length: ', len(self.short_mem))\n\n        if len(self.short_mem) &gt;= self.min_short_mem_trigger:\n            self._novelty_detect()\n\n    def _offline(self, X_train, y_train):\n        microclusters = []\n        # in offline phase, consider all instances arriving at the same time in the microclusters:\n        timestamp = len(X_train)\n\n        for y_class in np.unique(y_train):\n            # subset with instances from each class\n            X_class = X_train[y_train == y_class]\n\n            if self.cluster_algorithm == 'kmeans':\n                class_cluster_clf = KMeans(n_clusters=self.kini, n_init='auto',\n                                            random_state=self.random_state)\n                class_cluster_clf.fit(X_class)\n                labels = class_cluster_clf.labels_\n\n            else:\n                class_cluster_clf = CluStream(m=self.kini)\n                class_cluster_clf.init_offline(X_class, seed=self.random_state)\n\n                cluster_centers = class_cluster_clf.get_partial_cluster_centers()\n\n                labels, _ = get_closest_clusters(X_class, cluster_centers)\n\n            for class_cluster in np.unique(labels):\n                # get instances in cluster\n                cluster_instances = X_class[labels == class_cluster]\n\n                microclusters.append(\n                    MicroCluster(y_class, cluster_instances, timestamp)\n                )\n\n        return microclusters\n\n    def _novelty_detect(self):\n        if self.verbose &gt; 1: print(\"Novelty detection started\")\n        possible_clusters = []\n        X = self.short_mem.get_all_points()\n        K0 = min(self.kini, len(X)) #Can't create K clusters if K is higher than the number of samples\n\n        if self.cluster_algorithm == 'kmeans':\n            cluster_clf = KMeans(n_clusters=K0, n_init='auto',\n                                 random_state=self.random_state)\n            cluster_clf.fit(X)\n            labels = cluster_clf.labels_\n\n        else:\n            cluster_clf = CluStream(m=K0)\n            cluster_clf.init_offline(X, seed=self.random_state)\n\n            cluster_centers = cluster_clf.get_partial_cluster_centers()\n\n            labels, _ = get_closest_clusters(X, cluster_centers)\n\n\n\n        for cluster_label in np.unique(labels):\n            cluster_instances = X[labels == cluster_label]\n            possible_clusters.append(\n                MicroCluster(-1, cluster_instances, self.sample_counter))\n\n        for cluster in possible_clusters:\n            if cluster.is_cohesive(self.microclusters) and cluster.is_representative(self.min_examples_cluster):\n                closest_cluster = cluster.find_closest_cluster(self.microclusters)\n                closest_distance = cluster.distance_to_centroid(closest_cluster.centroid)\n\n                threshold = self._best_threshold(cluster, closest_cluster,\n                                                self.threshold_strategy)\n\n                # TODO make these ifs elifs cleaner\n                if closest_distance &lt;= threshold:  # the new microcluster is an extension\n                    if self.verbose &gt; 1:\n                            print(\"Extension of cluster: \", closest_cluster)\n                    elif self.verbose &gt; 0:\n                        print(\"Extension of cluster: \", closest_cluster.small_str())\n\n                    cluster.label = closest_cluster.label\n\n                elif self.sleep_mem:  # look in the sleep memory, if not empty\n                    closest_cluster = cluster.find_closest_cluster(self.sleep_mem)\n                    closest_distance = cluster.distance_to_centroid(closest_cluster.centroid)\n\n                    if closest_distance &lt;= threshold:  # check again: the new microcluster is an extension\n                        if self.verbose &gt; 1:\n                            print(\"Waking cluster: \", closest_cluster)\n                        elif self.verbose &gt; 0:\n                            print(\"Waking cluster: \", closest_cluster.small_str())\n\n                        cluster.label = closest_cluster.label\n                        # awake old cluster\n                        self.sleep_mem.remove(closest_cluster)\n                        closest_cluster.timestamp = self.sample_counter\n                        self.microclusters.append(closest_cluster)\n\n                    else:  # the new microcluster is a novelty pattern\n                        cluster.label = max([cluster.label for cluster in self.microclusters]) + 1\n                        if self.verbose &gt; 1:\n                            print(\"Novel cluster: \", cluster)\n                        elif self.verbose &gt; 0:\n                            print(\"Novel cluster: \", cluster.small_str())\n\n                else:  # the new microcluster is a novelty pattern\n                    cluster.label = max([cluster.label for cluster in self.microclusters]) + 1\n                    if self.verbose &gt; 1:\n                            print(\"Novel cluster: \", cluster)\n                    elif self.verbose &gt; 0:\n                        print(\"Novel cluster: \", cluster.small_str())\n\n                # add the new cluster to the model\n                self.microclusters.append(cluster)\n\n                # remove these examples from short term memory\n                for point in cluster.instances:\n                    index = self.short_mem.index(np.array(point))\n                    y_true = self.short_mem.get_instance(index).y_true\n                    if y_true is not None:\n                        self.nb_class_unknown[y_true] -= 1\n                    self.short_mem.remove(index)\n\n\n    def _best_threshold(self, new_cluster, closest_cluster, strategy):\n        def run_strategy_1():\n            factor_1 = self.threshold_factor\n            # factor_1 = 5  # good for artificial, separated data sets\n            return factor_1 * np.std(closest_cluster.distance_to_centroid(closest_cluster.instances))\n\n        if strategy == 1:\n            return run_strategy_1()\n        else:\n            factor_2 = factor_3 = self.threshold_factor\n            # factor_2 = factor_3 = 1.2 # good for artificial, separated data sets\n            clusters_same_class = self._get_clusters_in_class(closest_cluster.label)\n            if len(clusters_same_class) == 1:\n                return run_strategy_1()\n            else:\n                class_centroids = np.array([cluster.centroid for cluster in clusters_same_class])\n                distances = closest_cluster.distance_to_centroid(class_centroids)\n                if strategy == 2:\n                    return factor_2 * np.max(distances)\n                elif strategy == 3:\n                    return factor_3 * np.mean(distances)\n\n    def _get_clusters_in_class(self, label):\n        return [cluster for cluster in self.microclusters if cluster.label == label]\n\n    def _trigger_forget(self):\n        for cluster in list(self.microclusters):\n            # Remove cluster if it hasn't been updated for more than window_size time and there is more than 1 cluster\n            if (cluster.timestamp &lt; self.sample_counter - self.window_size) and (len(self.microclusters) &gt; 1):\n                if self.verbose &gt; 1:\n                    print(\"Forgetting cluster: \", cluster)\n                elif self.verbose &gt; 0:\n                    print(\"Forgetting cluster: \", cluster.small_str())\n\n                self.sleep_mem.append(cluster)\n                self.microclusters.remove(cluster)\n\n        for instance in self.short_mem.get_all_instances():\n            if instance.timestamp &lt; self.sample_counter - self.window_size:\n                index = self.short_mem.index(instance)\n                y_true = instance.y_true\n                if y_true is not None:\n                    self.nb_class_unknown[y_true] -= 1\n                self.short_mem.remove(index)\n</code></pre>"},{"location":"reference/model/minas/#streamndr.model.minas.Minas.get_class_unknown_rate","title":"<code>get_class_unknown_rate()</code>","text":"<p>Returns the unknown rate per class. Represents the percentage of unknown samples on the total number of samples of that class seen during the stream.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing the unknown rate of each class</p> Source code in <code>streamndr/model/minas.py</code> <pre><code>def get_class_unknown_rate(self):\n    \"\"\"Returns the unknown rate per class. Represents the percentage of unknown samples on the total number of samples of that class seen during the stream.\n\n    Returns\n    -------\n    dict\n        Dictionary containing the unknown rate of each class\n    \"\"\"\n    return {key: val / self.class_sample_counter[key] for key, val in self.nb_class_unknown.items()}\n</code></pre>"},{"location":"reference/model/minas/#streamndr.model.minas.Minas.get_unknown_rate","title":"<code>get_unknown_rate()</code>","text":"<p>Returns the unknown rate, represents the percentage of unknown samples on the total number of samples classified in the online phase.</p> <p>Returns:</p> Type Description <code>float</code> <p>Unknown rate</p> Source code in <code>streamndr/model/minas.py</code> <pre><code>def get_unknown_rate(self):\n    \"\"\"Returns the unknown rate, represents the percentage of unknown samples on the total number of samples classified in the online phase.\n\n    Returns\n    -------\n    float\n        Unknown rate\n    \"\"\"\n    return len(self.short_mem) / self.sample_counter\n</code></pre>"},{"location":"reference/model/minas/#streamndr.model.minas.Minas.learn_many","title":"<code>learn_many(X, y, w=1.0)</code>","text":"<p>Represents the offline phase of the algorithm. Receives a number of samples and their given labels and learns all of the known classes.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame or ndarray</code> <p>Samples to be learned by the model</p> required <code>y</code> <code>list of int</code> <p>Labels corresponding to the given samples, must be the same length as the number of samples</p> required <code>w</code> <code>float</code> <p>Weights, not used, by default 1.0</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Minas</code> <p>Fitted estimator</p> Source code in <code>streamndr/model/minas.py</code> <pre><code>def learn_many(self, X, y, w=1.0):\n    \"\"\"Represents the offline phase of the algorithm. Receives a number of samples and their given labels and learns all of the known classes.\n\n    Parameters\n    ----------\n    X : pandas.DataFrame or numpy.ndarray\n        Samples to be learned by the model\n    y : list of int\n        Labels corresponding to the given samples, must be the same length as the number of samples\n    w : float, optional\n        Weights, not used, by default 1.0\n\n    Returns\n    -------\n    Minas\n        Fitted estimator\n    \"\"\"\n    if isinstance(X, pd.DataFrame):\n        X = X.to_numpy()\n\n    self.microclusters = self._offline(X, y)\n    self.before_offline_phase = False\n\n    return self\n</code></pre>"},{"location":"reference/model/minas/#streamndr.model.minas.Minas.learn_one","title":"<code>learn_one(x, y, w=1.0)</code>","text":"<p>Function used by river algorithms to learn one sample. It is not applicable to this algorithm since the offline phase requires all samples to arrive at once. It is only added as to follow River's API.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>dict</code> <p>Sample</p> required <code>y</code> <code>int</code> <p>Label of the given sample</p> required <code>w</code> <code>float</code> <p>Weight, not used, by default 1.0</p> <code>1.0</code> Source code in <code>streamndr/model/minas.py</code> <pre><code>def learn_one(self, x, y, w=1.0):\n    \"\"\"Function used by river algorithms to learn one sample. It is not applicable to this algorithm since the offline phase requires all samples\n    to arrive at once. It is only added as to follow River's API.\n\n    Parameters\n    ----------\n    x : dict\n        Sample\n    y : int\n        Label of the given sample\n    w : float, optional\n        Weight, not used, by default 1.0\n    \"\"\"\n    # Not applicable\n    pass\n</code></pre>"},{"location":"reference/model/minas/#streamndr.model.minas.Minas.predict_many","title":"<code>predict_many(X, y=None)</code>","text":"<p>Represents the online phase. Receives multiple samples, for each sample predict its label and adds it to the cluster if it is a known class.  Otherwise, if it's unknown, it is added to the short term memory and novelty detection is performed once the trigger has been reached (min_short_mem_trigger).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame or ndarray</code> <p>Samples</p> required <code>y</code> <code>list of int</code> <p>True y values of the samples, if available. Only used for metric evaluation (UnkRate).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of length len(X) containing the predicted labels, predicts -1 if the corresponding sample labeled as unknown</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the model has not been trained first with learn_many() (offline phase)</p> Source code in <code>streamndr/model/minas.py</code> <pre><code>def predict_many(self, X, y=None):\n    \"\"\"Represents the online phase. Receives multiple samples, for each sample predict its label and adds it to the cluster if it is a known class. \n    Otherwise, if it's unknown, it is added to the short term memory and novelty detection is performed once the trigger has been reached (min_short_mem_trigger).\n\n    Parameters\n    ----------\n    X : pandas.DataFrame or numpy.ndarray\n        Samples\n    y : list of int\n        True y values of the samples, if available. Only used for metric evaluation (UnkRate).\n\n    Returns\n    -------\n    numpy.ndarray\n        Array of length len(X) containing the predicted labels, predicts -1 if the corresponding sample labeled as unknown\n\n    Raises\n    ------\n    Exception\n        If the model has not been trained first with learn_many() (offline phase)\n    \"\"\"\n    if self.before_offline_phase:\n        raise Exception(\"Model must be fitted first\")\n\n    if isinstance(X, pd.DataFrame):\n        X = X.to_numpy() #Converting DataFrame to numpy array\n\n    # Finding closest clusters for received samples\n    closest_clusters, _ = get_closest_clusters(X, [microcluster.centroid for microcluster in self.microclusters])\n\n    pred_label = []\n\n    for i in range(len(closest_clusters)):\n        self.sample_counter += 1\n        if y is not None:\n            if y[i] not in self.class_sample_counter:\n                self.class_sample_counter[y[i]] = 1\n            else:\n                self.class_sample_counter[y[i]] += 1\n\n        if closest_clusters[i] != -1:\n            closest_cluster = self.microclusters[closest_clusters[i]]\n\n            if closest_cluster.encompasses(X[i]):  # classify in this cluster\n                pred_label.append(closest_cluster.label)\n\n                closest_cluster.update_cluster(X[i], self.sample_counter, self.update_summary)\n\n            else:  # classify as unknown\n                pred_label.append(-1)\n                self._label_as_unknown(X[i], y[i])\n\n        else: # classify as unknown\n            pred_label.append(-1)\n            self._label_as_unknown(X[i], y[i])\n\n    # forgetting mechanism\n    if self.sample_counter % self.window_size == 0:\n        self._trigger_forget()\n\n\n    return np.array(pred_label)\n</code></pre>"},{"location":"reference/model/minas/#streamndr.model.minas.Minas.predict_one","title":"<code>predict_one(X, y=None)</code>","text":"<p>Represents the online phase. Equivalent to predict_many() with only one sample. Receives only one sample, predict its label and adds  it to the cluster if it is a known class. Otherwise, if it's unknown, it is added to the short term memory and novelty detection is  performed once the trigger has been reached (min_short_mem_trigger).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>dict</code> <p>Sample</p> required <code>y</code> <code>int</code> <p>True y value of the sample, if available. Only used for metric evaluation (UnkRate).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Label predicted for the given sample, predicts -1 if labeled as unknown</p> Source code in <code>streamndr/model/minas.py</code> <pre><code>def predict_one(self, X, y=None):\n    \"\"\"Represents the online phase. Equivalent to predict_many() with only one sample. Receives only one sample, predict its label and adds \n    it to the cluster if it is a known class. Otherwise, if it's unknown, it is added to the short term memory and novelty detection is \n    performed once the trigger has been reached (min_short_mem_trigger).\n\n    Parameters\n    ----------\n    X : dict\n        Sample\n    y : int\n        True y value of the sample, if available. Only used for metric evaluation (UnkRate).\n\n    Returns\n    -------\n    numpy.ndarray\n        Label predicted for the given sample, predicts -1 if labeled as unknown\n    \"\"\"\n    return self.predict_many(np.array(list(X.values()))[None,:], [y])\n</code></pre>"},{"location":"reference/utils/cluster_utils/","title":"cluster_utils","text":""},{"location":"reference/utils/cluster_utils/#streamndr.utils.cluster_utils.get_closest_clusters","title":"<code>get_closest_clusters(X, centroids)</code>","text":"<p>Function returning the closest centroid and distance for each given point.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Array of points</p> required <code>centroids</code> <code>ndarray</code> <p>Array of centroids</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Index of the closest cluster for each point</p> <code>ndarray</code> <p>Distance to the closest cluster for each point</p> Source code in <code>streamndr/utils/cluster_utils.py</code> <pre><code>def get_closest_clusters(X, centroids):\n    \"\"\"Function returning the closest centroid and distance for each given point.\n\n    Parameters\n    ----------\n    X : numpy.ndarray\n        Array of points\n    centroids : numpy.ndarray\n        Array of centroids\n\n    Returns\n    -------\n    numpy.ndarray\n        Index of the closest cluster for each point\n    numpy.ndarray\n        Distance to the closest cluster for each point\n    \"\"\"\n    if len(centroids) == 0:\n        print(\"No clusters\")\n        return np.full(len(X), -1), -1\n\n    centroids = np.array(centroids)\n    norm_dists = np.zeros((X.shape[0],centroids.shape[0]))\n\n    # Cut into batches if there are too many samples to save on memory\n    for idx in range(math.ceil(X.shape[0]/MAX_MEMORY_SIZE)):\n        sl = slice(idx*MAX_MEMORY_SIZE, (idx+1)*MAX_MEMORY_SIZE)\n        norm_dists[sl] = np.linalg.norm(np.subtract(X[sl, :, None], np.transpose(centroids)), axis=1)\n\n    return np.argmin(norm_dists, axis=1), np.amin(norm_dists, axis=1)\n</code></pre>"},{"location":"reference/utils/cluster_utils/#streamndr.utils.cluster_utils.qnsc","title":"<code>qnsc(pseudopoints, model, q_p=5)</code>","text":"<p>Computes the q-neighborhood silhouette coefficient, as described in [1].</p> <p>[1] Masud, Mohammad, et al. \"Classification and novel class detection in concept-drifting data streams under time constraints.\"  IEEE Transactions on knowledge and data engineering 23.6 (2010): 859-874.</p> <p>Parameters:</p> Name Type Description Default <code>pseudopoints</code> <code>ndarray</code> <p>List of points</p> required <code>model</code> <code>list of MicroCluster</code> <p>Microclusters representing a model</p> required <code>q_p</code> <code>int</code> <p>Number of neighboring points to consider</p> <code>5</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>List of computed qnscs for each point</p> Source code in <code>streamndr/utils/cluster_utils.py</code> <pre><code>def qnsc(pseudopoints, model, q_p=5):\n    \"\"\"Computes the q-neighborhood silhouette coefficient, as described in [1].\n\n    [1] Masud, Mohammad, et al. \"Classification and novel class detection in concept-drifting data streams under time constraints.\" \n    IEEE Transactions on knowledge and data engineering 23.6 (2010): 859-874.\n\n    Parameters\n    ----------\n    pseudopoints : numpy.ndarray\n        List of points\n    model : list of MicroCluster\n        Microclusters representing a model\n    q_p : int\n        Number of neighboring points to consider\n\n    Returns\n    -------\n    numpy.ndarray\n        List of computed qnscs for each point\n    \"\"\"\n    qnscs = []\n    cluster_by_label = {}\n\n    for cluster in model:\n        if cluster.label not in cluster_by_label:\n            cluster_by_label[cluster.label] = []\n\n        cluster_by_label[cluster.label].append(cluster.centroid)\n\n\n    #1 - Find the Q closest distances for each point\n    distances = np.linalg.norm(pseudopoints[:, np.newaxis] - pseudopoints, axis=2)\n\n    # Set the diagonal elements to a large value to avoid selecting the same point\n    np.fill_diagonal(distances, np.inf)\n    q = min(q_p, len(pseudopoints)-2) #-2 because we can't select the same point\n    indices = np.argpartition(distances, q, axis=1)[:, :q] \n\n    # Retrieve the Q minimum distances for each point\n    min_distances = np.take_along_axis(distances, indices, axis=1)\n    dc_outs = np.mean(min_distances, axis=1)\n\n    for i, point in enumerate(pseudopoints):\n        dc_out = dc_outs[i]\n        dc_q = []\n        for _, clusters in cluster_by_label.items():\n            dists = []\n            for centroid in clusters:\n                dists.append(np.linalg.norm(point-centroid))\n\n            q = min(q_p, len(dists))\n            q_closest = np.partition(dists, q-1)[:q]\n            dc_q.append(np.sum(q_closest)/q)\n\n        dcmin_q = np.min(dc_q)\n        qnsc = (dcmin_q - dc_out) / max(dcmin_q, dc_out)\n\n        qnscs.append(qnsc)\n\n\n    return qnscs\n</code></pre>"},{"location":"reference/utils/data_structure/","title":"data_structure","text":""},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.ClusterModel","title":"<code>ClusterModel</code>","text":"<p>Data class which represent a model containing a list of microclusters and a list of labels which it was trained on</p> <p>Attributes:</p> Name Type Description <code>microclusters</code> <code>list of MicroCluster</code> <p>List of MicroClusters representing the model</p> <code>timestamp</code> <code>list of int</code> <p>List of labels on which the model was trained on</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>class ClusterModel:\n    \"\"\"Data class which represent a model containing a list of microclusters and a list of labels which it was trained on\n\n    Attributes\n    ----------\n    microclusters : list of MicroCluster\n        List of MicroClusters representing the model\n    timestamp : list of int\n        List of labels on which the model was trained on\n    \"\"\"\n    def __init__(self, microclusters, labels):\n        self.microclusters = microclusters\n        self.labels = labels\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.ImpurityBasedCluster","title":"<code>ImpurityBasedCluster</code>","text":"<p>             Bases: <code>MicroCluster</code></p> <p>Cluster which implements the concept of entropy and dissimilarity if samples of a same class albel are not in the same cluster [1].</p> <p>[1] Masud, Mohammad M., et al. \"A practical approach to classify evolving data streams: Training with limited amount of labeled data.\"  2008 Eighth IEEE International Conference on Data Mining. IEEE, 2008.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>int</code> <p>Label of the cluster</p> required <code>centroid</code> <code>ndarray</code> <p>Current centroid of the cluster</p> required <p>Attributes:</p> Name Type Description <code>entropy</code> <code>int</code> <p>Entropy of the cluster as defined in [1]</p> <code>number_of_labeled_samples</code> <code>int</code> <p>Number of labeled samples currently in the cluster</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>class ImpurityBasedCluster(MicroCluster):\n    \"\"\"Cluster which implements the concept of entropy and dissimilarity if samples of a same class albel are not in the same cluster [1].\n\n    [1] Masud, Mohammad M., et al. \"A practical approach to classify evolving data streams: Training with limited amount of labeled data.\" \n    2008 Eighth IEEE International Conference on Data Mining. IEEE, 2008.\n\n    Parameters\n    ----------\n    label : int\n        Label of the cluster\n    centroid : numpy.ndarray\n        Current centroid of the cluster\n\n    Attributes\n    ----------\n    entropy : int\n        Entropy of the cluster as defined in [1]\n    number_of_labeled_samples : int\n        Number of labeled samples currently in the cluster\n    \"\"\"\n    def __init__(self,\n                 label,\n                 centroid):\n\n        super().__init__(label)\n\n        self.centroid = centroid\n        self.samples_by_label = {}\n\n        self.entropy = 0\n        self.number_of_labeled_samples = 0\n\n    def add_sample(self, sample, update_summary=False):\n        \"\"\"Add a sample to the cluster, the sample can be labeled or not. Expects -1 as the label for an unlabeled sample.\n\n        Parameters\n        ----------\n        sample : ShortMemInstance\n            Instance to add to the cluster\n        update_summary : bool\n            Whether or not to update the microcluster supplementary properties (mean distance &amp; squared sum) with this new point\n        \"\"\"\n\n        if sample.y_true not in self.samples_by_label:\n            self.samples_by_label[sample.y_true] = 0\n\n\n        self.samples_by_label[sample.y_true] += 1\n\n\n        if sample.y_true != -1:\n            self.number_of_labeled_samples += 1\n\n        X = sample.point\n        if self.instances is not None:\n            self.instances.append(sample.point)\n            self.linear_sum = np.sum([self.linear_sum, X], axis=0)\n        else:\n            self.instances = [sample.point]\n            self.linear_sum = X\n\n        self.n += 1\n\n        if update_summary:\n            self.mean_distance = (self.n * self.mean_distance + self.distance_to_centroid(X)) / (self.n)\n            self.squared_sum = np.sum([self.squared_sum, np.square(X).sum()], axis=0)\n\n    def remove_sample(self, sample, update_summary=False):\n        \"\"\"Remove a sample from the cluster, the sample can be labeled or not. Expects -1 as the label for an unlabeled sample.\n\n        Parameters\n        ----------\n        sample : ShortMemInstance\n            Instance to remove from the cluster\n        update_summary : bool\n            Whether or not to update the microcluster supplementary properties (mean distance &amp; squared sum) with this new point\n        \"\"\"\n        self.samples_by_label[sample.y_true] -= 1\n\n        if sample.y_true != -1:\n            self.number_of_labeled_samples -= 1\n\n        self.instances.remove(sample.point)\n        self.n -= 1\n        X = sample.point\n        self.linear_sum = np.sum([self.linear_sum, -1*X], axis=0)\n\n        if update_summary:        \n            self.mean_distance = (self.n * self.mean_distance - self.distance_to_centroid(X)) / (self.n)\n            self.squared_sum = np.sum([self.squared_sum, -1*np.square(X).sum()], axis=0)\n\n    def update_entropy(self):\n        label_probabilities = [self.calculate_label_probability(label) for label in self.samples_by_label if label != -1]\n        self.entropy = -sum(p * math.log(p) for p in label_probabilities if p &gt; 0)\n\n    def calculate_label_probability(self, label):\n        return self.samples_by_label[label] / self.number_of_labeled_samples\n\n    def dissimilarity_count(self, labeled_sample):\n        if labeled_sample.y_true not in self.samples_by_label:\n            return self.number_of_labeled_samples\n        if labeled_sample.y_true == -1:\n            return 0\n\n        return self.number_of_labeled_samples - self.samples_by_label[labeled_sample.y_true]\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.ImpurityBasedCluster.add_sample","title":"<code>add_sample(sample, update_summary=False)</code>","text":"<p>Add a sample to the cluster, the sample can be labeled or not. Expects -1 as the label for an unlabeled sample.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>ShortMemInstance</code> <p>Instance to add to the cluster</p> required <code>update_summary</code> <code>bool</code> <p>Whether or not to update the microcluster supplementary properties (mean distance &amp; squared sum) with this new point</p> <code>False</code> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def add_sample(self, sample, update_summary=False):\n    \"\"\"Add a sample to the cluster, the sample can be labeled or not. Expects -1 as the label for an unlabeled sample.\n\n    Parameters\n    ----------\n    sample : ShortMemInstance\n        Instance to add to the cluster\n    update_summary : bool\n        Whether or not to update the microcluster supplementary properties (mean distance &amp; squared sum) with this new point\n    \"\"\"\n\n    if sample.y_true not in self.samples_by_label:\n        self.samples_by_label[sample.y_true] = 0\n\n\n    self.samples_by_label[sample.y_true] += 1\n\n\n    if sample.y_true != -1:\n        self.number_of_labeled_samples += 1\n\n    X = sample.point\n    if self.instances is not None:\n        self.instances.append(sample.point)\n        self.linear_sum = np.sum([self.linear_sum, X], axis=0)\n    else:\n        self.instances = [sample.point]\n        self.linear_sum = X\n\n    self.n += 1\n\n    if update_summary:\n        self.mean_distance = (self.n * self.mean_distance + self.distance_to_centroid(X)) / (self.n)\n        self.squared_sum = np.sum([self.squared_sum, np.square(X).sum()], axis=0)\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.ImpurityBasedCluster.remove_sample","title":"<code>remove_sample(sample, update_summary=False)</code>","text":"<p>Remove a sample from the cluster, the sample can be labeled or not. Expects -1 as the label for an unlabeled sample.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>ShortMemInstance</code> <p>Instance to remove from the cluster</p> required <code>update_summary</code> <code>bool</code> <p>Whether or not to update the microcluster supplementary properties (mean distance &amp; squared sum) with this new point</p> <code>False</code> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def remove_sample(self, sample, update_summary=False):\n    \"\"\"Remove a sample from the cluster, the sample can be labeled or not. Expects -1 as the label for an unlabeled sample.\n\n    Parameters\n    ----------\n    sample : ShortMemInstance\n        Instance to remove from the cluster\n    update_summary : bool\n        Whether or not to update the microcluster supplementary properties (mean distance &amp; squared sum) with this new point\n    \"\"\"\n    self.samples_by_label[sample.y_true] -= 1\n\n    if sample.y_true != -1:\n        self.number_of_labeled_samples -= 1\n\n    self.instances.remove(sample.point)\n    self.n -= 1\n    X = sample.point\n    self.linear_sum = np.sum([self.linear_sum, -1*X], axis=0)\n\n    if update_summary:        \n        self.mean_distance = (self.n * self.mean_distance - self.distance_to_centroid(X)) / (self.n)\n        self.squared_sum = np.sum([self.squared_sum, -1*np.square(X).sum()], axis=0)\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.MicroCluster","title":"<code>MicroCluster</code>","text":"<p>             Bases: <code>object</code></p> <p>A representation of a cluster with compressed information.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>int</code> <p>Label associated with this microcluster</p> required <code>instances</code> <code>ndarray</code> <p>Instances in this microcluster, preferably these would not be stored if not needed using keep_instances=False. Will be converted to Python list for append performance.</p> <code>None</code> <code>timestamp</code> <code>int</code> <p>Timestamp this microcluster was last updated, used for forgetting mechanisms</p> <code>0</code> <code>keep_instances</code> <code>bool</code> <p>Whether or not to store the instances within the microcluster. Should preferably set to false, but some implementations require access to the instances</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>n</code> <code>int</code> <p>Number of instances stored in this microcluster</p> <code>linear_sum</code> <code>ndarray</code> <p>Linear sum of the points belonging to this microcluster</p> <code>squared_sum</code> <code>ndarray</code> <p>Sum of the squared l2 norms of all samples belonging to this microcluster</p> <code>centroid</code> <code>ndarray</code> <p>Centroid coordinates of the microcluster</p> <code>max_distance</code> <code>ndarray</code> <p>Maximum distance between a point belonging to the microcluster and its centroid</p> <code>mean_distance</code> <code>ndarray</code> <p>Mean distance of the distances between the cluster's points and its centroid</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>class MicroCluster(object):\n    \"\"\"A representation of a cluster with compressed information.\n\n    Parameters\n    ----------\n    label : int\n        Label associated with this microcluster\n    instances : numpy.ndarray\n        Instances in this microcluster, preferably these would not be stored if not needed using keep_instances=False. Will be converted to Python list for append performance.\n    timestamp : int\n        Timestamp this microcluster was last updated, used for forgetting mechanisms  \n    keep_instances : bool\n        Whether or not to store the instances within the microcluster. Should preferably set to false, but some implementations require\n        access to the instances\n\n    Attributes\n    ----------\n    n : int\n        Number of instances stored in this microcluster\n    linear_sum : numpy.ndarray\n        Linear sum of the points belonging to this microcluster\n    squared_sum : numpy.ndarray\n        Sum of the squared l2 norms of all samples belonging to this microcluster\n    centroid : numpy.ndarray\n        Centroid coordinates of the microcluster\n    max_distance : numpy.ndarray\n        Maximum distance between a point belonging to the microcluster and its centroid\n    mean_distance : numpy.ndarray\n        Mean distance of the distances between the cluster's points and its centroid\n    \"\"\"\n\n    def __init__(self,\n                 label,  # the class the microcluster belongs to\n                 instances=None,\n                 timestamp=0, \n                 keep_instances=True #Required True for MINAS\n                 ):\n\n        # TODO: remove instances entirely so it doesn't need to be stored in memory; Might not be possible because of _best_threshold used by MINAS which needs instances\n        super(MicroCluster, self).__init__()\n        self.label = label\n\n        if instances is not None:\n            self.instances = instances.tolist()\n            self.n = len(instances)\n            self.linear_sum = instances.sum(axis=0)\n\n            # Sum of the squared l2 norms of all samples belonging to a microcluster:\n            self.squared_sum = np.square(np.linalg.norm(self.instances, axis=1)).sum()\n            # self.squared_sum = np.square(instances).sum(axis=0)  # From CluSTREAM paper\n            self.centroid = self.linear_sum / self.n\n            self.max_distance = np.max(self.distance_to_centroid(instances))\n            self.mean_distance = np.mean(self.distance_to_centroid(instances))\n            self.update_properties()\n\n        else:\n            self.instances = None\n            self.n = 0\n            self.linear_sum = 0\n            self.squared_sum = 0\n            self.max_distance = 0\n            self.mean_distance = 0\n\n        self.timestamp = timestamp\n\n\n        if not keep_instances:\n            self.instances = None\n\n    def __str__(self):\n        \"\"\"Returns string representation of a microcluster.\n\n        Returns\n        -------\n        str\n            String representation of microcluster\n        \"\"\"\n\n        return f\"\"\"Target class {self.label}\n                # of instances: {self.n}\n                Linear sum: {self.linear_sum}\n                Squared sum: {self.squared_sum}\n                Centroid: {self.centroid}\n                Radius: {self.radius}\n                Timestamp of last change: {self.timestamp}\"\"\"\n\n    def small_str(self):\n        \"\"\"Returns string representation of a microcluster.\n\n        Returns\n        -------\n        str\n            Small string representation of microcluster\n        \"\"\"\n\n        return f\"\"\"Target class {self.label}\n                # of instances: {self.n}\n                Timestamp of last change: {self.timestamp}\"\"\"\n\n    def get_radius(self):\n        \"\"\"Returns radius of the microcluster.\n\n        Returns\n        -------\n        float\n            Radius of the microcluster\n        \"\"\"\n\n        factor = 1.5\n        # from BIRCH Wikipedia\n        diff = (self.squared_sum / self.n) - np.dot(self.centroid, self.centroid)\n        if diff &gt; 1e-15:\n            return factor * np.sqrt(diff)\n        else:  # in this case diff should be zero, but sometimes it's an infinitesimal difference\n            return 0\n        # from MINAS paper:\n        #return factor*np.std(self.distance_to_centroid(self.instances))\n\n    def distance_to_centroid(self, X):\n        \"\"\"Returns distance from X to centroid of this cluster.\n\n        Parameters\n        ----------\n        X : numpy.ndarray or list\n            Point or multiple points\n\n        Returns\n        -------\n        numpy.ndarray\n            Distance from X to the microcluster's centroid\n        \"\"\"\n        if not isinstance(X, np.ndarray):\n            X = np.array(X)\n\n        if len(X.shape) == 1:  # X is only one point\n            return np.linalg.norm(X - self.centroid)\n        else:  # X contains several points\n            return np.linalg.norm(X - self.centroid, axis=1)\n\n    def encompasses(self, X):\n        \"\"\"Checks if point X is inside this microcluster. The point X is considered within the microcluster if the distance \n        between the point and the microcluster's centroid is less than the radius of the microcluster.\n\n        Parameters\n        ----------\n        X : numpy.ndarray\n            One point\n\n        Returns\n        -------\n        bool\n            If the point distance to centroid is contained within the microcluster or not\n        \"\"\"\n\n        return np.less(self.distance_to_centroid(X), self.radius)\n\n    def find_closest_cluster(self, clusters):\n        \"\"\"Finds closest microcluster to this one among passed microclusters.\n\n        Parameters\n        ----------\n        clusters : list of MicroCluster\n\n        Returns\n        -------\n        MicroCluster\n            Closest microcluster\n        \"\"\"\n\n        return min(clusters, key=lambda cl: cl.distance_to_centroid(self.centroid))\n\n    def update_cluster(self, X, timestamp, update_summary):\n        \"\"\"Adds point received in parameter to the cluster and update cluster's centroid if wanted.\n\n        Parameters\n        ----------\n        X : numpy.ndarray\n            One point\n        timestamp : int\n            Timestamp when this point was added to this microcluster\n        update_summary : bool\n            Whether or not to update the microcluster properties with this new point\n        \"\"\"\n\n        assert len(X.shape) == 1  # it's just one point\n        self.timestamp = timestamp\n\n        if self.instances is not None:\n            self.instances.append(X)\n\n        if update_summary:\n            self.mean_distance = (self.n * self.mean_distance + self.distance_to_centroid(X)) / (self.n + 1)\n            self.n += 1\n            self.linear_sum = np.sum([self.linear_sum, X], axis=0)\n            self.squared_sum = np.sum([self.squared_sum, np.square(X).sum()], axis=0)\n            self.update_properties()\n\n    def update_properties(self):\n        \"\"\"Updates centroid and radius based on current cluster properties.\"\"\"\n        self.centroid = self.linear_sum / self.n\n\n        if self.instances is not None:\n            self.radius = self.get_radius()\n            if np.max(self.distance_to_centroid(self.instances)) &gt; self.max_distance:\n                self.max_distance = np.max(self.distance_to_centroid(self.instances))\n\n    def is_cohesive(self, clusters):\n        \"\"\"Verifies if this cluster is cohesive for novelty detection purposes.\n        A new micro-cluster is cohesive if its silhouette coefficient is larger than 0.\n        'b' represents the Euclidean distance between the centroid of the new micro-cluster and the centroid of its\n        closest micro-cluster, and 'a' represents the standard deviation of the distances between the examples of the\n        new micro-cluster and the centroid of the new micro-cluster.\n\n        Parameters\n        ----------\n        clusters : List of MicroCluster\n            Existing known micro-clusters\n\n        Returns\n        -------\n        bool\n            If the cluster is cohesive (silhouette&gt;0) or not\n        \"\"\"\n        b = self.distance_to_centroid(self.find_closest_cluster(clusters).centroid)\n        a = np.std(self.distance_to_centroid(self.instances))\n        silhouette = (b - a) / max(a, b)  # hm, this is always positive if b &gt; a\n        return silhouette &gt; 0\n\n    def is_representative(self, min_examples):\n        \"\"\"Verifies if this cluster is representative for novelty detection purposes.\n        A new micro-cluster is representative if it contains a minimal number of examples,\n        where this number is a user-defined parameter.\n\n        Parameters\n        ----------\n        min_examples : int\n            The number of samples the microcluster needs to have to be considered representative.\n\n        Returns\n        -------\n        bool\n            If the cluster is representative or not\n        \"\"\"\n        return self.n &gt;= min_examples\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.MicroCluster.__str__","title":"<code>__str__()</code>","text":"<p>Returns string representation of a microcluster.</p> <p>Returns:</p> Type Description <code>str</code> <p>String representation of microcluster</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def __str__(self):\n    \"\"\"Returns string representation of a microcluster.\n\n    Returns\n    -------\n    str\n        String representation of microcluster\n    \"\"\"\n\n    return f\"\"\"Target class {self.label}\n            # of instances: {self.n}\n            Linear sum: {self.linear_sum}\n            Squared sum: {self.squared_sum}\n            Centroid: {self.centroid}\n            Radius: {self.radius}\n            Timestamp of last change: {self.timestamp}\"\"\"\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.MicroCluster.distance_to_centroid","title":"<code>distance_to_centroid(X)</code>","text":"<p>Returns distance from X to centroid of this cluster.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray or list</code> <p>Point or multiple points</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Distance from X to the microcluster's centroid</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def distance_to_centroid(self, X):\n    \"\"\"Returns distance from X to centroid of this cluster.\n\n    Parameters\n    ----------\n    X : numpy.ndarray or list\n        Point or multiple points\n\n    Returns\n    -------\n    numpy.ndarray\n        Distance from X to the microcluster's centroid\n    \"\"\"\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n\n    if len(X.shape) == 1:  # X is only one point\n        return np.linalg.norm(X - self.centroid)\n    else:  # X contains several points\n        return np.linalg.norm(X - self.centroid, axis=1)\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.MicroCluster.encompasses","title":"<code>encompasses(X)</code>","text":"<p>Checks if point X is inside this microcluster. The point X is considered within the microcluster if the distance  between the point and the microcluster's centroid is less than the radius of the microcluster.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>One point</p> required <p>Returns:</p> Type Description <code>bool</code> <p>If the point distance to centroid is contained within the microcluster or not</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def encompasses(self, X):\n    \"\"\"Checks if point X is inside this microcluster. The point X is considered within the microcluster if the distance \n    between the point and the microcluster's centroid is less than the radius of the microcluster.\n\n    Parameters\n    ----------\n    X : numpy.ndarray\n        One point\n\n    Returns\n    -------\n    bool\n        If the point distance to centroid is contained within the microcluster or not\n    \"\"\"\n\n    return np.less(self.distance_to_centroid(X), self.radius)\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.MicroCluster.find_closest_cluster","title":"<code>find_closest_cluster(clusters)</code>","text":"<p>Finds closest microcluster to this one among passed microclusters.</p> <p>Parameters:</p> Name Type Description Default <code>clusters</code> <code>list of MicroCluster</code> required <p>Returns:</p> Type Description <code>MicroCluster</code> <p>Closest microcluster</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def find_closest_cluster(self, clusters):\n    \"\"\"Finds closest microcluster to this one among passed microclusters.\n\n    Parameters\n    ----------\n    clusters : list of MicroCluster\n\n    Returns\n    -------\n    MicroCluster\n        Closest microcluster\n    \"\"\"\n\n    return min(clusters, key=lambda cl: cl.distance_to_centroid(self.centroid))\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.MicroCluster.get_radius","title":"<code>get_radius()</code>","text":"<p>Returns radius of the microcluster.</p> <p>Returns:</p> Type Description <code>float</code> <p>Radius of the microcluster</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def get_radius(self):\n    \"\"\"Returns radius of the microcluster.\n\n    Returns\n    -------\n    float\n        Radius of the microcluster\n    \"\"\"\n\n    factor = 1.5\n    # from BIRCH Wikipedia\n    diff = (self.squared_sum / self.n) - np.dot(self.centroid, self.centroid)\n    if diff &gt; 1e-15:\n        return factor * np.sqrt(diff)\n    else:  # in this case diff should be zero, but sometimes it's an infinitesimal difference\n        return 0\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.MicroCluster.is_cohesive","title":"<code>is_cohesive(clusters)</code>","text":"<p>Verifies if this cluster is cohesive for novelty detection purposes. A new micro-cluster is cohesive if its silhouette coefficient is larger than 0. 'b' represents the Euclidean distance between the centroid of the new micro-cluster and the centroid of its closest micro-cluster, and 'a' represents the standard deviation of the distances between the examples of the new micro-cluster and the centroid of the new micro-cluster.</p> <p>Parameters:</p> Name Type Description Default <code>clusters</code> <code>List of MicroCluster</code> <p>Existing known micro-clusters</p> required <p>Returns:</p> Type Description <code>bool</code> <p>If the cluster is cohesive (silhouette&gt;0) or not</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def is_cohesive(self, clusters):\n    \"\"\"Verifies if this cluster is cohesive for novelty detection purposes.\n    A new micro-cluster is cohesive if its silhouette coefficient is larger than 0.\n    'b' represents the Euclidean distance between the centroid of the new micro-cluster and the centroid of its\n    closest micro-cluster, and 'a' represents the standard deviation of the distances between the examples of the\n    new micro-cluster and the centroid of the new micro-cluster.\n\n    Parameters\n    ----------\n    clusters : List of MicroCluster\n        Existing known micro-clusters\n\n    Returns\n    -------\n    bool\n        If the cluster is cohesive (silhouette&gt;0) or not\n    \"\"\"\n    b = self.distance_to_centroid(self.find_closest_cluster(clusters).centroid)\n    a = np.std(self.distance_to_centroid(self.instances))\n    silhouette = (b - a) / max(a, b)  # hm, this is always positive if b &gt; a\n    return silhouette &gt; 0\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.MicroCluster.is_representative","title":"<code>is_representative(min_examples)</code>","text":"<p>Verifies if this cluster is representative for novelty detection purposes. A new micro-cluster is representative if it contains a minimal number of examples, where this number is a user-defined parameter.</p> <p>Parameters:</p> Name Type Description Default <code>min_examples</code> <code>int</code> <p>The number of samples the microcluster needs to have to be considered representative.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>If the cluster is representative or not</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def is_representative(self, min_examples):\n    \"\"\"Verifies if this cluster is representative for novelty detection purposes.\n    A new micro-cluster is representative if it contains a minimal number of examples,\n    where this number is a user-defined parameter.\n\n    Parameters\n    ----------\n    min_examples : int\n        The number of samples the microcluster needs to have to be considered representative.\n\n    Returns\n    -------\n    bool\n        If the cluster is representative or not\n    \"\"\"\n    return self.n &gt;= min_examples\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.MicroCluster.small_str","title":"<code>small_str()</code>","text":"<p>Returns string representation of a microcluster.</p> <p>Returns:</p> Type Description <code>str</code> <p>Small string representation of microcluster</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def small_str(self):\n    \"\"\"Returns string representation of a microcluster.\n\n    Returns\n    -------\n    str\n        Small string representation of microcluster\n    \"\"\"\n\n    return f\"\"\"Target class {self.label}\n            # of instances: {self.n}\n            Timestamp of last change: {self.timestamp}\"\"\"\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.MicroCluster.update_cluster","title":"<code>update_cluster(X, timestamp, update_summary)</code>","text":"<p>Adds point received in parameter to the cluster and update cluster's centroid if wanted.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>One point</p> required <code>timestamp</code> <code>int</code> <p>Timestamp when this point was added to this microcluster</p> required <code>update_summary</code> <code>bool</code> <p>Whether or not to update the microcluster properties with this new point</p> required Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def update_cluster(self, X, timestamp, update_summary):\n    \"\"\"Adds point received in parameter to the cluster and update cluster's centroid if wanted.\n\n    Parameters\n    ----------\n    X : numpy.ndarray\n        One point\n    timestamp : int\n        Timestamp when this point was added to this microcluster\n    update_summary : bool\n        Whether or not to update the microcluster properties with this new point\n    \"\"\"\n\n    assert len(X.shape) == 1  # it's just one point\n    self.timestamp = timestamp\n\n    if self.instances is not None:\n        self.instances.append(X)\n\n    if update_summary:\n        self.mean_distance = (self.n * self.mean_distance + self.distance_to_centroid(X)) / (self.n + 1)\n        self.n += 1\n        self.linear_sum = np.sum([self.linear_sum, X], axis=0)\n        self.squared_sum = np.sum([self.squared_sum, np.square(X).sum()], axis=0)\n        self.update_properties()\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.MicroCluster.update_properties","title":"<code>update_properties()</code>","text":"<p>Updates centroid and radius based on current cluster properties.</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def update_properties(self):\n    \"\"\"Updates centroid and radius based on current cluster properties.\"\"\"\n    self.centroid = self.linear_sum / self.n\n\n    if self.instances is not None:\n        self.radius = self.get_radius()\n        if np.max(self.distance_to_centroid(self.instances)) &gt; self.max_distance:\n            self.max_distance = np.max(self.distance_to_centroid(self.instances))\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.ShortMem","title":"<code>ShortMem</code>","text":"<p>Data structure for efficient addition and search of ShortMemInstances.</p> <p>Attributes:</p> Name Type Description <code>list</code> <code>list of tuples (hash, [ShortMemInstance1, ShortMemInstance2, ...])</code> <p>List containing the instances and their corresponding hash compiled from their point</p> <code>dictionary</code> <code>dictionary</code> <p>Contains for each hash its index in the list</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>class ShortMem:\n    \"\"\"Data structure for efficient addition and search of ShortMemInstances.\n\n    Attributes\n    ----------\n    list : list of tuples (hash, [ShortMemInstance1, ShortMemInstance2, ...])\n        List containing the instances and their corresponding hash compiled from their point\n    dictionary : dictionary\n        Contains for each hash its index in the list\n    \"\"\"\n    def __init__(self):\n        self.list = []\n        self.dictionary = {}\n\n    def append(self, instance):\n        \"\"\"Adds an element to the data structure\n\n        Parameters\n        ----------\n        instance : ShortMemInstance\n            Element to add\n        \"\"\"\n        h = hashlib.sha256(instance.point.tobytes()).hexdigest()\n\n        # Check if the hash already exists\n        if h in self.dictionary:\n            # Add the instance to the existing list of instances for that hash\n            self.list[self.dictionary[h]][1].append(instance)\n        else:\n            index = len(self.list)\n            self.list.append((h, [instance]))\n            self.dictionary[h] = index\n\n    def remove(self, index):\n        \"\"\"Remove the element at the given index from the data structure.\n\n        Parameters\n        ----------\n        index : int\n            Index of the element to remove\n        \"\"\"\n        if 0 &lt;= index &lt; len(self.list):\n            instance_list = self.list[index][1]\n\n            # If there's only one instance for the hash, remove the entire entry\n            if len(instance_list) == 1:\n                del self.dictionary[self.list[index][0]]\n                self.list.pop(index)\n\n                # Update indices for remaining entries\n                for i in range(index, len(self.list)):\n                    self.dictionary[self.list[i][0]] = i\n            else:\n                # If there are multiple instances, remove just one instance\n                instance_list.pop(-1)\n\n    def index(self, instance):\n        \"\"\"Get the index of the given element.\n\n        Parameters\n        ----------\n        instance : np.ndarray or ShortMemInstance\n            Element to find\n\n        Returns\n        -------\n        int\n            Index of the element, -1 if not found\n        \"\"\"\n        if type(instance) == np.ndarray:\n            return self.dictionary.get(hashlib.sha256(instance.tobytes()).hexdigest(), -1)\n        elif type(instance) == ShortMemInstance:\n            return self.dictionary.get(hashlib.sha256(instance.point.tobytes()).hexdigest(), -1)\n\n    def get_all_instances(self):\n        \"\"\"Returns all ShortMemInstances instances within the data structure\n\n        Returns\n        -------\n        list of ShortMemInstance\n            All ShortMemInstances instances within the data structure\n        \"\"\"\n        return [instance for _, instances_list in self.list for instance in instances_list]\n\n    def get_instance(self, index):\n        \"\"\"Return specific ShortMemInstance at given index\n\n        Parameters\n        ----------\n        index : int\n            Index\n\n        Returns\n        -------\n        ShortMemInstance\n            The instance at the given index, None if index not found\n        \"\"\"\n        if 0 &lt;= index &lt; len(self.list):\n            return self.list[index][1][0]\n\n    def get_all_points(self):\n        \"\"\"Returns all points within the data structure\n\n        Returns\n        -------\n        np.ndarray\n            All points contained in the data structure\n        \"\"\"\n        return np.array([instance.point for _, instances_list in self.list for instance in instances_list])\n\n    def __len__(self):\n        return len(self.list)\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.ShortMem.append","title":"<code>append(instance)</code>","text":"<p>Adds an element to the data structure</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>ShortMemInstance</code> <p>Element to add</p> required Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def append(self, instance):\n    \"\"\"Adds an element to the data structure\n\n    Parameters\n    ----------\n    instance : ShortMemInstance\n        Element to add\n    \"\"\"\n    h = hashlib.sha256(instance.point.tobytes()).hexdigest()\n\n    # Check if the hash already exists\n    if h in self.dictionary:\n        # Add the instance to the existing list of instances for that hash\n        self.list[self.dictionary[h]][1].append(instance)\n    else:\n        index = len(self.list)\n        self.list.append((h, [instance]))\n        self.dictionary[h] = index\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.ShortMem.get_all_instances","title":"<code>get_all_instances()</code>","text":"<p>Returns all ShortMemInstances instances within the data structure</p> <p>Returns:</p> Type Description <code>list of ShortMemInstance</code> <p>All ShortMemInstances instances within the data structure</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def get_all_instances(self):\n    \"\"\"Returns all ShortMemInstances instances within the data structure\n\n    Returns\n    -------\n    list of ShortMemInstance\n        All ShortMemInstances instances within the data structure\n    \"\"\"\n    return [instance for _, instances_list in self.list for instance in instances_list]\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.ShortMem.get_all_points","title":"<code>get_all_points()</code>","text":"<p>Returns all points within the data structure</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>All points contained in the data structure</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def get_all_points(self):\n    \"\"\"Returns all points within the data structure\n\n    Returns\n    -------\n    np.ndarray\n        All points contained in the data structure\n    \"\"\"\n    return np.array([instance.point for _, instances_list in self.list for instance in instances_list])\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.ShortMem.get_instance","title":"<code>get_instance(index)</code>","text":"<p>Return specific ShortMemInstance at given index</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index</p> required <p>Returns:</p> Type Description <code>ShortMemInstance</code> <p>The instance at the given index, None if index not found</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def get_instance(self, index):\n    \"\"\"Return specific ShortMemInstance at given index\n\n    Parameters\n    ----------\n    index : int\n        Index\n\n    Returns\n    -------\n    ShortMemInstance\n        The instance at the given index, None if index not found\n    \"\"\"\n    if 0 &lt;= index &lt; len(self.list):\n        return self.list[index][1][0]\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.ShortMem.index","title":"<code>index(instance)</code>","text":"<p>Get the index of the given element.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>ndarray or ShortMemInstance</code> <p>Element to find</p> required <p>Returns:</p> Type Description <code>int</code> <p>Index of the element, -1 if not found</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def index(self, instance):\n    \"\"\"Get the index of the given element.\n\n    Parameters\n    ----------\n    instance : np.ndarray or ShortMemInstance\n        Element to find\n\n    Returns\n    -------\n    int\n        Index of the element, -1 if not found\n    \"\"\"\n    if type(instance) == np.ndarray:\n        return self.dictionary.get(hashlib.sha256(instance.tobytes()).hexdigest(), -1)\n    elif type(instance) == ShortMemInstance:\n        return self.dictionary.get(hashlib.sha256(instance.point.tobytes()).hexdigest(), -1)\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.ShortMem.remove","title":"<code>remove(index)</code>","text":"<p>Remove the element at the given index from the data structure.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of the element to remove</p> required Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def remove(self, index):\n    \"\"\"Remove the element at the given index from the data structure.\n\n    Parameters\n    ----------\n    index : int\n        Index of the element to remove\n    \"\"\"\n    if 0 &lt;= index &lt; len(self.list):\n        instance_list = self.list[index][1]\n\n        # If there's only one instance for the hash, remove the entire entry\n        if len(instance_list) == 1:\n            del self.dictionary[self.list[index][0]]\n            self.list.pop(index)\n\n            # Update indices for remaining entries\n            for i in range(index, len(self.list)):\n                self.dictionary[self.list[i][0]] = i\n        else:\n            # If there are multiple instances, remove just one instance\n            instance_list.pop(-1)\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.ShortMemInstance","title":"<code>ShortMemInstance</code>","text":"<p>Instance of a point associated with a timestamp. Used for the buffer memory which stores the unknown samples.</p> <p>Attributes:</p> Name Type Description <code>point</code> <code>ndarray</code> <p>The coordinates of the point</p> <code>timestamp</code> <code>int</code> <p>The timestamp the point was added/treated</p> <code>y_true</code> <code>int</code> <p>The true value of the class</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>class ShortMemInstance:\n    \"\"\"Instance of a point associated with a timestamp. Used for the buffer memory which stores the unknown samples.\n\n    Attributes\n    ----------\n    point : numpy.ndarray\n        The coordinates of the point\n    timestamp : int\n        The timestamp the point was added/treated\n    y_true : int\n        The true value of the class\n    \"\"\"\n    def __init__(self, point, timestamp, y_true=None):\n        self.point = point\n        self.timestamp = timestamp\n        self.y_true = y_true\n\n    def __eq__(self, other):\n        \"\"\"Elements are equal if they have the same values for all variables.\n        This currently does not consider the timestamp.\n\n        Parameters\n        ----------\n        other : ShortMemInstance\n            Other instance to compared to\n\n        Returns\n        -------\n        bool\n            If the instances are equals or not\n        \"\"\"\n        if type(other) == np.ndarray:\n            return np.all(self.point == other)\n</code></pre>"},{"location":"reference/utils/data_structure/#streamndr.utils.data_structure.ShortMemInstance.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Elements are equal if they have the same values for all variables. This currently does not consider the timestamp.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>ShortMemInstance</code> <p>Other instance to compared to</p> required <p>Returns:</p> Type Description <code>bool</code> <p>If the instances are equals or not</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def __eq__(self, other):\n    \"\"\"Elements are equal if they have the same values for all variables.\n    This currently does not consider the timestamp.\n\n    Parameters\n    ----------\n    other : ShortMemInstance\n        Other instance to compared to\n\n    Returns\n    -------\n    bool\n        If the instances are equals or not\n    \"\"\"\n    if type(other) == np.ndarray:\n        return np.all(self.point == other)\n</code></pre>"},{"location":"reference/utils/mcikmeans/","title":"mcikmeans","text":""},{"location":"reference/utils/mcikmeans/#streamndr.utils.mcikmeans.MCIKMeans","title":"<code>MCIKMeans</code>","text":"<p>Implementation of K-Means with Minimization of Cluster Impurity (MCI-Kmeans), as described in [1].</p> <p>This algorithm implements a semi-supervised version of K-Means, that aims to minimize the intra-cluster dispersion while also minimizing the impurity of each cluster.</p> <p>[1] Masud, Mohammad M., et al. \"A practical approach to classify evolving data streams: Training with limited amount of labeled data.\"  2008 Eighth IEEE International Conference on Data Mining. IEEE, 2008.</p> <p>Parameters:</p> Name Type Description Default <code>n_clusters</code> <code>int</code> <p>Number of clusters to generate</p> <code>8</code> <code>max_iter</code> <code>int</code> <p>Maximum number of iterations of the M-Step</p> <code>300</code> <code>conditional_mode_max_iter</code> <code>int</code> <p>Maximum number of iterations of the E-Step</p> <code>300</code> <code>random_state</code> <code>int</code> <p>Seed for the random number generation. Makes the algorithm deterministic if a number is provided.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>clusters</code> <code>dict</code> <p>Dictionary containing each cluster with their label as key</p> <code>cluster_centers_</code> <code>ndarray</code> <p>Array containing the coordinates of the cluster centers</p> <code>labels_</code> <code>ndarray</code> <p>Labels of each point</p> Source code in <code>streamndr/utils/mcikmeans.py</code> <pre><code>class MCIKMeans():\n    \"\"\"Implementation of K-Means with Minimization of Cluster Impurity (MCI-Kmeans), as described in [1].\n\n    This algorithm implements a semi-supervised version of K-Means, that aims to minimize the intra-cluster dispersion while also minimizing the impurity of each cluster.\n\n    [1] Masud, Mohammad M., et al. \"A practical approach to classify evolving data streams: Training with limited amount of labeled data.\" \n    2008 Eighth IEEE International Conference on Data Mining. IEEE, 2008.\n\n    Parameters\n    ----------\n    n_clusters : int\n        Number of clusters to generate\n    max_iter : int\n        Maximum number of iterations of the M-Step\n    conditional_mode_max_iter : int\n        Maximum number of iterations of the E-Step\n    random_state : int\n        Seed for the random number generation. Makes the algorithm deterministic if a number is provided.\n\n    Attributes\n    ----------\n    clusters : dict\n        Dictionary containing each cluster with their label as key\n    cluster_centers_ : numpy.ndarray\n        Array containing the coordinates of the cluster centers\n    labels_ : numpy.ndarray\n        Labels of each point\n    \"\"\"\n    def __init__(self,\n                 n_clusters=8,\n                 max_iter=300,\n                 conditional_mode_max_iter=300,\n                 random_state=None):\n\n        self.n_clusters = n_clusters\n        self.max_iter = max_iter\n        self.conditional_mode_max_iter = conditional_mode_max_iter\n        self.random_state = random_state\n\n        if random_state != None:\n            random.seed(random_state)\n\n        self.clusters = []\n\n    def fit(self, X, y):\n        \"\"\"Compute MCI-Kmeans clustering.\n\n        Parameters\n        ----------\n        X : numpy.ndarray\n            Samples\n        y : list of int\n            Labels of the samples, expects -1 if the label is not known\n\n        Returns\n        -------\n        MCIKmeans\n            Fitted estimator\n        \"\"\"\n        y = np.array(y)\n\n        samples_per_class = {}\n        number_of_centroids = {}\n        unlabeled_samples = []\n\n        nb_labeled_samples = len(X[y!=-1])\n        remaining = 0\n        for label in np.unique(y):\n            if label == -1:\n                unlabeled_samples = X[y==-1]\n                continue\n\n            samples_per_class[label] = X[y==label]\n\n            weight = self.n_clusters * len(samples_per_class[label]) / nb_labeled_samples\n            remaining += weight - round(weight)\n            number_of_centroids[label] = round(weight)\n\n        while(remaining &gt; 0):\n            #Find the label with the smallest number of centroids and add the remainder to it\n            key_with_min_value = min(number_of_centroids, key=lambda i: number_of_centroids[i])\n            number_of_centroids[key_with_min_value] += 1\n\n            remaining -= 1\n\n        centroids = []\n        for label in samples_per_class:\n            centroids.extend(self._init_centroids(samples_per_class[label], number_of_centroids[label]))\n\n            if (len(centroids) &lt; number_of_centroids[label]) and (len(unlabeled_samples) &gt; 0):\n                filling_samples = copy.deepcopy(unlabeled_samples)\n\n                while ((len(centroids) &lt; number_of_centroids[label]) and (len(filling_samples) &gt; 0)):\n                    choice = filling_samples.pop(random.randrange(len(filling_samples)))\n                    centroids.append(choice)\n\n        for i in range(len(centroids)):\n            self.clusters.append(ImpurityBasedCluster(i, centroids[i]))\n\n        iterations = 0\n        changing = True\n\n        while changing and iterations &lt; self.max_iter:\n            changing = self._iterative_conditional_mode(samples_per_class, unlabeled_samples)\n\n            for cluster in self.clusters:\n                if cluster.n &gt; 0:\n                    cluster.update_properties()\n\n            iterations += 1\n\n        self.cluster_centers_ = np.array([cluster.centroid for cluster in self.clusters])\n        self.labels_ = self.predict(X)\n\n        return self\n\n\n    def predict(self, X):\n        \"\"\"Predict the closest cluster each sample in X belongs to.\n\n        Parameters\n        ----------\n        X : numpy.ndarray\n            Samples to predict\n\n        Returns\n        -------\n        numpy.ndarray\n            Index of the cluster each sample belongs to\n        \"\"\"\n        labels, _ = get_closest_clusters(X, [cluster.centroid for cluster in self.clusters])\n\n        return labels\n\n    def fit_predict(self, X, y):\n        \"\"\"Compute cluster centers and predict cluster index for each sample. Convenience method; equivalent to calling fit(X) followed by predict(X).\n\n        Parameters\n        ----------\n        X : numpy.ndarray\n            Samples\n        y : list of int\n            Labels of the samples, expects -1 if the label is not known\n\n        Returns\n        -------\n        numpy.ndarray\n            Index of the cluster each sample belongs to\n        \"\"\"\n        return self.fit(X, y).labels_\n\n    def _init_centroids(self, samples, numbers_of_centroids):\n        centroids = []\n\n        if len(samples) &lt;= numbers_of_centroids:\n            centroids.extend(samples)\n            return centroids\n\n        candidates = copy.deepcopy(samples).tolist()\n\n        for i in range(numbers_of_centroids):\n            selected = candidates.pop(random.randrange(len(candidates)))\n\n            centroids.append(selected)\n\n        return centroids\n\n    def _iterative_conditional_mode(self, samples_per_class, unlabeled_samples):\n\n        _labeled_samples = []\n        for key, value in samples_per_class.items():\n            _labeled_samples.extend([ShortMemInstance(x, None, key) for x in value])\n\n        _unlabeled_samples = [ShortMemInstance(x, None, -1) for x in unlabeled_samples]\n\n        iterations = 0\n        changed = True\n        no_change = True\n\n        while iterations &lt; self.conditional_mode_max_iter and changed:\n            total_nb_samples = len(_labeled_samples) + len(_unlabeled_samples)\n\n            iterations += 1\n            changed = False\n\n            for i in range(total_nb_samples):\n                sample = None\n\n                if (len(_labeled_samples) &gt; 0) and ((len(unlabeled_samples) == 0) or bool(random.getrandbits(1))):\n                    sample = _labeled_samples.pop(random.randrange(len(_labeled_samples)))\n\n                else:\n                    sample = _unlabeled_samples.pop(random.randrange(len(_unlabeled_samples)))\n\n                previous_cluster_id = sample.timestamp\n\n                if previous_cluster_id is not None:\n                    self.clusters[previous_cluster_id].remove_sample(sample)\n                    sample.timestamp = None\n\n                distances = np.linalg.norm([cluster.centroid for cluster in self.clusters] - sample.point, axis=1)\n\n                if sample.y_true != -1:\n                    entropies = np.array([cluster.entropy for cluster in self.clusters])\n                    dissimilarities = np.array([cluster.dissimilarity_count(sample) for cluster in self.clusters])\n                    distances = distances * (1 + entropies * dissimilarities)\n\n                chosen_cluster = np.argmin(distances)\n\n\n                self.clusters[chosen_cluster].add_sample(sample)\n                sample.timestamp = chosen_cluster\n\n                self.clusters[chosen_cluster].update_entropy()\n\n                if self.clusters[chosen_cluster].label != previous_cluster_id:\n                    changed = True\n                    no_change = False\n\n        return not no_change\n</code></pre>"},{"location":"reference/utils/mcikmeans/#streamndr.utils.mcikmeans.MCIKMeans.fit","title":"<code>fit(X, y)</code>","text":"<p>Compute MCI-Kmeans clustering.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Samples</p> required <code>y</code> <code>list of int</code> <p>Labels of the samples, expects -1 if the label is not known</p> required <p>Returns:</p> Type Description <code>MCIKmeans</code> <p>Fitted estimator</p> Source code in <code>streamndr/utils/mcikmeans.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Compute MCI-Kmeans clustering.\n\n    Parameters\n    ----------\n    X : numpy.ndarray\n        Samples\n    y : list of int\n        Labels of the samples, expects -1 if the label is not known\n\n    Returns\n    -------\n    MCIKmeans\n        Fitted estimator\n    \"\"\"\n    y = np.array(y)\n\n    samples_per_class = {}\n    number_of_centroids = {}\n    unlabeled_samples = []\n\n    nb_labeled_samples = len(X[y!=-1])\n    remaining = 0\n    for label in np.unique(y):\n        if label == -1:\n            unlabeled_samples = X[y==-1]\n            continue\n\n        samples_per_class[label] = X[y==label]\n\n        weight = self.n_clusters * len(samples_per_class[label]) / nb_labeled_samples\n        remaining += weight - round(weight)\n        number_of_centroids[label] = round(weight)\n\n    while(remaining &gt; 0):\n        #Find the label with the smallest number of centroids and add the remainder to it\n        key_with_min_value = min(number_of_centroids, key=lambda i: number_of_centroids[i])\n        number_of_centroids[key_with_min_value] += 1\n\n        remaining -= 1\n\n    centroids = []\n    for label in samples_per_class:\n        centroids.extend(self._init_centroids(samples_per_class[label], number_of_centroids[label]))\n\n        if (len(centroids) &lt; number_of_centroids[label]) and (len(unlabeled_samples) &gt; 0):\n            filling_samples = copy.deepcopy(unlabeled_samples)\n\n            while ((len(centroids) &lt; number_of_centroids[label]) and (len(filling_samples) &gt; 0)):\n                choice = filling_samples.pop(random.randrange(len(filling_samples)))\n                centroids.append(choice)\n\n    for i in range(len(centroids)):\n        self.clusters.append(ImpurityBasedCluster(i, centroids[i]))\n\n    iterations = 0\n    changing = True\n\n    while changing and iterations &lt; self.max_iter:\n        changing = self._iterative_conditional_mode(samples_per_class, unlabeled_samples)\n\n        for cluster in self.clusters:\n            if cluster.n &gt; 0:\n                cluster.update_properties()\n\n        iterations += 1\n\n    self.cluster_centers_ = np.array([cluster.centroid for cluster in self.clusters])\n    self.labels_ = self.predict(X)\n\n    return self\n</code></pre>"},{"location":"reference/utils/mcikmeans/#streamndr.utils.mcikmeans.MCIKMeans.fit_predict","title":"<code>fit_predict(X, y)</code>","text":"<p>Compute cluster centers and predict cluster index for each sample. Convenience method; equivalent to calling fit(X) followed by predict(X).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Samples</p> required <code>y</code> <code>list of int</code> <p>Labels of the samples, expects -1 if the label is not known</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Index of the cluster each sample belongs to</p> Source code in <code>streamndr/utils/mcikmeans.py</code> <pre><code>def fit_predict(self, X, y):\n    \"\"\"Compute cluster centers and predict cluster index for each sample. Convenience method; equivalent to calling fit(X) followed by predict(X).\n\n    Parameters\n    ----------\n    X : numpy.ndarray\n        Samples\n    y : list of int\n        Labels of the samples, expects -1 if the label is not known\n\n    Returns\n    -------\n    numpy.ndarray\n        Index of the cluster each sample belongs to\n    \"\"\"\n    return self.fit(X, y).labels_\n</code></pre>"},{"location":"reference/utils/mcikmeans/#streamndr.utils.mcikmeans.MCIKMeans.predict","title":"<code>predict(X)</code>","text":"<p>Predict the closest cluster each sample in X belongs to.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Samples to predict</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Index of the cluster each sample belongs to</p> Source code in <code>streamndr/utils/mcikmeans.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict the closest cluster each sample in X belongs to.\n\n    Parameters\n    ----------\n    X : numpy.ndarray\n        Samples to predict\n\n    Returns\n    -------\n    numpy.ndarray\n        Index of the cluster each sample belongs to\n    \"\"\"\n    labels, _ = get_closest_clusters(X, [cluster.centroid for cluster in self.clusters])\n\n    return labels\n</code></pre>"},{"location":"reference/utils/mcikmeans/#streamndr.utils.mcikmeans.MicroCluster","title":"<code>MicroCluster</code>","text":"<p>             Bases: <code>object</code></p> <p>A representation of a cluster with compressed information.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>int</code> <p>Label associated with this microcluster</p> required <code>instances</code> <code>ndarray</code> <p>Instances in this microcluster, preferably these would not be stored if not needed using keep_instances=False. Will be converted to Python list for append performance.</p> <code>None</code> <code>timestamp</code> <code>int</code> <p>Timestamp this microcluster was last updated, used for forgetting mechanisms</p> <code>0</code> <code>keep_instances</code> <code>bool</code> <p>Whether or not to store the instances within the microcluster. Should preferably set to false, but some implementations require access to the instances</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>n</code> <code>int</code> <p>Number of instances stored in this microcluster</p> <code>linear_sum</code> <code>ndarray</code> <p>Linear sum of the points belonging to this microcluster</p> <code>squared_sum</code> <code>ndarray</code> <p>Sum of the squared l2 norms of all samples belonging to this microcluster</p> <code>centroid</code> <code>ndarray</code> <p>Centroid coordinates of the microcluster</p> <code>max_distance</code> <code>ndarray</code> <p>Maximum distance between a point belonging to the microcluster and its centroid</p> <code>mean_distance</code> <code>ndarray</code> <p>Mean distance of the distances between the cluster's points and its centroid</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>class MicroCluster(object):\n    \"\"\"A representation of a cluster with compressed information.\n\n    Parameters\n    ----------\n    label : int\n        Label associated with this microcluster\n    instances : numpy.ndarray\n        Instances in this microcluster, preferably these would not be stored if not needed using keep_instances=False. Will be converted to Python list for append performance.\n    timestamp : int\n        Timestamp this microcluster was last updated, used for forgetting mechanisms  \n    keep_instances : bool\n        Whether or not to store the instances within the microcluster. Should preferably set to false, but some implementations require\n        access to the instances\n\n    Attributes\n    ----------\n    n : int\n        Number of instances stored in this microcluster\n    linear_sum : numpy.ndarray\n        Linear sum of the points belonging to this microcluster\n    squared_sum : numpy.ndarray\n        Sum of the squared l2 norms of all samples belonging to this microcluster\n    centroid : numpy.ndarray\n        Centroid coordinates of the microcluster\n    max_distance : numpy.ndarray\n        Maximum distance between a point belonging to the microcluster and its centroid\n    mean_distance : numpy.ndarray\n        Mean distance of the distances between the cluster's points and its centroid\n    \"\"\"\n\n    def __init__(self,\n                 label,  # the class the microcluster belongs to\n                 instances=None,\n                 timestamp=0, \n                 keep_instances=True #Required True for MINAS\n                 ):\n\n        # TODO: remove instances entirely so it doesn't need to be stored in memory; Might not be possible because of _best_threshold used by MINAS which needs instances\n        super(MicroCluster, self).__init__()\n        self.label = label\n\n        if instances is not None:\n            self.instances = instances.tolist()\n            self.n = len(instances)\n            self.linear_sum = instances.sum(axis=0)\n\n            # Sum of the squared l2 norms of all samples belonging to a microcluster:\n            self.squared_sum = np.square(np.linalg.norm(self.instances, axis=1)).sum()\n            # self.squared_sum = np.square(instances).sum(axis=0)  # From CluSTREAM paper\n            self.centroid = self.linear_sum / self.n\n            self.max_distance = np.max(self.distance_to_centroid(instances))\n            self.mean_distance = np.mean(self.distance_to_centroid(instances))\n            self.update_properties()\n\n        else:\n            self.instances = None\n            self.n = 0\n            self.linear_sum = 0\n            self.squared_sum = 0\n            self.max_distance = 0\n            self.mean_distance = 0\n\n        self.timestamp = timestamp\n\n\n        if not keep_instances:\n            self.instances = None\n\n    def __str__(self):\n        \"\"\"Returns string representation of a microcluster.\n\n        Returns\n        -------\n        str\n            String representation of microcluster\n        \"\"\"\n\n        return f\"\"\"Target class {self.label}\n                # of instances: {self.n}\n                Linear sum: {self.linear_sum}\n                Squared sum: {self.squared_sum}\n                Centroid: {self.centroid}\n                Radius: {self.radius}\n                Timestamp of last change: {self.timestamp}\"\"\"\n\n    def small_str(self):\n        \"\"\"Returns string representation of a microcluster.\n\n        Returns\n        -------\n        str\n            Small string representation of microcluster\n        \"\"\"\n\n        return f\"\"\"Target class {self.label}\n                # of instances: {self.n}\n                Timestamp of last change: {self.timestamp}\"\"\"\n\n    def get_radius(self):\n        \"\"\"Returns radius of the microcluster.\n\n        Returns\n        -------\n        float\n            Radius of the microcluster\n        \"\"\"\n\n        factor = 1.5\n        # from BIRCH Wikipedia\n        diff = (self.squared_sum / self.n) - np.dot(self.centroid, self.centroid)\n        if diff &gt; 1e-15:\n            return factor * np.sqrt(diff)\n        else:  # in this case diff should be zero, but sometimes it's an infinitesimal difference\n            return 0\n        # from MINAS paper:\n        #return factor*np.std(self.distance_to_centroid(self.instances))\n\n    def distance_to_centroid(self, X):\n        \"\"\"Returns distance from X to centroid of this cluster.\n\n        Parameters\n        ----------\n        X : numpy.ndarray or list\n            Point or multiple points\n\n        Returns\n        -------\n        numpy.ndarray\n            Distance from X to the microcluster's centroid\n        \"\"\"\n        if not isinstance(X, np.ndarray):\n            X = np.array(X)\n\n        if len(X.shape) == 1:  # X is only one point\n            return np.linalg.norm(X - self.centroid)\n        else:  # X contains several points\n            return np.linalg.norm(X - self.centroid, axis=1)\n\n    def encompasses(self, X):\n        \"\"\"Checks if point X is inside this microcluster. The point X is considered within the microcluster if the distance \n        between the point and the microcluster's centroid is less than the radius of the microcluster.\n\n        Parameters\n        ----------\n        X : numpy.ndarray\n            One point\n\n        Returns\n        -------\n        bool\n            If the point distance to centroid is contained within the microcluster or not\n        \"\"\"\n\n        return np.less(self.distance_to_centroid(X), self.radius)\n\n    def find_closest_cluster(self, clusters):\n        \"\"\"Finds closest microcluster to this one among passed microclusters.\n\n        Parameters\n        ----------\n        clusters : list of MicroCluster\n\n        Returns\n        -------\n        MicroCluster\n            Closest microcluster\n        \"\"\"\n\n        return min(clusters, key=lambda cl: cl.distance_to_centroid(self.centroid))\n\n    def update_cluster(self, X, timestamp, update_summary):\n        \"\"\"Adds point received in parameter to the cluster and update cluster's centroid if wanted.\n\n        Parameters\n        ----------\n        X : numpy.ndarray\n            One point\n        timestamp : int\n            Timestamp when this point was added to this microcluster\n        update_summary : bool\n            Whether or not to update the microcluster properties with this new point\n        \"\"\"\n\n        assert len(X.shape) == 1  # it's just one point\n        self.timestamp = timestamp\n\n        if self.instances is not None:\n            self.instances.append(X)\n\n        if update_summary:\n            self.mean_distance = (self.n * self.mean_distance + self.distance_to_centroid(X)) / (self.n + 1)\n            self.n += 1\n            self.linear_sum = np.sum([self.linear_sum, X], axis=0)\n            self.squared_sum = np.sum([self.squared_sum, np.square(X).sum()], axis=0)\n            self.update_properties()\n\n    def update_properties(self):\n        \"\"\"Updates centroid and radius based on current cluster properties.\"\"\"\n        self.centroid = self.linear_sum / self.n\n\n        if self.instances is not None:\n            self.radius = self.get_radius()\n            if np.max(self.distance_to_centroid(self.instances)) &gt; self.max_distance:\n                self.max_distance = np.max(self.distance_to_centroid(self.instances))\n\n    def is_cohesive(self, clusters):\n        \"\"\"Verifies if this cluster is cohesive for novelty detection purposes.\n        A new micro-cluster is cohesive if its silhouette coefficient is larger than 0.\n        'b' represents the Euclidean distance between the centroid of the new micro-cluster and the centroid of its\n        closest micro-cluster, and 'a' represents the standard deviation of the distances between the examples of the\n        new micro-cluster and the centroid of the new micro-cluster.\n\n        Parameters\n        ----------\n        clusters : List of MicroCluster\n            Existing known micro-clusters\n\n        Returns\n        -------\n        bool\n            If the cluster is cohesive (silhouette&gt;0) or not\n        \"\"\"\n        b = self.distance_to_centroid(self.find_closest_cluster(clusters).centroid)\n        a = np.std(self.distance_to_centroid(self.instances))\n        silhouette = (b - a) / max(a, b)  # hm, this is always positive if b &gt; a\n        return silhouette &gt; 0\n\n    def is_representative(self, min_examples):\n        \"\"\"Verifies if this cluster is representative for novelty detection purposes.\n        A new micro-cluster is representative if it contains a minimal number of examples,\n        where this number is a user-defined parameter.\n\n        Parameters\n        ----------\n        min_examples : int\n            The number of samples the microcluster needs to have to be considered representative.\n\n        Returns\n        -------\n        bool\n            If the cluster is representative or not\n        \"\"\"\n        return self.n &gt;= min_examples\n</code></pre>"},{"location":"reference/utils/mcikmeans/#streamndr.utils.mcikmeans.MicroCluster.__str__","title":"<code>__str__()</code>","text":"<p>Returns string representation of a microcluster.</p> <p>Returns:</p> Type Description <code>str</code> <p>String representation of microcluster</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def __str__(self):\n    \"\"\"Returns string representation of a microcluster.\n\n    Returns\n    -------\n    str\n        String representation of microcluster\n    \"\"\"\n\n    return f\"\"\"Target class {self.label}\n            # of instances: {self.n}\n            Linear sum: {self.linear_sum}\n            Squared sum: {self.squared_sum}\n            Centroid: {self.centroid}\n            Radius: {self.radius}\n            Timestamp of last change: {self.timestamp}\"\"\"\n</code></pre>"},{"location":"reference/utils/mcikmeans/#streamndr.utils.mcikmeans.MicroCluster.distance_to_centroid","title":"<code>distance_to_centroid(X)</code>","text":"<p>Returns distance from X to centroid of this cluster.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray or list</code> <p>Point or multiple points</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Distance from X to the microcluster's centroid</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def distance_to_centroid(self, X):\n    \"\"\"Returns distance from X to centroid of this cluster.\n\n    Parameters\n    ----------\n    X : numpy.ndarray or list\n        Point or multiple points\n\n    Returns\n    -------\n    numpy.ndarray\n        Distance from X to the microcluster's centroid\n    \"\"\"\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n\n    if len(X.shape) == 1:  # X is only one point\n        return np.linalg.norm(X - self.centroid)\n    else:  # X contains several points\n        return np.linalg.norm(X - self.centroid, axis=1)\n</code></pre>"},{"location":"reference/utils/mcikmeans/#streamndr.utils.mcikmeans.MicroCluster.encompasses","title":"<code>encompasses(X)</code>","text":"<p>Checks if point X is inside this microcluster. The point X is considered within the microcluster if the distance  between the point and the microcluster's centroid is less than the radius of the microcluster.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>One point</p> required <p>Returns:</p> Type Description <code>bool</code> <p>If the point distance to centroid is contained within the microcluster or not</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def encompasses(self, X):\n    \"\"\"Checks if point X is inside this microcluster. The point X is considered within the microcluster if the distance \n    between the point and the microcluster's centroid is less than the radius of the microcluster.\n\n    Parameters\n    ----------\n    X : numpy.ndarray\n        One point\n\n    Returns\n    -------\n    bool\n        If the point distance to centroid is contained within the microcluster or not\n    \"\"\"\n\n    return np.less(self.distance_to_centroid(X), self.radius)\n</code></pre>"},{"location":"reference/utils/mcikmeans/#streamndr.utils.mcikmeans.MicroCluster.find_closest_cluster","title":"<code>find_closest_cluster(clusters)</code>","text":"<p>Finds closest microcluster to this one among passed microclusters.</p> <p>Parameters:</p> Name Type Description Default <code>clusters</code> <code>list of MicroCluster</code> required <p>Returns:</p> Type Description <code>MicroCluster</code> <p>Closest microcluster</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def find_closest_cluster(self, clusters):\n    \"\"\"Finds closest microcluster to this one among passed microclusters.\n\n    Parameters\n    ----------\n    clusters : list of MicroCluster\n\n    Returns\n    -------\n    MicroCluster\n        Closest microcluster\n    \"\"\"\n\n    return min(clusters, key=lambda cl: cl.distance_to_centroid(self.centroid))\n</code></pre>"},{"location":"reference/utils/mcikmeans/#streamndr.utils.mcikmeans.MicroCluster.get_radius","title":"<code>get_radius()</code>","text":"<p>Returns radius of the microcluster.</p> <p>Returns:</p> Type Description <code>float</code> <p>Radius of the microcluster</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def get_radius(self):\n    \"\"\"Returns radius of the microcluster.\n\n    Returns\n    -------\n    float\n        Radius of the microcluster\n    \"\"\"\n\n    factor = 1.5\n    # from BIRCH Wikipedia\n    diff = (self.squared_sum / self.n) - np.dot(self.centroid, self.centroid)\n    if diff &gt; 1e-15:\n        return factor * np.sqrt(diff)\n    else:  # in this case diff should be zero, but sometimes it's an infinitesimal difference\n        return 0\n</code></pre>"},{"location":"reference/utils/mcikmeans/#streamndr.utils.mcikmeans.MicroCluster.is_cohesive","title":"<code>is_cohesive(clusters)</code>","text":"<p>Verifies if this cluster is cohesive for novelty detection purposes. A new micro-cluster is cohesive if its silhouette coefficient is larger than 0. 'b' represents the Euclidean distance between the centroid of the new micro-cluster and the centroid of its closest micro-cluster, and 'a' represents the standard deviation of the distances between the examples of the new micro-cluster and the centroid of the new micro-cluster.</p> <p>Parameters:</p> Name Type Description Default <code>clusters</code> <code>List of MicroCluster</code> <p>Existing known micro-clusters</p> required <p>Returns:</p> Type Description <code>bool</code> <p>If the cluster is cohesive (silhouette&gt;0) or not</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def is_cohesive(self, clusters):\n    \"\"\"Verifies if this cluster is cohesive for novelty detection purposes.\n    A new micro-cluster is cohesive if its silhouette coefficient is larger than 0.\n    'b' represents the Euclidean distance between the centroid of the new micro-cluster and the centroid of its\n    closest micro-cluster, and 'a' represents the standard deviation of the distances between the examples of the\n    new micro-cluster and the centroid of the new micro-cluster.\n\n    Parameters\n    ----------\n    clusters : List of MicroCluster\n        Existing known micro-clusters\n\n    Returns\n    -------\n    bool\n        If the cluster is cohesive (silhouette&gt;0) or not\n    \"\"\"\n    b = self.distance_to_centroid(self.find_closest_cluster(clusters).centroid)\n    a = np.std(self.distance_to_centroid(self.instances))\n    silhouette = (b - a) / max(a, b)  # hm, this is always positive if b &gt; a\n    return silhouette &gt; 0\n</code></pre>"},{"location":"reference/utils/mcikmeans/#streamndr.utils.mcikmeans.MicroCluster.is_representative","title":"<code>is_representative(min_examples)</code>","text":"<p>Verifies if this cluster is representative for novelty detection purposes. A new micro-cluster is representative if it contains a minimal number of examples, where this number is a user-defined parameter.</p> <p>Parameters:</p> Name Type Description Default <code>min_examples</code> <code>int</code> <p>The number of samples the microcluster needs to have to be considered representative.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>If the cluster is representative or not</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def is_representative(self, min_examples):\n    \"\"\"Verifies if this cluster is representative for novelty detection purposes.\n    A new micro-cluster is representative if it contains a minimal number of examples,\n    where this number is a user-defined parameter.\n\n    Parameters\n    ----------\n    min_examples : int\n        The number of samples the microcluster needs to have to be considered representative.\n\n    Returns\n    -------\n    bool\n        If the cluster is representative or not\n    \"\"\"\n    return self.n &gt;= min_examples\n</code></pre>"},{"location":"reference/utils/mcikmeans/#streamndr.utils.mcikmeans.MicroCluster.small_str","title":"<code>small_str()</code>","text":"<p>Returns string representation of a microcluster.</p> <p>Returns:</p> Type Description <code>str</code> <p>Small string representation of microcluster</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def small_str(self):\n    \"\"\"Returns string representation of a microcluster.\n\n    Returns\n    -------\n    str\n        Small string representation of microcluster\n    \"\"\"\n\n    return f\"\"\"Target class {self.label}\n            # of instances: {self.n}\n            Timestamp of last change: {self.timestamp}\"\"\"\n</code></pre>"},{"location":"reference/utils/mcikmeans/#streamndr.utils.mcikmeans.MicroCluster.update_cluster","title":"<code>update_cluster(X, timestamp, update_summary)</code>","text":"<p>Adds point received in parameter to the cluster and update cluster's centroid if wanted.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>One point</p> required <code>timestamp</code> <code>int</code> <p>Timestamp when this point was added to this microcluster</p> required <code>update_summary</code> <code>bool</code> <p>Whether or not to update the microcluster properties with this new point</p> required Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def update_cluster(self, X, timestamp, update_summary):\n    \"\"\"Adds point received in parameter to the cluster and update cluster's centroid if wanted.\n\n    Parameters\n    ----------\n    X : numpy.ndarray\n        One point\n    timestamp : int\n        Timestamp when this point was added to this microcluster\n    update_summary : bool\n        Whether or not to update the microcluster properties with this new point\n    \"\"\"\n\n    assert len(X.shape) == 1  # it's just one point\n    self.timestamp = timestamp\n\n    if self.instances is not None:\n        self.instances.append(X)\n\n    if update_summary:\n        self.mean_distance = (self.n * self.mean_distance + self.distance_to_centroid(X)) / (self.n + 1)\n        self.n += 1\n        self.linear_sum = np.sum([self.linear_sum, X], axis=0)\n        self.squared_sum = np.sum([self.squared_sum, np.square(X).sum()], axis=0)\n        self.update_properties()\n</code></pre>"},{"location":"reference/utils/mcikmeans/#streamndr.utils.mcikmeans.MicroCluster.update_properties","title":"<code>update_properties()</code>","text":"<p>Updates centroid and radius based on current cluster properties.</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def update_properties(self):\n    \"\"\"Updates centroid and radius based on current cluster properties.\"\"\"\n    self.centroid = self.linear_sum / self.n\n\n    if self.instances is not None:\n        self.radius = self.get_radius()\n        if np.max(self.distance_to_centroid(self.instances)) &gt; self.max_distance:\n            self.max_distance = np.max(self.distance_to_centroid(self.instances))\n</code></pre>"},{"location":"reference/utils/mcikmeans/#streamndr.utils.mcikmeans.ShortMemInstance","title":"<code>ShortMemInstance</code>","text":"<p>Instance of a point associated with a timestamp. Used for the buffer memory which stores the unknown samples.</p> <p>Attributes:</p> Name Type Description <code>point</code> <code>ndarray</code> <p>The coordinates of the point</p> <code>timestamp</code> <code>int</code> <p>The timestamp the point was added/treated</p> <code>y_true</code> <code>int</code> <p>The true value of the class</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>class ShortMemInstance:\n    \"\"\"Instance of a point associated with a timestamp. Used for the buffer memory which stores the unknown samples.\n\n    Attributes\n    ----------\n    point : numpy.ndarray\n        The coordinates of the point\n    timestamp : int\n        The timestamp the point was added/treated\n    y_true : int\n        The true value of the class\n    \"\"\"\n    def __init__(self, point, timestamp, y_true=None):\n        self.point = point\n        self.timestamp = timestamp\n        self.y_true = y_true\n\n    def __eq__(self, other):\n        \"\"\"Elements are equal if they have the same values for all variables.\n        This currently does not consider the timestamp.\n\n        Parameters\n        ----------\n        other : ShortMemInstance\n            Other instance to compared to\n\n        Returns\n        -------\n        bool\n            If the instances are equals or not\n        \"\"\"\n        if type(other) == np.ndarray:\n            return np.all(self.point == other)\n</code></pre>"},{"location":"reference/utils/mcikmeans/#streamndr.utils.mcikmeans.ShortMemInstance.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Elements are equal if they have the same values for all variables. This currently does not consider the timestamp.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>ShortMemInstance</code> <p>Other instance to compared to</p> required <p>Returns:</p> Type Description <code>bool</code> <p>If the instances are equals or not</p> Source code in <code>streamndr/utils/data_structure.py</code> <pre><code>def __eq__(self, other):\n    \"\"\"Elements are equal if they have the same values for all variables.\n    This currently does not consider the timestamp.\n\n    Parameters\n    ----------\n    other : ShortMemInstance\n        Other instance to compared to\n\n    Returns\n    -------\n    bool\n        If the instances are equals or not\n    \"\"\"\n    if type(other) == np.ndarray:\n        return np.all(self.point == other)\n</code></pre>"}]}